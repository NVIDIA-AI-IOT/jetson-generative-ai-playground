{
  "models": {
    "name": "Models",
    "tags": []
  },
  "api": {
    "name": "Runtime API",
    "tags": "enum",
    "help": "The runtime library or container to use for inferencing."
  },
  "mlc": {
    "name": "MLC",
    "tags": "api",
    "links": {
      "mlc": {
        "name": "MLC",
        "url": "https://llm.mlc.ai/"
      }
    }
  },
  "mlc:jp6": {
    "name": "dustynv/mlc:r36.4.0",
    "docker_image": "dustynv/mlc:r36.4.0",
    "tags": ["sudonim", "mlc", "l4t-r36"]
  },
  "llama_cpp": {
    "name": "llama.cpp",
    "tags": "api",
    "links": {
      "llama_cpp": {
        "name": "llama.cpp",
        "url": "https://github.com/ggerganov/llama.cpp"
      }
    }
  },
  "llama_cpp:jp6": {
    "name": "dustynv/llama_cpp:r36.4.0",
    "docker_image": "dustynv/llama_cpp:r36.4.0",
    "tags": ["sudonim", "llama_cpp", "l4t-r36"]
  },
  "ollama": {
    "name": "ollama",
    "tags": "api",
    "links": {
      "ollama": {
        "name": "ollama",
        "url": "https://ollama.com/"
      }
    }
  },
  "ollama:jp6": {
    "name": "dustynv/ollama:main-r36.4.0",
    "docker_image": "dustynv/ollama:main-r36.4.0",
    "docker_options": "-it --rm -e OLLAMA_MODEL=${MODEL} -e OLLAMA_MODELS=/root/.ollama -e OLLAMA_HOST=${SERVER_HOST} -e OLLAMA_CONTEXT_LEN=${MAX_CONTEXT_LEN} -e OLLAMA_LOGS=/root/.ollama/ollama.log -v ${CACHE_DIR}/ollama:/root/.ollama",
    "server_host": "0.0.0.0:9000",
    "tags": ["container", "ollama", "l4t-r36"]
  },
  "vllm": {
    "name": "vLLM",
    "tags": "api",
    "links": {
      "mlc": {
        "name": "vLLM",
        "url": "https://github.com/vllm-project/vllm"
      }
    }
  },
  "vllm:jp6": {
    "name": "dustynv/vllm:0.7.4-r36.4.0-cu128-24.04",
    "docker_image": "dustynv/vllm:0.7.4-r36.4.0-cu128-24.04",
    "docker_cmd": "vllm serve ${MODEL}",
    "docker_args": "--host=${SERVER_ADDR} --port=${SERVER_PORT} --dtype=auto --max-num-seqs=${MAX_BATCH_SIZE} --max-model-len=${MAX_CONTEXT_LEN} --gpu-memory-utilization=0.75 ${VLLM_QUANTIZATION}",
    "docker_options": "-it --rm",
    "server_host": "0.0.0.0:9000",
    "tags": ["container", "vllm", "l4t-r36"]
  },
  "awq": {
    "name": "AWQ TinyChat",
    "tags": "api"
  },
  "sudonim": {
    "docker_cmd": "sudonim serve",
    "docker_options": "-it --rm",
    "server_host": "0.0.0.0:9000",
    "tags": ["container"]
  }
}