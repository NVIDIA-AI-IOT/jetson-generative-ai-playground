{
  "llm": {
    "name": "Language Models (LLM / SLM)",
    "tags": "models",
    "refs": "resource",
    "max_batch_size": 1,
    "max_context_len": null,
    "prefill_chunk": null,
    "chat_template": null,
    "hf_token": null,
    "property_order": [
      "url", 
      "docker_image", 
      "quantization", 
      "max_batch_size", 
      "max_context_len", 
      "prefill_chunk", 
      "chat_template",
      "hf_token",
      "cache_dir",
      "docker_run",
      "docker_cmd",
      "docker_options",
      "server_host",
      "auto_update"
    ]
  },
  "max_batch_size": {
    "name": "Max Batch Size",
    "tags": "number",
    "help": "The maximum number of generation requests to handle in parallel at one time."
  },
  "max_context_len": {
    "name": "Max Context Len",
    "tags": "number",
    "help": [
      "The maximum number of tokens in the chat history, including any system instruction, prompt, and future reply.",
      "Reduce this from the model's default to decrease memory usage. This can be left unset, and the model's default will be used."
    ]
  },
  "prefill_chunk": {
    "name": "Prefill Chunk Len",
    "tags": "number",
    "help": [
      "The maximum number of input tokens that can be prefilled into the KV cache at once.",
      "Longer prompts are prefilled in multiple batches.\nReduce this from the model's default to decrease memory usage."
    ]
  },
  "chat_template": {
    "name": "Chat Template",
    "tags": "string",
    "placeholder": "<default>",
    "help": [
      "Manually set the model's conversation template.  Normally this will be attempted to be determined automatically,", 
      "but in some cases needs set and is specific to runtime APIs and model types."
    ]
  },
  "tensor_parallel": {
    "name": "Tensor Parallel",
    "tags": "number",
    "help": "The number of GPUs to split the model across (for multi-GPU systems)"
  }
}