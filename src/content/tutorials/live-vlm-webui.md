---
title: "Live VLM Web UI"
description: "Run vision-language models on live camera feeds or video streams with a web interface. Features real-time video analysis, event filtering, multimodal RAG, and NanoDB integration."
category: "Multimodal"
section: "Vision Language Models"
order: 1
tags: ["vlm", "vision", "camera", "live-streaming", "llava", "vila", "nano-llm", "webrtc", "multimodal"]
featured: true
isNew: true
---

This multimodal agent runs a vision-language model on a live camera feed or video stream, repeatedly applying the same prompts to it.

<a href="https://youtu.be/X-OXxPiUTuU" target="_blank">
<img src="https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava.gif" alt="Live LLaVA Demo">
</a>

It uses models like [LLaVA](https://llava-vl.github.io/) or [VILA](https://github.com/Efficient-Large-Model/VILA) and has been quantized with 4-bit precision. This runs an optimized multimodal pipeline from the [NanoLLM](https://dusty-nv.github.io/NanoLLM) library, including:

- Running the CLIP/SigLIP vision encoder in TensorRT
- Event filters and alerts
- Multimodal RAG

![Multimodal Agent Architecture](https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/multimodal_agent.jpg)

---

## Prerequisites

**Supported Devices:**
- Jetson AGX Orin (64GB)
- Jetson AGX Orin (32GB)
- Jetson Orin NX (16GB)
- Jetson Orin Nano (8GB) ⚠️ *Can run Llava-7b, VILA-7b, and Obsidian-3B*

**JetPack Version:**
- JetPack 6 (L4T r36.x)

**Storage:** NVMe SSD **highly recommended**
- `22GB` for `nano_llm` container image
- Space for models (`>10GB`)

**Supported Models:**
- `liuhaotian/llava-v1.5-7b`, `liuhaotian/llava-v1.5-13b`
- `liuhaotian/llava-v1.6-vicuna-7b`, `liuhaotian/llava-v1.6-vicuna-13b`
- `Efficient-Large-Model/VILA-2.7b`, `Efficient-Large-Model/VILA-7b`, `Efficient-Large-Model/VILA-13b`
- `Efficient-Large-Model/VILA1.5-3b`, `Efficient-Large-Model/Llama-3-VILA1.5-8B`, `Efficient-Large-Model/VILA1.5-13b`
- `VILA-2.7b`, `VILA1.5-3b`, `VILA-7b`, `Llava-7b`, and `Obsidian-3B` can run on Orin Nano 8GB

---

## Running the Live VLM Demo

The [VideoQuery](https://dusty-nv.github.io/NanoLLM/agents.html#video-query) agent applies prompts to the incoming video feed with the VLM.

Navigate your browser to `https://<IP_ADDRESS>:8050` after launching it with your camera.

> **Tip:** Chrome is recommended with `chrome://flags#enable-webrtc-hide-local-ips-with-mdns` disabled.

```bash
jetson-containers run $(autotag nano_llm) \
  python3 -m nano_llm.agents.video_query --api=mlc \
    --model Efficient-Large-Model/VILA1.5-3b \
    --max-context-len 256 \
    --max-new-tokens 32 \
    --video-input /dev/video0 \
    --video-output webrtc://@:8554/output
```

This uses `jetson_utils` for video I/O. In the example above, it captures a V4L2 USB webcam connected to the Jetson (under the device `/dev/video0`) and outputs a WebRTC stream.

For options related to protocols and file formats, see [Camera Streaming and Multimedia](https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-streaming.md).

---

## Processing a Video File or Stream

The example above was running on a live camera, but you can also read and write a video file or network stream by substituting the path or URL to the `--video-input` and `--video-output` command-line arguments:

```bash
jetson-containers run \
  -v /path/to/your/videos:/mount \
  $(autotag nano_llm) \
    python3 -m nano_llm.agents.video_query --api=mlc \
      --model Efficient-Large-Model/VILA1.5-3b \
      --max-context-len 256 \
      --max-new-tokens 32 \
      --video-input /mount/my_video.mp4 \
      --video-output /mount/output.mp4 \
      --prompt "What does the weather look like?"
```

This example processes a pre-recorded video (in MP4, MKV, AVI, FLV formats with H.264/H.265 encoding), but it also can input/output live network streams like RTP, RTSP, and WebRTC using Jetson's hardware-accelerated video codecs.

---

## NanoDB Integration

If you launch the VideoQuery agent with the `--nanodb` flag along with a path to your NanoDB database, it will perform reverse-image search on the incoming feed against the database by re-using the CLIP embeddings generated by the VLM.

First, set up a NanoDB database, then launch VideoQuery like this:

```bash
jetson-containers run $(autotag nano_llm) \
  python3 -m nano_llm.agents.video_query --api=mlc \
    --model Efficient-Large-Model/VILA1.5-3b \
    --max-context-len 256 \
    --max-new-tokens 32 \
    --video-input /dev/video0 \
    --video-output webrtc://@:8554/output \
    --nanodb /data/nanodb/coco/2017
```

You can also tag incoming images and add them to the database using the web UI, for one-shot recognition tasks.

---

## Video VILA

The VILA-1.5 family of models can understand multiple images per query, enabling:

- Video search/summarization
- Action & behavior analysis
- Change detection
- Other temporal-based vision functions

The `vision/video.py` example keeps a rolling history of frames:

```bash
jetson-containers run $(autotag nano_llm) \
  python3 -m nano_llm.vision.video \
    --model Efficient-Large-Model/VILA1.5-3b \
    --max-images 8 \
    --max-new-tokens 48 \
    --video-input /data/my_video.mp4 \
    --video-output /data/my_output.mp4 \
    --prompt 'What changes occurred in the video?'
```

<a href="https://youtu.be/_7gughth8C0" target="_blank">
<img src="https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/video_vila_wildfire.gif" alt="Video VILA Wildfire Demo">
</a>

---

## Python Code Example

For a simplified code example of doing live VLM streaming from Python, see the [NanoLLM documentation](https://dusty-nv.github.io/NanoLLM/multimodal.html#code-example).

You can use this to implement customized prompting techniques and integrate with other vision pipelines. This code applies the same set of prompts to the latest image from the video feed. See the [video.py source](https://github.com/dusty-nv/NanoLLM/blob/main/nano_llm/vision/video.py) for the version that does multi-image queries on video sequences.

---

## Walkthrough Videos

<div style="display: flex; flex-wrap: wrap; gap: 16px;">
<iframe width="400" height="225" src="https://www.youtube.com/embed/wZq7ynbgRoE" title="Live LLaVA Walkthrough" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
<iframe width="400" height="225" src="https://www.youtube.com/embed/8Eu6zG0eEGY" title="Video VILA Demo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</div>

---

## Next Steps

- [Jetson Platform Services](/tutorials/jetson-platform-services) - Build full AI applications with microservices
- [GenAI Benchmarking](/tutorials/genai-benchmarking) - Measure your model's performance
- [Supported Models](/models) - Browse optimized models for Jetson

