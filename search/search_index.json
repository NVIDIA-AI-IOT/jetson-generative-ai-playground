{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"agent_studio.html","title":"Agent Studio","text":"<p>Rapidly design and experiment with creating your own automation agents, personal assistants, and edge AI systems in an interactive sandbox for connecting multimodal LLMs, speech and vision transformers, vector databases, prompt templating, and function calling to live sensors and I/O.  Optimized for deployment onboard Jetson with on-device compute, low-latency streaming, and unified memory.</p> \u2022 Edge LLM inference with quantization and KV caching (NanoLLM) \u2022 Realtime vision/language models (ala Live Llava and Video VILA) \u2022 Speech recognition and synthesis (Whisper ASR, Piper TTS, Riva) \u2022 Multimodal vector database from NanoDB \u2022 Audio and video streaming (WebRTC, RTP, RTSP, V4L2) \u2022 Performance monitoring and profiling \u2022 Native bot-callable functions and agent tools \u2022 Extensible plugins with auto-generated UI controls \u2022 Save, load, and export pipeline presets \u2022 Native bot-callable functions and agent tools"},{"location":"agent_studio.html#running-agent-studio","title":"Running Agent Studio","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>22GB</code> for <code>nano_llm</code> container image</li> <li>Space for models (<code>&gt;5GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol> <pre><code>jetson-containers run --env HUGGINGFACE_TOKEN=hf_xyz123abc456 \\\n  $(autotag nano_llm) \\\n    python3 -m nano_llm.studio\n</code></pre> <p>This will start the server running on your device.  You can then navigate your browser to <code>https://IP_ADDRESS:8050</code></p> <ul> <li>You can load a preset at startup with the <code>--load</code> flag (like <code>--load /data/nano_llm/presets/xyz.json</code>)         <li>The default port is 8050, but can be changed with <code>--web-port</code> (and <code>--ws-port</code> for the websocket port)</li> <li>Request access to the Llama models on HuggingFace and substitute your account's API token above.</li>"},{"location":"agent_studio.html#dev-mode","title":"Dev Mode","text":"<p>To make code changes without rebuilding the container, clone the NanoLLM sources and then mount them over <code>/opt/NanoLLM</code></p> <pre><code>git clone https://github.com/dusty-nv/NanoLLM\njetson-containers run \\\n  -v ${PWD}/NanoLLM:/opt/NanoLLM \\\n  $(autotag nano_llm)\n</code></pre> <p>You can then edit the source from outside the container.  And in the terminal that starts, you can install other packages from apt/pip/ect.</p>"},{"location":"agent_studio.html#plugins","title":"Plugins","text":"<p>The models, I/O, and logic blocks share a lightweight plugin interface, exposing a <code>process()</code> function and any runtime properties:</p> Custom Plugin<pre><code>from nano_llm import Plugin\n\nclass TextFilter(Plugin):\n    def __init__(self, search: str='bad word', replace: str='censored', **kwargs):\n        \"\"\"\n        A simple plugin performing text replacement.  Write docs because they're extracted for the UI,\n        and for the tool descriptions that go to the bot for function calling (and use the type hints)        \n\n        Args:\n          search: The string to look for an replace in incoming text.\n          replace: What to replace occurances of the search string with.\n        \"\"\"\n        super().__init__(inputs=['text'], outputs=['text'])\n\n        # set these attributes and register them as configurable\n        self.add_parameters(search=search, replace=replace)\n\n    def process(self, input, **kwargs):\n        \"\"\" Substitute the text, and the return value will be sent to the plugins connected to this. \"\"\"\n        return input.replace(self.search, self.replace)\n</code></pre> <p>Typically each plugin runs asynchronously its own thread and queue of data, although some lightweight routines are executed inline.  Usually they're in the same process, but could make requests to microservices.  Websockets are used for clients and the web UI.</p> <p>Below are descriptions of commonly-used components. Help text for these is extracted from their Python docs and shown in the UI, along with settings that you can control.  In the node editor, click on a plugin to open its grid widget or configuration window.</p>"},{"location":"agent_studio.html#llm","title":"LLM","text":"NanoLLMAutoPromptUserPromptTextStream &gt; <code>nano_llm.plugins.NanoLLM</code> <code>(source)</code><pre><code> Load quantized LLM/VLM with MLC (speed), AWQ (quality), or HF Transformers (compatability)\n\n - Inputs\n     * str, list[str], np.ndarray, torch.Tensor, cudaImage, PIL.Image  \n\n - Commands     \n     * /reset /refresh /generate (these inputs can control the chat)\n\n - Outputs\n     * delta    The latest detokenized text to be generated\n     * partial  All of the text generated in the response so far\n     * final    The complete response sent at the end of the request\n     * words    Similar to 'delta', but with at least one word at a time\n     * history  A list of the messages in the chat history (as dictionaries)\n     * tools    Enable the bot to call functions in plugins connected on this channel\n</code></pre> &gt; <code>nano_llm.plugins.AutoPrompt</code> <code>(source)</code><pre><code> Apply a template each time new data comes in, for example '&lt;image&gt; Describe the image' \n would tag each incoming image along with that text prompt (with images, usually for VLM)\n\n AutoPrompt can be used to compose structured messages that reference prior inputs, \n like `Image 1: &lt;image&gt;  Image 2:  &lt;image&gt;` or using &lt;text&gt; for last text recieved:  \n\n    The three most recent replies were:\n      * &lt;text&gt;\n      * &lt;text&gt;\n      * &lt;text&gt; \n\nThese most recent inputs are used in newest to oldest order from a LIFO queue.\n</code></pre> &gt; <code>nano_llm.plugins.UserPrompt</code> <code>(source)</code><pre><code> This plugin is for inputting text on the keyboard, either from the terminal or text box in the UI.\n\n It can also load prompts from a text or json file, and those files can reference other files to load.\n</code></pre> &gt; <code>nano_llm.plugins.TextStream</code> <code>(source)</code><pre><code> Simple plugin for viewing any stream of text from the system in a text box in the UI.  \n\n For sources emanating from ASR or LLM, can apply color highlighting to partial/final responses.\n</code></pre>"},{"location":"agent_studio.html#speech","title":"Speech","text":"Whisper ASRPiper TTSVAD FilterRiva &gt; <code>nano_llm.plugins.WhisperASR</code> <code>(source)</code><pre><code> Whisper streaming speech-to-text with TensorRT (github.com/NVIDIA-AI-IOT/whisper_trt)\n\n Supports these Whisper models:  'tiny' (39M), 'base' (74M), 'small' (244M)\n\n - Inputs\n     * audio    bytes, list[int], np.ndarray, torch.Tensor (int16, float32)\n\n - Outputs\n     * final    The text transcript of the entire segment spoken in the input\n     * partial  Rolling partial transcript of the words spoken so far in the audio segment\n</code></pre> <p>The Whisper plugin is designed to be used in a pipeline with the VAD Filter to reduce erroneous output from background noise:</p> <p></p> &gt; <code>nano_llm.plugins.PiperTTS</code> <code>(source)</code><pre><code> Piper text-to-speech using CUDA and onnxruntime (github.com/rhasspy/piper)\n\n Populates a list of the available Piper models that it can download, along with the speaker voices \n for multi-speaker models.  The default model is 'en_US-libritts-high'.  Has controls for voice speed.\n\n - Inputs\n     * str      either individual words, sentences, or an entire passage to synthesize\n\n - Outputs\n     * audio    np.ndarray (int16) at 16KHz sample rate (for low-quality models) or 22.5KHz (for medium/high)\n</code></pre> <p>This PiperTTS preset is available, which uses RateLimit to stream the output at 1x realtime factor (so it can be muted later)</p> <p></p> &gt; <code>nano_llm.plugins.VADFilter</code> <code>(source)</code><pre><code> Voice activity detection model using Silero.  Drops incoming audio unless it exceeds the VAD threshold.\n\n Use this on incoming audio before ASR plugins to reduce spurious transcripts produced from background noise.\n\n - Inputs\n     * audio    bytes, list[int], np.ndarray, torch.Tensor (int16, float32)\n\n - Outputs\n     * audio    np.ndarray or torch.Tensor (only when VAD confidence &gt; threshold)\n</code></pre> <p>Riva ASR / TTS services - these run in another container over GRPC.  See these resources for setup:</p> <ul> <li><code>NGC - riva-embedded quickstart guide</code></li> <li><code>jetson-containers - riva container setup</code></li> <li><code>JetsonHacks - Speech AI on NVIDIA Jetson Tutorial</code></li> </ul>"},{"location":"agent_studio.html#audio","title":"Audio","text":"<code>AudioInputDevice</code>USB/I2S soundcard input via PulseAudio/PyAudio (for microphone directly attached to system) <code>AudioOutputDevice</code>USB/I2S soundcard output via PulseAudio/PyAudio (for speakers directly attached to system) <code>AudioRecorder</code>Save an audio stream to WAV file on the server <code>WebAudioIn</code>Recieve audio samples streamed from the client over websockets <code>WebAudioOut</code>Transmit audio samples to the client over websockets"},{"location":"agent_studio.html#video","title":"Video","text":"<code>VideoSource</code>Capture images from camera device (V4L2/CSI), network stream (RTP, RTSP), video file (MP4, MKV, AVI, FLV) <code>VideoOutput</code>Output H264/H265-encoded video to network stream (RTP, RTSP, WebRTC), display, or file (MP4, MKV, AVI, FLV) <code>VideoOverlay</code>Draw text on top of video streams for HUD or OSD-style display <code>RateLimit</code>Throttle transmission to a specified rate (can also be used with audio)"},{"location":"agent_studio.html#database","title":"Database","text":"<code>NanoDB</code>Optimized in-memory multimodal vectorDB for txt2img/img2img similarity search and image tagging [[\u2197]](tutorial_nanodb.md) <code>Deduplicate</code>Filter incoming text against previous entries with embedding model and cosine similarity over a timeout period <code>EventFilter</code>Look for keywords or search strings in streams of text, keeping track of the begin/end times that these occur."},{"location":"agent_studio.html#tips-tricks","title":"Tips &amp; Tricks","text":"<p>Many of the previous demos (like Llamaspeak and Live Llava) can quickly be recreated in the dynamic environment of Agent Studio without needing to touch any code.  Here are some practical tips as you go about trying different combinations of model pipelines:</p>"},{"location":"agent_studio.html#ui","title":"UI","text":"<ul> <li>To add an element to the agent pipeline, either use the menus in the node editor title bar, or right click inside the editor.  </li> <li>Inserting elements via the context menu (right click) will position them at your cursor, so you needn't reposition them.  </li> <li>To remove a component from the pipeline, right click on it and then select the little X button in the top right.  </li> <li>To remove a connection between components, first select the link and then right click for the X button to appear.  </li> <li>Click on a node in the graph to open its grid widget if it implements one.  Click again to open its settings dialog.  </li> </ul>"},{"location":"agent_studio.html#save-load","title":"Save &amp; Load","text":"<ul> <li>Under the Agent menu, there is a Save button which will serialize the current pipeline to JSON.</li> <li>These presets are saved in a mounted directory on the host, under <code>jetson-containers/data/nano_llm/presets</code></li> <li>Some commonly-used subgraphs are already included to quickly add and combine (like ASR, TTS, and video VLM)</li> <li>You can load these via the Agent menu, by right-clicking inside the node editor, or with the <code>--load</code> flag at startup.</li> </ul>"},{"location":"agent_studio.html#memory","title":"Memory","text":"<ul> <li>As you are adding models, keep an eye on the system resources in the top right to make sure you don't run out of memory.  </li> <li>Due to loading times, the models are cached in memory even after you remove them from the node editor.  </li> <li>To actually free the memory, use the <code>Clear Cache</code> button in the Agent menu. </li> </ul>"},{"location":"agent_studio.html#performance","title":"Performance","text":"<ul> <li>You can monitor the system's CPU and GPU usage in the top right corner.  </li> <li>Various statistics are refreshed in the nodes to see performance of each component.</li> <li>Use the <code>RateLimiter</code> plugin to throttle data sources and balance resources.</li> <li>The <code>drop_inputs</code> setting that some plugins expose will always keep them up with the latest request.</li> </ul>"},{"location":"agent_studio.html#templating","title":"Templating","text":"<ul> <li>In your prompts and system templates, you can perform variable substitution like <code>${DATE}</code> and <code>${TIME}</code></li> <li>These can reference plugin attributes and tools by using the class selector:  <code>${HOMEASSISTANT.DEVICES}</code></li> <li>AutoPrompt can queue up inputs for structured messages, like <code>Image 1: &lt;image&gt;  Image 2:  &lt;image&gt;</code> or: <pre><code>The three most recent replies were:\n    * &lt;text&gt;\n    * &lt;text&gt;\n    * &lt;text&gt;\n</code></pre></li> </ul> <p>\ud83e\udd16 Have fun bot building!  If you need help, reach out on the Jetson Forums or GitHub Issues.</p>"},{"location":"benchmarks.html","title":"Benchmarks","text":"<p>Below are AI inferencing benchmarks for Jetson AGX Thor Developer Kit, Jetson Orin Nano Super and Jetson AGX Orin.</p>"},{"location":"benchmarks.html#jetson-agx-thor-developer-kit","title":"Jetson AGX Thor Developer Kit","text":""},{"location":"benchmarks.html#jetson-orin-nano-super","title":"Jetson Orin Nano Super","text":"LLM / SLMVision / Language ModelsVision Transformers Model Jetson Orin Nano (original) Jetson Orin Nano Super Perf Gain (X) Llama 3.1 8B 14 19.14 1.37 Llama 3.2 3B 27.7 43.07 1.55 Qwen2.5 7B 14.2 21.75 1.53 Gemma 2 2B 21.5 34.97 1.63 Gemma 2 9B 7.2 9.21 1.28 Phi 3.5 3B 24.7 38.1 1.54 SmolLM2 41 64.5 1.57 <p>For running these benchmarks, this script will launch a series of containers that download/build/run the models with MLC and INT4 quantization.</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\nbash jetson-containers/packages/llm/mlc/benchmarks.sh\n</code></pre> <p></p> Model Jetson Orin Nano (original) Jetson Orin Nano Super Perf Gain (X) VILA 1.5 3B 0.7 1.06 1.51 VILA 1.5 8B 0.574 0.83 1.45 LLAVA 1.6 7B 0.412 0.57 1.38 Qwen2 VL 2B 2.8 4.4 1.57 InternVL2.5 4B 2.5 5.1 2.04 PaliGemma2 3B 13.7 21.6 1.58 SmolVLM 2B 8.1 12.9 1.59 <p></p> Model Jetson Orin Nano (original) Jetson Orin Nano Super Perf Gain (X) clip-vit-base-patch32 196 314 1.60 clip-vit-base-patch16 95 161 1.69 DINOv2-base-patch14 75 126 1.68 SAM2 base 4.42 6.34 1.43 Grounding DINO 4.11 6.23 1.52 vit-base-patch16-224 98 158 1.61 vit-base-patch32-224 171 273 1.60"},{"location":"benchmarks.html#jetson-agx-orin","title":"Jetson AGX Orin","text":"Large Language Models (LLM)Small Language Models (SLM)Vision Language Models (VLM)Vision Transformers (ViT)Stable DiffusionRivaVector Database <p>For running LLM benchmarks, see the <code>MLC</code> container documentation.</p> <p></p> <p>Small language models are generally defined as having fewer than 7B parameters (Llama-7B shown for reference) For more data and info about running these models, see the <code>SLM</code> tutorial and <code>MLC</code> container documentation.</p> <p></p> <p>This measures the end-to-end pipeline performance for continuous streaming like with Live Llava. For more data and info about running these models, see the <code>NanoVLM</code> tutorial.</p> <p></p> <p>VIT performance data from [1] [2] [3]</p> <p></p> <p></p> <p>For running Riva benchmarks, see ASR Performance and TTS Performance.</p> <p></p> <p>For running vector database benchmarks, see the <code>NanoDB</code> container documentation.</p>"},{"location":"community_articles.html","title":"Community Projects","text":"<p>Below, you'll find a collection of guides, tutorials, and articles contributed by the community showcasing the implementation of generative AI on the Jetson platform.</p>"},{"location":"community_articles.html#github-jetbot-voice-activated-copilot-tools-with-ros2-riva-and-nanollm-9-21-2024","title":"GitHub Jetbot Voice-Activated Copilot Tools with ROS2, RIVA, and NanoLLM <sup>(9-21-2024)</sup>","text":"<p>Jen Hung Ho created ROS2 nodes for ASR/TTS and LLM/VLM on Jetson that can can be used to control JetBot, including customizable voice commands and the execution of advanced actions.  Check it out on GitHub under <code>Jen-Hung-Ho/ros2_jetbot_tools</code> and <code>Jen-Hung-Ho/ros2_jetbot_voice</code> and on the forums here.</p>"},{"location":"community_articles.html#hackster-ai-powered-photorealistic-talking-avatar-4-26-2024","title":"Hackster AI-Powered Photorealistic Talking Avatar <sup>(4-26-2024)</sup>","text":"<p>Nurgaliyev Shakhizat creates an interactive talking avatar using ASR, TTS, LLM, and Audio2Face with NVIDIA Omniverse and Unreal Engine:</p>"},{"location":"community_articles.html#hackster-an-eye-for-an-item-4-26-2024","title":"Hackster An Eye for an Item <sup>(4-26-2024)</sup>","text":"<p>Allen Tao builds an indoor robot with Isaac ROS that maps your home and remembers where things are using SLAM and NanoDB!</p>"},{"location":"community_articles.html#hackster-escalator-people-tracker-4-2-2024","title":"Hackster Escalator People Tracker <sup>(4-2-2024)</sup>","text":"<p>George Profenza &amp; team install local CV solution for detecting and tracking people in large retail spaces to drive live Generative AI graphics:</p>"},{"location":"community_articles.html#hackster-edge-style-fashion-preview-at-the-edge-4-1-2024","title":"Hackster Edge Style: Fashion Preview at the Edge <sup>(4-1-2024)</sup>","text":"<p>AI-powered app from Andrei Ciobanu shows virtual try-ons with customer images, enhancing retail shopping using Jetson Orin for speed and privacy:</p>"},{"location":"community_articles.html#hackster-cooking-meals-with-a-local-ai-assistant-on-jetson-axg-orin-4-1-2024","title":"Hackster Cooking meals with a local AI assistant on Jetson AXG Orin <sup>(4-1-2024)</sup>","text":"<p>Dimiter Kendri builds a multimodal, multi AI agent, fully local, conversational chatbot with multi agent research capabilities via speech queries:</p>"},{"location":"community_articles.html#hackster-realtime-language-segment-anything-on-jetson-orin-3-4-2024","title":"Hackster Realtime Language-Segment-Anything on Jetson Orin <sup>(3-4-2024)</sup>","text":"<p>Huy Mai enables Segment Anything (SAM) with natural language prompting using GroundingDINO for object detection:</p>"},{"location":"community_articles.html#github-japanese-nmt-translation-for-stable-diffusion-2-23-2024","title":"GitHub Japanese NMT Translation for Stable Diffusion <sup>(2-23-2024)</sup>","text":"<p>Toshihiko Aoki has created a prompt generator for <code>stable-diffusion-webui</code> that translates Japanese queries into English using a fine-tuned GPT-2 NMT model before feeding them into Stable Diffusion.  Check out the full guide on GitHub under <code>to-aoki/ja-tiny-sd-webui</code>, including the training dataset and LoRA building!</p>      Your browser does not support the video tag."},{"location":"community_articles.html#hackster-clearwater-underwater-image-enhancement-with-generative-ai-2-16-2024","title":"Hackster ClearWater: Underwater Image Enhancement with Generative AI <sup>(2-16-2024)</sup>","text":"<p>Vy Pham has created a novel denoising pipeline using a custom trained Transformer-based diffusion model and GAN upscaler for image enhancement, running on Jetson AGX Orin.  It runs interactively in a Streamlit web UI for photo capturing and the processing of images and videos.  Great work!</p>      Your browser does not support the video tag."},{"location":"community_articles.html#hackster-ai-powered-application-for-the-blind-and-visually-impaired-12-13-2023","title":"Hackster AI-Powered Application for the Blind and Visually Impaired <sup>(12-13-2023)</sup>","text":"<p>Nurgaliyev Shakhizat demonstrates a locally-hosted Blind Assistant Device running on Jetson AGX Orin 64GB Developer Kit for realtime image-to-speech translation:</p> <p></p> <p> \u00a0 Find more resources about this project here:  [Hackster] [GitHub]</p>"},{"location":"community_articles.html#daves-armoury-bringing-glados-to-life-with-robotics-and-ai-2-8-2024","title":"Dave's Armoury Bringing GLaDOS to life with Robotics and AI <sup>(2-8-2024)</sup>","text":"<p>See how DIY robotics legend Dave Niewinski from davesarmoury.com brings GLaDOS to life using Jetson AGX Orin, running LLMs onboard alongside object + depth tracking, and RIVA ASR/TTS with a custom-trained voice model for speech recognition and synthesis!  Using Unitree Z1 arm with 3D printing and StereoLabs ZED2.</p> <p> \u00a0 Find more resources about this project here:  [Forums] [GitHub]</p>"},{"location":"community_articles.html#hackster-seeed-studios-local-voice-chatbot-puts-a-speech-recognizing-llama-2-llm-on-your-jetson-2-7-2024","title":"Hackster Seeed Studio's Local Voice Chatbot Puts a Speech-Recognizing LLaMa-2 LLM on Your Jetson <sup>(2-7-2024)</sup>","text":"<p>Seeed Studio has announced the launch of the Local Voice Chatbot, an NVIDIA Riva- and LLaMa-2-based large language model (LLM) chatbot with voice recognition capabilities \u2014 running entirely locally on NVIDIA Jetson devices, including the company's own reComputer range.  Follow the step-by-step guide on the Seeed Studio wiki.</p>"},{"location":"community_articles.html#youtube-genai-nerds-react-insider-look-at-nvidias-newest-generative-ai-2-6-2024","title":"YouTube GenAI Nerds React - Insider Look at NVIDIA's Newest Generative AI <sup>(2-6-2024)</sup>","text":"<p>Watch this panel about the latest trends &amp; tech in edge AI, featuring Kerry Shih from OStream, Jim Benson from JetsonHacks, and Dusty from NVIDIA.</p>"},{"location":"community_articles.html#nvidia-bringing-generative-ai-to-life-with-nvidia-jetson-11-7-2023","title":"NVIDIA Bringing Generative AI to Life with NVIDIA Jetson <sup>(11-7-2023)</sup>","text":"<p>Watch this webinar about deploying LLMs, VLMs, ViTs, and vector databases onboard Jetson Orin for building next-generation applications using Generative AI:</p>"},{"location":"community_articles.html#jetsonhacks-jetson-ai-labs-generative-ai-playground-10-31-2023","title":"JetsonHacks Jetson AI Labs \u2013 Generative AI Playground <sup>(10-31-2023)</sup>","text":"<p>JetsonHacks publishes an insightful video that walks developers through the typical steps for running generative AI models on Jetson following this site's tutorials. The video shows the interaction with the LLaVA model. </p>"},{"location":"community_articles.html#hackster-vision2audio-giving-the-blind-an-understanding-through-ai-10-15-2023","title":"Hackster Vision2Audio - Giving the blind an understanding through AI <sup>(10-15-2023)</sup>","text":"<p>Nurgaliyev Shakhizat demonstrates Vision2Audio running on Jetson AGX Orin 64GB Developer Kit to harness the power of LLaVA to help visually impaired people:</p> <p></p>"},{"location":"community_articles.html#nvidia-generative-ai-models-at-the-edge-10-19-2023","title":"NVIDIA Generative AI Models at the Edge <sup>(10-19-2023)</sup>","text":"<p>Follow this walkthrough of the Jetson AI Lab tutorials along with coverage of the latest features and advances coming to JetPack 6 and beyond:</p> <p> \u00a0 Technical Blog - https://developer.nvidia.com/blog/bringing-generative-ai-to-life-with-jetson/</p>"},{"location":"community_articles.html#medium-how-to-set-up-your-jetson-device-for-llm-inference-and-fine-tuning-10-02-2023","title":"Medium How to set up your Jetson device for LLM inference and fine-tuning <sup>(10-02-2023)</sup>","text":"<p>Michael Yuan's guide demonstrating how to set up the Jetson AGX Orin 64GB Developer Kit specifically for large language model (LLM) inference, highlighting the crucial role of GPUs and the cost-effectiveness of the Jetson AGX Orin for LLM tasks.</p> <p> https://medium.com/@michaelyuan_88928/how-to-set-up-your-jetson-device-for-llm-inference-and-fine-tuning-682e36444d43</p>"},{"location":"community_articles.html#hackster-getting-started-with-ai-on-nvidia-jetson-agx-orin-dev-kit-09-16-2023","title":"Hackster Getting Started with AI on Nvidia Jetson AGX Orin Dev Kit <sup>(09-16-2023)</sup>","text":"<p>Nurgaliyev Shakhizat demonstrates <code>llamaspeak</code> on Jetson AGX Orin 64GB Developer Kit in this Hackster post:</p> <p></p>"},{"location":"community_articles.html#hackster-new-ai-tool-is-generating-a-lot-of-buzz-09-13-2023","title":"Hackster New AI Tool Is Generating a Lot of Buzz <sup>(09-13-2023)</sup>","text":"<p>Nick Bild provides an insightful introduction to the Jetson Generative AI Playground:</p> <p> https://www.hackster.io/news/new-ai-tool-is-generating-a-lot-of-buzz-3cc5f23a3598</p>"},{"location":"community_articles.html#jetsonhacks-use-these-jetson-docker-containers-tutorial-09-04-2023","title":"JetsonHacks Use These! Jetson Docker Containers Tutorial <sup>(09-04-2023)</sup>","text":"<p>JetsonHacks has a in-depth tutorial on how to use <code>jetson-containers</code> and even show <code>text-generation-webui</code> and <code>stable-diffusion-webui</code> containers in action!</p>"},{"location":"community_articles.html#hackster-llama-2-llms-w-nvidia-jetson-and-textgeneration-web-ui-08-17-2023","title":"Hackster LLaMa 2 LLMs w/ NVIDIA Jetson and textgeneration-web-ui <sup>(08-17-2023)</sup>","text":"<p>Paul DeCarlo demonstrates 13B and 70B parameter LLama 2 models running locally on Jetson AGX Orin 64GB Developer Kit in this Hackster post:</p> <p></p>"},{"location":"community_articles.html#hackster-running-a-chatgpt-like-llm-llama2-on-a-nvidia-jetson-cluster-08-14-2023","title":"Hackster Running a ChatGPT-Like LLM-LLaMA2 on a Nvidia Jetson Cluster <sup>(08-14-2023)</sup>","text":"<p>Discover how to run a LLaMA-2 7B model on an NVIDIA Jetson cluster in this insightful tutorial by Nurgaliyev Shakhizat:</p> <p></p>"},{"location":"community_articles.html#jetsonhacks-speech-ai-on-nvidia-jetson-tutorial-08-07-2023","title":"JetsonHacks Speech AI on NVIDIA Jetson Tutorial <sup>(08-07-2023)</sup>","text":"<p>JetsonHacks gives a nice introduction to NVIDIA RIVA SDK and demonstrate its automated speech recognition (ASR) capability on Jetson Orin Nano Developer Kit.</p>"},{"location":"community_articles.html#hackster-llm-based-multimodal-ai-w-azure-open-ai-nvidia-jetson-07-12-2023","title":"Hackster LLM based Multimodal AI w/ Azure Open AI &amp; NVIDIA Jetson <sup>(07-12-2023)</sup>","text":"<p>Learn how to harness the power of Multimodal AI by running Microsoft JARVIS on an Jetson AGX Orin 64GB Developer Kit, enabling a wide range of AI tasks with ChatGPT-like capabilities, image generation, and more, in this comprehensive guide by Paul DeCarlo.</p> <p></p>"},{"location":"community_articles.html#hackster-how-to-run-a-chatgpt-like-llm-on-nvidia-jetson-board-06-13-2023","title":"Hackster How to Run a ChatGPT-Like LLM on NVIDIA Jetson board <sup>(06-13-2023)</sup>","text":"<p>Nurgaliyev Shakhizat explores voice AI assistant on Jetson using FastChat and VoskAPI.</p> <p></p>"},{"location":"cosmos.html","title":"Cosmos - World Foundation Models","text":"<p>Cosmos is a world model development platform that consists of world foundation models, tokenizers and video processing pipeline to accelerate the development of Physical AI at Robotics &amp; AV labs. Cosmos is purpose built for physical AI. The Cosmos repository will enable end users to run the Cosmos models, run inference scripts and generate videos.</p> <p></p> <p>Special thanks to Johnny N\u00fa\u00f1ez Cano for porting the Cosmos and Transformer Engine Jetson! See Cosmos Official page by Nvidia. See Transformer Engine by Nvidia.</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson Thor (XGB) Jetson AGX Orin (64GB) Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>12.26GB</code> for <code>cosmos</code> container image</li> <li>Space for models and datasets (<code>&gt;50GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol> <p>WARNING</p> <p>Transformer Engine:  </p> <ul> <li>Cosmos is optimized for NVIDIA ADA GPU architecture generations and later due running in FP8.  </li> <li>Jetson AGX Orin is based on Ampere.  </li> <li>Support for optimizations across all precisions (FP16, BF16) on NVIDIA Ampere GPU architecture generations and later.  </li> </ul>"},{"location":"cosmos.html#start-container","title":"Start Container","text":"<p>Use this command to automatically run, build, or pull a compatible container image for cosmos:</p> <pre><code>jetson-containers run $(autotag cosmos)\n</code></pre> <p>To mount your own directories into the container, use the  <code>-v</code> or  <code>--volume</code> flags:</p> <pre><code>jetson-containers run -v /path/on/host:/path/in/container $(autotag cosmos)\n</code></pre> <p>Recommendation (This download all models outside docker container):</p> <pre><code>git clone --recursive https://github.com/NVIDIA/Cosmos.git\ncd Cosmos\njetson-containers run -it -v $(pwd):/workspace $(autotag cosmos)\n</code></pre>"},{"location":"cosmos.html#follow-the-instructions-from-cosmos-repository","title":"Follow the instructions from Cosmos repository.","text":"<p>Here is the summarized steps to run the Cosmos models:</p> <p>Generate a Hugging Face access token. Set the access token to 'Read' permission (default is 'Fine-grained').</p> <pre><code>huggingface-cli login\n</code></pre> <p>Download Models:</p> <pre><code>PYTHONPATH=$(pwd) python3 cosmos1/scripts/download_diffusion.py --model_sizes 7B 14B --model_types Text2World Video2World\n</code></pre> <p>Run Demo:</p> <pre><code>PROMPT=\"A sleek, humanoid robot stands in a vast warehouse filled with neatly stacked cardboard boxes on industrial shelves. \\\nThe robot's metallic body gleams under the bright, even lighting, highlighting its futuristic design and intricate joints. \\\nA glowing blue light emanates from its chest, adding a touch of advanced technology. The background is dominated by rows of boxes, \\\nsuggesting a highly organized storage system. The floor is lined with wooden pallets, enhancing the industrial setting. \\\nThe camera remains static, capturing the robot's poised stance amidst the orderly environment, with a shallow depth of \\\nfield that keeps the focus on the robot while subtly blurring the background for a cinematic effect.\"\n</code></pre> <pre><code>PYTHONPATH=$(pwd) python3 cosmos1/models/diffusion/inference/text2world.py \\\n    --checkpoint_dir checkpoints \\\n    --diffusion_transformer_dir Cosmos-1.0-Diffusion-7B-Text2World \\\n    --prompt \"$PROMPT\" \\\n    --video_save_name Cosmos-1.0-Diffusion-7B-Text2World_memory_efficient \\\n    --offload_tokenizer \\\n    --offload_diffusion_transformer \\\n    --offload_text_encoder_model \\\n    --offload_prompt_upsampler \\\n    --offload_guardrail_models\n</code></pre> <p>It will generate a video file in the <code>outputs</code> directory.</p> <p>Another example:</p> <pre><code>PROMPT=\"The video showcases a vibrant, magical garden where flowers bloom dynamically, opening and moving as though responding to a gentle rhythm in nature. \\\nColorful butterflies glide gracefully through the air, and a small, clear stream winds its way through the scene, reflecting the warm glow of sunlight. \\\nA curious rabbit hops along a winding path, leading the viewer to a hidden alcove where a tree with golden, shimmering leaves stands, its branches moving slightly as if alive with energy. \\\nThe entire scene radiates tranquility and wonder, inviting viewers to immerse themselves in the beauty of nature and magic combined.\"\n</code></pre> <pre><code>PYTHONPATH=$(pwd) python3 cosmos1/models/diffusion/inference/text2world.py \\\n    --checkpoint_dir checkpoints \\\n    --diffusion_transformer_dir Cosmos-1.0-Diffusion-7B-Text2World \\\n    --prompt \"$PROMPT\" \\\n    --video_save_name Cosmos-1.0-Diffusion-7B-Text2World_memory_efficient \\\n    --offload_tokenizer \\\n    --offload_diffusion_transformer \\\n    --offload_text_encoder_model \\\n    --offload_prompt_upsampler \\\n    --offload_guardrail_models\n</code></pre>"},{"location":"genesis.html","title":"Genesis: A Generative and Universal Physics Engine for Robotics and Beyond","text":"<p>Genesis is an open-source physics simulation platform with massively-parallel GPU acceleration and provisions for generative automation. </p> <p>As per the project's user guide and GitHub repo, it is designed for robotics &amp; embodied AI by a broad consortium of organizations:</p> <ol> <li>A universal physics engine re-built from the ground up, capable of simulating a wide range of materials and physical phenomena.</li> <li>A lightweight, ultra-fast, pythonic, and user-friendly robotics simulation platform.</li> <li>A powerful and fast photo-realistic rendering system.</li> <li>A generative data engine that transforms user-prompted natural language description into various modalities of data.</li> </ol> \"A mobile franka arm throws all the objects on the floor into the basket.\" \"A mobile franka arm re-organizes the books on the table by pushing them...\" Speech Audio, Facial Animation &amp; Emotion Generation for Digital Human \"Quadraped locomotion with various base heights (Sim2Real)\" <p>Special thanks to Johnny N\u00fa\u00f1ez Cano for porting Genesis to arm64+CUDA and Jetson, along with the extensive stack to support it.   </p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>11.1GB</code> for <code>dustynv/genesis:r36.4.0-cu128</code> container image</li> <li>Space for models and datasets (<code>&gt;50GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> <li> <p>Please refer to the upstream project for more resources:</p> <ul> <li>Code Examples - https://github.com/Genesis-Embodied-AI/Genesis/tree/main/examples</li> <li>Getting Started - https://genesis-world.readthedocs.io/en/latest/user_guide/index.html</li> <li>Documentation - https://genesis-world.readthedocs.io</li> </ul> </li> </ol> <p>!!! \"Final Version\"     Development status of the arm64 porting efforts (3/17/25)</p> <pre><code>- OMPL and LuisaRenders is working on Jetson.\n- [OMPL](https://github.com/ompl/ompl){:target=\"_blank\"} - support for motion planning and IK.\n- [LuisaRender](https://github.com/Alif-01/LuisaRender/tree/d3b8e341540832ebf517cbe6a8abf5da2cf2e026){:target=\"_blank\"} - support for CUDA-accelerated raytracing.\n- Ask questions in [`#robot-simulation-discuss`](https://discord.gg/BmqNSK4886){:target=\"_blank\"} on Discord or [`jetson-containers/issues`](https://github.com/dusty-nv/jetson-containers/issues){:target=\"_blank\"}\n</code></pre>"},{"location":"genesis.html#start-container","title":"Start Container","text":"<p>Currently a container image is available for JetPack 6.1+ and L4T R36.4, built with CUDA 12.8 and PyTorch 2.6:</p> <pre><code>jetson-containers run dustynv/genesis:r36.4.0-cu128\n</code></pre> <p>To mount your own directories into the container, use the  <code>-v</code> or  <code>--volume</code> flags:</p> <pre><code>jetson-containers run -v /path/on/host:/path/in/container dustynv/genesis:r36.4.0-cu128\n</code></pre>"},{"location":"genesis.html#demo-walkthroughs","title":"Demo Walkthroughs","text":"<p>There are numerous quickstart guides and code examples that come with Genesis and are installed inside the container under <code>/opt/genesis</code></p> <pre><code>jetson-containers run -w /opt/genesis/examples \\\n  dustynv/genesis:r36.4.0-cu128 \\\n    python3 elastic_dragon.py --vis\n</code></pre> <p>We encourage you to explore these resources and get involved in the efforts, and share results or feedback with the group and upstream maintainers. We'll continue updating this page with progress from porting the remaining dependencies, along with integration of AI &amp; robotics models and related SDKs like ROS and LeRobot.</p>"},{"location":"getting-started.html","title":"Getting started","text":""},{"location":"hackathon.html","title":"Hackathon Guide","text":"<p>The NVIDIA Jetson empowers you to bring your innovative ideas to life.</p> <p>It\u2019s a powerful, compact edge AI computer with plenty of examples and learning resources to help your team build an impressive AI project at a hackathon. (See an example of a successful team's project.)</p> <p>Explore the tips, guides, and resources below to get started and make the most of your Jetson experience.</p> <p>Good luck, have fun, and happy hacking!\ud83d\udc4d</p>"},{"location":"hackathon.html#initial-setup","title":"Initial Setup","text":"<p>Ideally, your Jetson developer kit is pre-setup. But you can also set it up by yourself.</p> Case 1: Jetson comes pre-setupCase 2: You open the box for the first time <p>Just check the default username and password with the person/organization who provide the hardware.</p> <p>Treehacks 2025</p> <ul> <li>Login information<ul> <li>Username: <code>jetson</code></li> <li>Password: <code>jetson</code></li> </ul> </li> <li>JetPack 6.2 is pre-installed and everything is set up on 1TB SSD.</li> </ul> <p>If the Jetson has not been set up, go through the following initial setup guide that matches your Jetson hardware.</p> <ul> <li>Jetson Orin Nano 8GB Developer Kit : Initial Setup Guide - Jetson Orin Nano</li> <li>Jetson AGX Orin 64GB Developer Kit : Initial Setup (SDK Manager method)</li> </ul>"},{"location":"hackathon.html#physical-setup","title":"Physical Setup","text":"Headless setupMonitor-attached setup <p>A headless setup with Jetson's USB Device Mode provides an easy way to connect your laptop (or a PC) directly to a Jetson with minimal cabling.</p> <p></p> <p>Follow these steps:</p> <ol> <li>Take the Jetson developer kit out of the box and connect the bundled power supply.</li> <li>Find a USB cable to connect the Jetson to your PC (USB-C end goes into the Jetson).</li> <li>You should see a \"L4T-README\" drive automatically gets mounted on your PC.</li> <li>Open a terminal on your PC and SSH into the Jetson using the following command:     <pre><code>ssh ${JETSON_USERNAME}@192.168.55.1\n</code></pre> <p>Replace the above <code>${JETSON_USERNAME}</code> with the actual username that you found or set. To log in, you need to input the password.</p> </li> </ol> Some more detail about USB Device Mode <p>This is realized using Jetson's USB Device Mode feature.</p> <p>When Jetson is connected to a PC, it acts as a USB Composite Device that presents a couple of USB device classes including:</p> <ul> <li>USB Ethernet (to provide the virtual NIC)</li> <li>USB Mass Storage (to let the PC mount \"L4T-README\" drive)</li> <li>USB Serial (to provide serial console access)</li> </ul> <p>When connected to a Windows PC, \"Device Manager\" in \"Devices by connection\" view shows something like following:</p> <p></p> <p>You can also get Jetson on a Wi-Fi network to have the Internet connection and that allows your team members to simultaneously access Jetson to do the work.</p> <p>If you have a PC monitor, DisplayPort cable, USB keyboard and monitor, you can use Jetson as an independent computer and do all the work on it.</p>"},{"location":"hackathon.html#wi-fi-connection","title":"Wi-Fi connection","text":"<p>Here is a command to connect Jetson to a Wi-Fi network. This comes handy especially when you are operating in headless mode.</p> <pre><code>sudo nmcli device wifi connect ${WIFI_SSID} password ${WIFI_PASSWORD}\n</code></pre> <p>Replace <code>${WIFI_SSID}</code> and <code>${WIFI_PASSWORD}</code> with the actual Wi-Fi facility information.</p> <p>Once you have your Jetson on the Wi-Fi network, you can try connect to the Jetson via Wi-Fi from your PC also connected to the same Wi-Fi network.</p> <p>First check what IP address your Jetson got assigned on the wireless network interface.</p> <pre><code>ip addr\n</code></pre> <p>Then try to SSH in using the IP address.</p> <pre><code>ssh jetson@${JETSON_WIFI_IP}\n</code></pre> <p>Replace <code>${JETSON_WIFI_IP}</code> with the actual IP address that you found in above step.</p> <p>Tip</p> <p>Sometimes, the Wi-Fi network policy is set up in such way that it does not allow device-to-device connection (like blocking SSH port).</p> <p>USB Device Mode still allows you to communicate to Jetson via the USB cable from your PC locally, but that would limit only one PC to access Jetson.</p> <p>If you want to have multiple PCs (like of your team members) access Jetson as a server at the same time, but the Wi-Fi network does not allow device-to-device connection, one alternative solution is to bring a Wi-Fi router and form a local network.</p>"},{"location":"hackathon.html#example-projects","title":"Example Projects","text":"<p>Three are many great AI project examples available on this website. Here's a few to get you inspired:</p>"},{"location":"hackathon.html#nanoowl-tree_demo-with-live-camera","title":"\ud83e\udd89 NanoOWL <code>tree_demo</code> with Live Camera","text":"<p>NanoOWL is a project that optimizes OWN-ViT to run in real-time on Jetson Orin Nano.</p> <p>The <code>tree_demo</code> showcases a hierarchical predictor operating on a live camera feed with dynamically editable text prompts.</p> <p></p> <p>For example, a prompt like <code>[a person (healthy, needing assistance)]</code> could help create a robot capable of patrolling large event spaces to identify individuals who might need medical attention, such a s someone collapsing or sitting on the floor from exhaustion, etc.</p> <p>Go to NanoOWL tutorial</p>"},{"location":"hackathon.html#llms-and-openai-compatible-microservices","title":"\ud83d\uddea LLMs and OpenAI compatible microservices","text":"<p>Jetson can run various LLMs locally using its onboard GPU, enabling you to build a robot or device capable of engaging in conversations without needing an Internet connection.</p> <p>Ollama, a popular open-source tool for running large language models locally, offers an official installer with Jetson support, allowing LLMs to run efficiently by leveraging the GPU's processing power.</p> <p>Additionally, you can find pre-build containers built for Jetson that run as microservices, providing OpenAI compatible API endpoints for popular models like DeepSeek-R1 (distilled). This makes it easy to create LLM application using familiar, standard API.</p> <p>Go to Open WebUI tutorial to learn how to use the MLC backend microservice containers</p>"},{"location":"hackathon.html#troubleshooting","title":"Troubleshooting","text":"Even after connecting USB-C cable, I cannot SSH into Jetson <ul> <li>Is the green LED near USB-C port on Jetson Orin Nano Developer Kit lit (power indicator)<ul> <li>If not, check your DC power supply connection (to the wall outlet and to the Jetson carrier board)</li> </ul> </li> <li>Check if you see \"L4T-README\" drive shows up on desktop<ul> <li>If not, check your USB-C cable connection on both PC end and Jetson end. Check the cable with other device.</li> </ul> </li> </ul> I cannot login with <code>jetson</code>/<code>jetson</code> credential <ul> <li>Check the username and password you typed again.</li> <li>Somebody might have changed the password of the default user. If you cannot find out the updated password, you may need to start from the initial setup.</li> </ul> Jetson seems slow <ul> <li>Check your power mode with <code>nvpmodel</code> command     <pre><code>nvpmodel -q\n</code></pre></li> <li>Use <code>jtop</code> (explained below)</li> </ul> How can I check the Jetson's resource utilization in realtime? <ul> <li>Install <code>jtop</code> (or jetson-stats) to monitor the resource utilization and control.     <pre><code>sudo apt install -y pip3-python\nsudo pip3 install -U jetson-stats\n</code></pre></li> <li><code>6CTRL</code> tab allows you to change the power mode (under <code>NVP modes</code>)</li> </ul> How can I power Jetson with a battery <ul> <li>You can use something like the followings:<ul> <li>USB power bank that supports 12V output via Power Delivery (PD) (example)</li> <li>USB PD to DC power jack adapter cable (example)</li> </ul> </li> </ul>"},{"location":"hackathon.html#resources","title":"Resources","text":"<ul> <li>NVIDIA Developer Forum - Jetson category<ul> <li>Jetson Orin Nano ... Check if your question is already answered, and if not ask here.</li> <li>Jetson AGX Orin</li> <li>Jetson Projects ... See cool projects people built with Jetson!</li> </ul> </li> <li>JetsonHacks ... A great site with lots of tips for using Jetson</li> <li>Hello AI World / jetson-inference</li> <li>Discord of Jetson AI Lab Research Group ... Learn more about this research group here<ul> <li>#jetson-getting-started</li> </ul> </li> </ul>"},{"location":"hello_ai_world.html","title":"Hello AI World","text":"<p>Hello AI World is an in-depth tutorial series for DNN-based inference and training of image classification, object detection, semantic segmentation, and more.  It is built on the <code>jetson-inference</code> library using TensorRT for optimized performance on Jetson. </p> <p></p> <p>It's highly recommended to familiarize yourself with the concepts of machine learning and computer vision before diving into the more advanced topics of generative AI here on the Jetson AI Lab.  Many of these models will prove useful to have during your development.</p> <p></p> <p></p> <p>HELLO AI WORLD &gt;&gt; https://github.com/dusty-nv/jetson-inference</p>"},{"location":"initial_setup_jon.html","title":"Initial Setup Guide for Jetson Orin Nano Developer Kit","text":"<p>Note</p> <p>This guide is to supplement the official Jetson Orin Nano Developer Kit Getting Started Guide.</p> <p>The NVIDIA\u00ae Jetson Orin Nano\u2122 Developer Kit is a perfect kit to start your journey of local generative AI evaluation and development.</p> <p>With the December 2024 software update (JetPack 6.1 (rev.1)), this advanced edge computer delivers up to 70% more performance, making it an even more powerful platform for the era of generative AI.</p> <p>This guide explains the complete flow from opening the box, updating the firmware if needed, flashing the latest JetPack 6.2 image on SD card, and the initial software setup, so that you will be ready for tutorials listed on this site and other AI projects.</p>"},{"location":"initial_setup_jon.html#check-your-inventory","title":"Check your inventory","text":"<p>The following item is needed or highly desired to set up your Jetson Orin Nano Developer Kit. If you don't have them in your inventory, you want to arrange them and return to this guide once they are available.</p> <p>What not come in the box - What you need/want to prepare</p>"},{"location":"initial_setup_jon.html#storage","title":"Storage","text":"<ul> <li> microSD card (64GB or bigger)</li> <li> NVMe SSD (Optional, but highly recommended for following tutorials on this site)</li> </ul>"},{"location":"initial_setup_jon.html#mean-to-access-terminal","title":"Mean to access terminal","text":"<p>You need either of the following set:</p> <ul> <li> DisplayPort cable,  DisplayPort capable monitor and a USB keyboard</li> <li> DisplayPort to HDMI cable and HDMI capable monitor (or TV) and a USB keyboard</li> <li> USB to TTL Serial cable  (Advanced)</li> </ul>"},{"location":"initial_setup_jon.html#alternative-method-sdk-manager","title":"\ud83d\udef8 Alternative method : SDK Manager","text":"<p>In case you have an x86 PC running Ubuntu 22.04 or 20.04, then you can flash your Jetson Orin Nano Developer Kit with the latest firmware and JetPack all at once using NVIDIA SDK Manager.</p> <p>Also, if you wish to not use a microSD card but rather use a large NVMe SSD for the OS and data, then you need to use SDK Manager to flash the latest JetPack on the NVMe SSD.</p> <pre><code>flowchart LR\n    A(start) --&gt; B{Want to ditch microSD&lt;br&gt;and only use NVMe SSD?}\n    B --[YES] --&gt; S[\ud83d\udef8 SDK Manager method]\n    B --[No] --&gt; C{Have x86 PC running&lt;br&gt; Ubuntu 22.04/20.04?}\n    C --[YES] --&gt; S\n    C --[No] --&gt; U[\ud83d\ude80 microSD-only method]\n\n    style S stroke-width:3px, fill:#f3e9f2,stroke:#b544c4\n    style U stroke-width:3px, fill:#d2e9e5,stroke:#0e7a71</code></pre> <p>Click the button below to jump to a page that explains the alternative setup method using SDK Manager if you want to set your Jetson Orin Nano Developer Kit with an NVMe SSD or just want to flash all at once with your Ubuntu PC.</p> <p>\ud83d\udef8 SDK Manager method</p> <p>Otherwise, continue reading on this page for the microSD-only setup method.</p>"},{"location":"initial_setup_jon.html#overall-flow-microsd-only-method","title":"Overall flow (microSD-only method)","text":"<p>Jetson Orin Nano Initial Setup Flowchart (microSD-only method)</p> <pre><code>flowchart\n    A(start) --&gt; B{1\ufe0f\u20e3 Check if Jetson UEFI Firmware&lt;br&gt;is newer than version 36.0}\n    B --[YES] --&gt; O[6\ufe0f\u20e3 Boot with JetPack 6.2 microSD card&lt;br&gt; to schedule firmware update]\n    B --[No] --&gt; C[2\ufe0f\u20e3 Boot with JetPack 5.1.3 microSD card&lt;br&gt;to schedule firmware update]\n    C --&gt; D[3\ufe0f\u20e3 Reboot] --&gt; E{{Firmware update to 5.0 during reboot}}\n    E --&gt; F[4\ufe0f\u20e3 Run QSPI updater] --&gt; G[5\ufe0f\u20e3 Reboot] --&gt; H{{QSPI update during reboot - Firmware 36.4.0}}\n    H --&gt; O\n    O --&gt; P[7\ufe0f\u20e3 Reboot]\n    P --&gt; Q{{Firmware update to 36.4.3 during reboot}}\n    Q --&gt; R[8\ufe0f\u20e3 Unlock super performance]\n    R --&gt; S(\ud83d\udc4d Start developing on JetPack 6.2)\n\n\n    style C fill:#fee\n    style D fill:#DEE,stroke:#333\n    style E stroke-width:2px,stroke-dasharray: 5 5\n    style F stroke-width:4px\n    style G fill:#DEE,stroke:#333\n    style H stroke-width:2px,stroke-dasharray: 5 5\n    style O fill:#fee\n    style P fill:#DEE,stroke:#333\n    style Q stroke-width:2px,stroke-dasharray: 5 5\n    style R fill:#f2d5ff</code></pre> <p>Note that it will undergo a total of three (3) reboot cycles.</p>"},{"location":"initial_setup_jon.html#walk-through-video","title":"Walk-through Video","text":"Click here to expand and watch video"},{"location":"initial_setup_jon.html#1-check-if-jetson-uefi-firmware-version-360","title":"1\ufe0f\u20e3 Check if Jetson UEFI Firmware version &gt; <code>36.0</code>","text":"<p>Your Jetson Orin Nano Developer Kit may have the latest firmware (\"Jetson UEFI firmware\" on QSPI-NOR flash memory) flashed at the factory.</p> <p>If not, we need to go through a set of procedures to upgrade to the latest firmware. (Luckily, we can now do this all just on Jetson, meaning we don't need to use a host Ubuntu PC any more!)</p> <p>So let's first check the version of your Jetson UEFI Firmware. You can take one of the following methods.</p>  Monitor-attached Headless\ud83d\ude01I'm feeling lucky <ol> <li>Connect your monitor and USB keyboard to your developer kit.</li> <li>Turn on the developer kit by plugging in the bundled DC power supply</li> <li>Repeatedly press Esc key on the keyboard, especially after NVIDIA logo boot splash screen first appears on the monitor</li> <li>You should see UEFI setup menu screen</li> <li>Check the third line from the top (below \"Not specified\"), which should be the version number of Jetson UEFI firmware</li> </ol> <p>Below image shows that this Jetson Orin Nano Developer Kit unit has the old UEFI firmware, indicating that the unit first needs its firmware/QSPI updated in order to run JetPack 6.x. </p> <ol> <li> <ul> <li><code>RXD</code> (Pin 3)  Adafruit adaptor cable Green</li> <li><code>TXD</code> (Pin 4)  Adafruit adaptor cable White</li> <li><code>GND</code> (Pin 7)  Adafruit adaptor cable Black</li> </ul> <p>Connect USB to TTL Serial cable onto the following pins on <code>J14</code> \"button\" header of carrier board located under the Jetson module.</p> <p></p> <p>For the detail, refer to Jetson Orin Nano Developer Kit Carrier Board Specification.</p> </li> <li> <p>On your PC, run your console monitor program and open the USB serial port.</p> </li> <li>Power on the developer kit by plugging in the bundled DC power supply</li> <li>On the PC console, repeatedly press Esc key on the keyboard, especially after NVIDIA logo boot splash screen first appears on the monitor</li> <li>You should see UEFI setup menu screen</li> <li>Check the third line from the top (below \"Not specified\"), which should be the version number of Jetson UEFI firmware</li> </ol> <p>Warning</p> <p>Avoid leaving the device in a black screen state for extended periods or attempting to boot repeatedly with an SD card containing an incompatible JetPack version.</p> <p>There is a chance that accumulated boot failures may trigger the L4T launcher to always boot into recovery kernel.</p> <p>If this happens, even after switching to the proper JetPack 5.1.3 SD card, it won't boot from the SD card until we manually change the L4T Launcher setting.</p> <p>Although we don't recommend for the above stated reason, you could skip to 6\ufe0f\u20e3 Boot with JetPack 6.x SD card, and try your luck to see if your Jetson just boots with the JetPack 6.x SD card.</p> <p>If the Ubuntu desktop does not appear within 3 minutes and the screen remains black (as shown below), it indicates that the unit is running an older UEFI firmware version that is incompatible with the JetPack 6.x SD card.</p> <p></p> <p>Turn the unit off, and skip to the next section (2\ufe0f\u20e3 Boot with JetPack 5.1.3 SD card to schedule firmware update).</p>"},{"location":"initial_setup_jon.html#determine-qspi-update-is-necessary-or-not","title":"Determine QSPI update is necessary or not","text":"<p>Attention</p> <p>Select the appropriate tab below based on your firmware version you found in the above step.</p> <p>If you found your Jetson Orin Nano needs its firmware updated to run JetPack 6.x, click \" Firmware &lt; 36.0\" tab, and then additional step 2 to 5 will appear for you to follow.</p> <p>If you know your Jetson Orin Nano has the latest firmware, stay on \" Firmware 36.x\" tab, and skip to the next section (6\ufe0f\u20e3 Boot with JetPack 6.x SD card)</p>  Firmware &lt; 36.0 Firmware 36.x <p>Your Jetson Orin Nano needs its firmware updated in order to make JetPack 6.x SD card work.</p> <p>Perform the following steps (2 to 5).</p> <p>Your Jetson Orin Nano has the latest firmware that is ready for JetPack 6.x SD card.</p> <p>Skip to the next section (6\ufe0f\u20e3 Boot with JetPack 6.x SD card)</p>"},{"location":"initial_setup_jon.html#2-boot-with-jetpack-513-sd-card-to-schedule-firmware-update","title":"2\ufe0f\u20e3 Boot with JetPack 5.1.3 SD card to schedule firmware update","text":"<p>First, we need to run JetPack 5.1.3 in order to let its <code>nvidia-l4t-bootloader</code> package get its bootloader/firmware updater activated, so that the firmware update automatically runs the next time it reboots.</p> <ol> <li> <p>Download SD card image on to your PC</p> <p>On your PC, download JetPack 5.1.3 image for Jetson Orin Nano Developer Kit from the official JetPack 5.1.3 page or from the below direct link button.</p> <p>Warning</p> <p>NVIDIA had updated the JetPack 5.1.3 image on 5/28/2024, as the old version had some issue and the following process did not work.So please download and use the latest image (the new file name is <code>JP513-orin-nano-sd-card-image_b29.zip</code>).</p> <p>Jetson Orin Nano Developer KitJetPack 5.1.3 image</p> </li> <li> <p>Use Balena Etcher to flash image to SD card</p> <p>If you don't have Balena Etcher on your PC, download from Balena official site.</p> <p></p> </li> <li> <p>Insert the flashed microSD card into the slot on Jetson module</p> <p></p> </li> <li> <p>Power-on</p> <p>Turn on the Jetson Orin Nano Developer Kit with JetPack 5.1.3 SD card inserted by plugging in the DC power supply.</p> </li> <li> <p>Complete the initial software setup (<code>oem-config</code>)</p> <p>Info</p> <p>There is a chance that JetPack 5.1.3 SD card does not show any desktop UI on the attached display (the display remains black).</p> <p>It would still schedule the UEFI firmware update in the background, so when you power cycle the unit, most likely it will perform the firmware update on the next boot up.</p> <p>So if this happens, skip to the next step (Step 3\ufe0f\u20e3's bullet point <code>2</code>.).</p> </li> <li> <p>Ensure firmware update is scheduled.</p> <p>Once Jetson boots into Jetson Linux system, a background service automatically runs to schedule a firmware update (if needed) to be performed during the next boot-up process.</p> <p>Once you see the following, or just wait about 5 minutes after powering on to ensure the scheduling is done, reboot.</p>  GUI CUI <p></p> <pre><code>$ sudo systemctl status nv-l4t-bootloader-config\n[sudo] password for jetson:\n\u25cf nv-l4t-bootloader-config.service - Configure bootloader service\n    Loaded: loaded (/etc/systemd/system/nv-l4t-bootloader-config.service; enabled; vendor preset: enabled)\n    Active: inactive (dead) since Fri 2024-05-03 13:36:13 PDT; 1min 57s ago\n    Process: 11439 ExecStart=/opt/nvidia/l4t-bootloader-config/nv-l4t-bootloader-config.sh -v (code=exited, status=0/SUCCESS)\nMain PID: 11439 (code=exited, status=0/SUCCESS)\n</code></pre> </li> </ol>"},{"location":"initial_setup_jon.html#3-reboot-and-observe-firmware-update-to-50","title":"3\ufe0f\u20e3 Reboot and observe firmware update to <code>5.0</code>","text":"<ol> <li> <p>Reboot</p> <p>Reboot your Jetson Orin Nano Developer Kit.</p>  GUI CUI <p>On the Ubuntu desktop click the power icon () and select \"Restart...\".</p> <pre><code>$ sudo reboot\n</code></pre> </li> <li> <p>Observe firmware update</p> <p>You should see the following during the boot up process.</p>  With monitor Headless (serial) <p></p> <p></p> <p>Once done, you will boot into JetPack 5.1.3 (again), with underlying firmware updated to <code>5.0-35550185</code>.</p> </li> </ol>"},{"location":"initial_setup_jon.html#4-run-qspi-updater-package-to-schedule-qspi-update","title":"4\ufe0f\u20e3 Run QSPI Updater package to schedule QSPI update","text":"<p>Now that your UEFI firmware is updated to 35.5.0 ( = JetPack 5.1.3 ), it is capable of updating the entire QSPI content to make it ready for JetPack 6.x.</p> <p>We will run a special tool so that the entire QSPI update is scheduled to run automatically on the next boot-up.</p> <ol> <li> <p>Double-check your firmware version is up to date (<code>35.5.0</code> = JetPack 5.1.3)</p> <p>Once it reboots back into Jetson Linux system, on Jetson terminal, run the following:</p> <pre><code>sudo nvbootctrl dump-slots-info\n</code></pre> <p>You should see something like the following, with the Current version indicating <code>35.5.0</code>.</p> <pre><code>Current version: 35.5.0\nCapsule update status: 0\nCurrent bootloader slot: A\nActive bootloader slot: A\nnum_slots: 2\nslot: 0,             status: normal\nslot: 1,             status: normal\n</code></pre> </li> <li> <p>Install QSPI Updater Debian package to trigger the entire QSPI update</p> <p>Note</p> <p>An Internet connection is required to download the QSPI updater package.</p> <p>On Jetson terminal, run the following:</p> <pre><code>sudo apt-get install nvidia-l4t-jetson-orin-nano-qspi-updater\n</code></pre> <p>Installing the <code>nvidia-l4t-jetson-orin-nano-qspi-updater</code> automatically runs its script to schedule the entire QSPI update to be performed during the next boot process, so that the firmware is ready for JetPack 6.x.</p> </li> </ol>"},{"location":"initial_setup_jon.html#5-reboot-observe-qspi-update-and-power-off","title":"5\ufe0f\u20e3 Reboot, observe QSPI update, and power off","text":"<ol> <li> <p>Reboot</p> <p>Once the QSPI update is scheduled, reboot your Jetson Orin Nano Developer Kit.</p> </li> <li> <p>Observe update</p> <p>You can observe the QSPI update during the boot up process.</p> </li> <li> <p>Power off</p> <p>Once the update is done, it reboots and tries to boot, however it will get stuck UNLESS you change the SD card to JetPack 6.x one.</p> <p>Therefore you should just power off the developer kit simply by disconnecting the DC power supply.</p> <p>Attention</p> <p>This part may look very confusing as neither the attached monitor nor the debug UART shows any explicit message on what action to take next.</p> <p>What is going on here is that the Jetson's firmware (inside the QSPI-NOR flash memory) is now updated, ready for the JetPack 6.x SD card, however it is now incompatible with JetPack 5.1.3 SD card left in the Jetson module's slot, so after the reboot it gets stuck in the boot process.</p> <p>So there is no issue with this boot halt (or endless rebooting).  Simply power off the device and insert the new SD card.</p> </li> </ol>"},{"location":"initial_setup_jon.html#6-boot-with-jetpack-6x-sd-card","title":"6\ufe0f\u20e3 Boot with JetPack 6.x SD card","text":"<p>Once we know the onboard firmware is up-to-date and ready for JetPack 6.x, we can boot Jetson Orin Nano Developer Kit with a microSD card for JetPack 6.2.</p> <ol> <li> <p>Download SD card image on to your PC</p> <p>On your PC, download the latest JetPack 6.x image for Jetson Orin Nano Developer Kit from the official JetPack page or from the below direct link button.</p> <p>Jetson Orin Nano Developer KitJetPack 6.2</p> </li> <li> <p>Use Balena Etcher to flash image to SD card</p> <p>Insert your microSD card into your PC's SD card slot, and use Balena Etcher to flash the SD card with the image you just downloaded.</p> <p>If you don't have Balena Etcher on your PC, download from Balena official site.</p> <p></p> </li> <li> <p>Insert the JetPack 6.2 microSD card into the slot on Jetson module</p> <p></p> </li> <li> <p>Power-on by plugging the DC power supply</p> </li> <li> <p>Complete the initial software setup (<code>oem-config</code>)</p> </li> <li> <p>Ensure firmware update is scheduled.</p> <p>JetPack 6.2 SD card will automatically schedule another firmware/bootloader update in the next (3rd and final) reboot cycle.</p>  GUI CUI <p></p> <pre><code>$ sudo systemctl status nv-l4t-bootloader-config\n[sudo] password for jetson:\n\u25cf nv-l4t-bootloader-config.service - Configure bootloader service\n    Loaded: loaded (/etc/systemd/system/nv-l4t-bootloader-config.service; enabled; vendor preset: enabled)\n    Active: inactive (dead) since Fri 2024-05-03 13:36:13 PDT; 1min 57s ago\n    Process: 11439 ExecStart=/opt/nvidia/l4t-bootloader-config/nv-l4t-bootloader-config.sh -v (code=exited, status=0/SUCCESS)\nMain PID: 11439 (code=exited, status=0/SUCCESS)\n</code></pre> </li> </ol>"},{"location":"initial_setup_jon.html#7-reboot-and-observe-firmware-update-to-3643","title":"7\ufe0f\u20e3 Reboot and observe firmware update to <code>36.4.3</code>","text":"<ol> <li> <p>Reboot your Jetson Orin Nano Developer Kit on JetPack 6.2 SD card to trigger the final firmware update (to <code>36.4.3</code>).</p>  GUI CUI <p>On the Ubuntu desktop click the power icon () and select \"Restart...\".</p> <pre><code>$ sudo reboot\n</code></pre> </li> <li> <p>Observe firmware update</p> <p>You should see the following during the boot up process.</p>  With monitor Headless (serial) <p></p> <pre><code>Jetson System firmware version 36.3.0-gcid-36106755 date 2025-04-25T02:38:25+00:\n\nESC   to enter Setup.\nF11   to enter Boot Manager Menu.\nEnter to continue boot.\n\nUpdate Progress - 10% *****++++++++++++++++++++++++++++++++++++++++++++++++++\n</code></pre> </li> </ol> <p>Once done, you will boot into JetPack 6.2 again, with the underlying firmware updated to <code>36.4.3</code>, which unlock the Super performance.</p>"},{"location":"initial_setup_jon.html#8-unlock-super-performance","title":"8\ufe0f\u20e3 Unlock Super Performance","text":"<p>Attention</p> <p>If your are booting with JetPack 6.1 (rev.1) SD card instead of JetPack 6.2 SD card, you may need to execute the following command after the final login and reboot your device.</p> <p>This ensures that the MAXN performance mode becomes available on your system.</p> <pre><code>sudo rm -rf /etc/nvpmodel.conf\n</code></pre>"},{"location":"initial_setup_jon.html#switch-to-maxn-super-mode","title":"Switch to MAXN SUPER mode","text":"<p>Note that the default power mode on JetPack 6.2 on Jetson Orin Nano Developer Kit is 25W.  To switch to the MAXN SUPER mode and unlock the unregulated performance, follow below steps:</p> <ol> <li>Click on the current power mode (25W) by clicking the NVIDIA icon on the right side of the Ubuntu desktop\u2019s top bar.</li> <li>Select Power mode from the menu.</li> <li>Choose MAXN SUPER to enable maximum performance.</li> </ol> <p></p>"},{"location":"initial_setup_jon.html#start-developing-on-jetpack-62","title":"\ud83d\udc4d Start developing on JetPack 6.2","text":"<p>\ud83c\udf8a Congratulations!  Your Jetson Orin Nano Developer Kit is set up with JetPack 6.2 and you are ready to develop on JetPack 6.2.</p>"},{"location":"initial_setup_jon.html#next-step","title":"Next step","text":""},{"location":"initial_setup_jon.html#nvme-ssd-installation","title":"NVMe SSD installation","text":"<p>Take a look at this page for installing NVMe SSD and setting up Docker with it.</p>"},{"location":"initial_setup_jon_sdkm.html","title":"\ud83d\udef8 Initial Setup using SDK Manager","text":"<p>Note</p> <p>This guide is to supplement the official Jetson Orin Nano Developer Kit Getting Started Guide.</p> <p>The NVIDIA\u00ae Jetson Orin Nano\u2122 Developer Kit is a perfect kit to start your journey of local generative AI evaluation and development.</p> <p>This guide explains the alternative method for setting up Jetson Orin Nano Developer Kit by flashing both the latest firmware (QSPI image) and the latest JetPack all at once, using a host x86 PC.</p>"},{"location":"initial_setup_jon_sdkm.html#check-your-toolbox","title":"Check your toolbox","text":"<p>The following item is needed or highly desired for flashing your Jetson Orin Nano Developer Kit using your host PC.</p> <p>If you don't have them, check the default microSD-only setup instruction or you want to arrange them first then return to this guide once they are available. </p> <p>What not come in the box - What you need/want to prepare</p>"},{"location":"initial_setup_jon_sdkm.html#host-pc","title":"Host PC","text":"<ul> <li> x86 PC running Ubuntu 22.04 or Ubuntu 20.04</li> </ul> <p> Source:  NVIDIA SDK Manager  Host Operating System Compatibility Matrix</p>"},{"location":"initial_setup_jon_sdkm.html#storage","title":"Storage","text":"<p>Either of the following.</p> <ul> <li> microSD card (64GB or bigger) </li> <li> NVMe SSD (Recommended for better performance)</li> </ul> <p> </p>"},{"location":"initial_setup_jon_sdkm.html#flashing-supply","title":"Flashing supply","text":"<ul> <li> USB cable (USB-C plug for Jetson Orin Nano Developer Kit side, other end depends on your PC)</li> <li> jumper pin (or metal paper clip)</li> </ul>"},{"location":"initial_setup_jon_sdkm.html#mean-to-access-terminal","title":"Mean to access terminal","text":"<p>You need either of the following set:</p> <ul> <li> DisplayPort cable,  DisplayPort capable monitor and a USB keyboard</li> <li> DisplayPort to HDMI cable and HDMI capable monitor (or TV) and a USB keyboard</li> <li> USB to TTL Serial cable  (Advanced)</li> </ul>"},{"location":"initial_setup_jon_sdkm.html#default-method-microsd-card-only-setup-method","title":"\ud83d\ude80 Default method : microSD card only setup method","text":"<p>In case you do NOT have an x86 PC running Ubuntu 22.04 or 20.04, you can fall back to the default \"microSD card only\" setup method that does not require any host PC.</p> <p>The decision process can look like this.</p> <pre><code>flowchart LR\n    A(start) --&gt; B{Want to ditch microSD&lt;br&gt;and only use NVMe SSD?}\n    B --[YES] --&gt; S[\ud83d\udef8 SDK Manager method]\n    B --[No] --&gt; C{Have x86 PC running&lt;br&gt; Ubuntu 22.04/20.04?}\n    C --[YES] --&gt; S\n    C --[No] --&gt; U[\ud83d\ude80 microSD-only method]\n\n    style S stroke-width:3px, fill:#f3e9f2,stroke:#b544c4\n    style U stroke-width:3px, fill:#d2e9e5,stroke:#0e7a71</code></pre> <p>Click the button below to jump to the default setup page.</p> <p>\ud83d\ude80 microSD-only method</p> <p>Otherwise, continue reading on this page for the SDK Manager method.</p>"},{"location":"initial_setup_jon_sdkm.html#overall-flow-sdk-manager-method","title":"Overall flow (SDK Manager method)","text":"<p>Jetson Orin Nano Initial Setup Flowchart (microSD-only method)</p> <pre><code>flowchart\n    A(start) --&gt; B[0\ufe0f\u20e3 Install SDK Manager]\n    B --&gt; C[1\ufe0f\u20e3 Connect Target Hardware] \n    C --&gt; D[2\ufe0f\u20e3 Select Software Components to Install]\n    D --&gt; E[3\ufe0f\u20e3 Download on Host PC]\n    E --&gt; F[4\ufe0f\u20e3 Flash]\n    F --&gt; G[5\ufe0f\u20e3 Boot and initial setup]\n    G --&gt; P(\ud83d\udc4d Start developing on JetPack 6.2) </code></pre>"},{"location":"initial_setup_jon_sdkm.html#0-install-sdk-manager","title":"0\ufe0f\u20e3 Install SDK Manager","text":"<p>Reference</p> <p> NVIDIA SDK Manager Documentation :  Download and Install Download and Run SDK Manager</p> <p>On your Ubuntu 22.04 / 20.04 PC, open a terminal and execute the following command.</p> <p>Select the tab that matches your Ubuntu version.</p>  Ubuntu 22.04 Ubuntu 20.04 <pre><code>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install sdkmanager\n</code></pre> <pre><code>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install sdkmanager\n</code></pre> <p>Then launch SDK Manager with the following command.</p> <pre><code>sdkmanager\n</code></pre> First time using SDK Manager <p>When using SDK Manager for the first time, log in with your NVIDIA Developer credentials.</p> <p>Remember to tick the checkbox for \"Stay logged in\" and click \"LOGIN\" button.</p> <p></p> <p>A browser window opens. Input your email address that is registered as NVIDIA Developer.</p> <p></p> <p>Two factor authentication prompt may pop up.</p> <p></p> <p>Once authenticated, the browser windows opens back the SDK Manager app window.</p> <p></p> <p>SDK Manager continues to log you in.</p> <p></p> <p>SDK Manager launched and the login completed.</p> <p></p>"},{"location":"initial_setup_jon_sdkm.html#1-connect-the-target-hardware","title":"1\ufe0f\u20e3 Connect the Target Hardware","text":"<p>Once SDK Manager is launched, connect your Jetson developer kit to your Ubuntu PC and power it on in Forced Recovery mode.</p> <ol> <li> <p>Connect your Jetson Orin Nano Developer Kit to your PC with a USB cable.Note that the USB cable goes into the USB-C port of the carrier board of the developer kit.</p> </li> <li> <p>While shorting <code>pin 9</code> and <code>pin 10</code> of <code>J14</code> header located below the Jetson module using a jumper pin, insert the DC power supply plug into the DC jack of the carrier board to power it on.</p> <p></p> <p>You can use a paper clip like a photo below as well. </p> <p></p> </li> <li> <p>Select \"Jetson Orin Nano [8GB developer kit version]\" and hit \"OK\"</p> <p></p> </li> <li> <p>Uncheck \"Host Machine\" and it should now look like this</p> <p></p> </li> <li> <p>Click \"Continue\" button to proceed to the next step.</p> </li> </ol>"},{"location":"initial_setup_jon_sdkm.html#2-select-software-components-to-install","title":"2\ufe0f\u20e3 Select Software Components to Install]","text":"<ol> <li> <p>Leave the only \"Jetson Linux\" component checked, and uncheck everything.</p> <p></p> </li> <li> <p>Remember to also check \"I accept the terms and conditions of the license agreements\".</p> </li> <li> <p>Click \"Continue\" button to proceed to the next step.</p> </li> <li> <p>It will prompt for the <code>sudo</code> command password.</p> <p></p> </li> </ol>"},{"location":"initial_setup_jon_sdkm.html#3-download-on-host-pc","title":"3\ufe0f\u20e3 Download on Host PC","text":"<ol> <li> <p>It will start downloading the \"BSP\" package and \"RootFS\" package.</p> <p></p> </li> <li> <p>Once downloads are complete, it will untar the package and start generating the images to flash in the background.</p> <p></p> </li> <li> <p>Once images are ready, SDK it will open the prompt for flashing.</p> <p></p> </li> </ol>"},{"location":"initial_setup_jon_sdkm.html#4-flash","title":"4\ufe0f\u20e3 Flash","text":"<ol> <li> <p>On the flashing prompt, select \"Runtime\" for \"OEM Configuration\".</p> <p></p> </li> <li> <p>On the flashing prompt, select \"NVMe\" if you want to flash Jetson Linux (BSP) to NVMe SSD. </p> <p></p> </li> <li> <p>Click \"Flash\" and the prompt popup will change like this.</p> <p></p> </li> <li> <p>Flash progress.</p> DetailsTerminals <p></p> <p></p> </li> <li> <p>Flash successfully completes.</p> DetailsTerminals <p></p> <p></p> </li> </ol>"},{"location":"initial_setup_jon_sdkm.html#5-boot-and-initial-setup","title":"5\ufe0f\u20e3 Boot and Initial Setup","text":"<ol> <li> <p>If still plugged, remove the jumper from header (that was used to put it in Forced Recovery mode)</p> </li> <li> <p>Connect the DisplayPort cable or adapter and USB keyboard and mouse to Jetson Orin Nano Developer Kit, or hook up the USB to TTL Serial cable.</p> </li> <li> <p>Unplug the power supply and put back in to power cycle.</p> </li> <li> <p>Jetson should now boot into the Jetson Linux (BSP) of your selected JetPack version from the storage of your choice.</p> </li> <li> <p>Complete the initial software setup (<code>oem-config</code>)</p> </li> </ol>"},{"location":"initial_setup_jon_sdkm.html#switch-to-maxn-super-mode","title":"Switch to MAXN SUPER mode","text":"<p>Note that the default power mode on JetPack 6.2 on Jetson Orin Nano Developer Kit is 25W.  To switch to the MAXN SUPER mode and unlock the unregulated performance, follow below steps:</p> <ol> <li>Click on the current power mode (25W) by clicking the NVIDIA icon on the right side of the Ubuntu desktop\u2019s top bar.</li> <li>Select Power mode from the menu.</li> <li>Choose MAXN SUPER to enable maximum performance.</li> </ol> <p></p> <p>\ud83c\udf8a Congratulations!  Your Jetson Orin Nano Developer Kit is set up with JetPack 6.2 image and you are ready to develop on JetPack 6.2.</p>"},{"location":"initial_setup_jon_sdkm.html#next-step","title":"Next step","text":"<p>Check this page for \"SSD\" (if you flashed on microSD) and/or \"Docker\" set up.</p>"},{"location":"lerobot.html","title":"HuggingFace LeRobot","text":"<p>Let's run HuggingFace <code>LeRobot</code> to train Transformer-based action diffusion policies and ACT onboard NVIDIA Jetson.  These models learn to predict actions for a particular task from visual inputs and prior trajectories, typically collected during teleoperation or in simulation.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)\u26a0\ufe0f</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 GA (L4T r36.3) JetPack 6.1 (L4T r36.4)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>16.5GB</code> for <code>lerobot</code> container image</li> <li>Space for models (<code>&gt;2GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"lerobot.html#work-with-real-world-robots-before-starting-containers","title":"Work with Real-World Robots - Before starting containers","text":"<p>This section gives the guide on how you can work through the LeRobot official example of Getting Started with Real-World Robots (<code>7_get_started_with_real_robot.md</code>) on your Jetson.</p> <p>Tip</p> <p>It's recommended to work on your Jetson in monitor-attached mode.</p> <p><code>lerobot</code> is designed to show camera view in windows and playback TTS audio while capturing dataset, so it is more convenient to setup your Jetson with its monitor (and speakers) attached to Jetson.d</p>"},{"location":"lerobot.html#a-check-jetson-containers-location","title":"a. Check <code>jetson-container</code>'s location","text":"<p>Through out the course of all the workflows of <code>lerobot</code>, we will be generating a lot of data, especially for capturing dataset.</p> <p>We will clone the <code>lerobot</code> directory on host and mount the directory in the container to keep all the data persistant, but first make sure your <code>jetson-containers</code> directory is placed on your SSD, not on your eMMC or microSD card.</p> <p>If you have created the <code>jetson-containers</code> directory on eMMC or microSD card (likely the case if you first set up your Jetson device without SSD first and later added SSD), then use the <code>rsync</code> command to move the entire directory under SSD mount point.</p> <pre><code>rsync -aHAX /home/jetson/jetson-containers/ /ssd/jetson-containers/\n</code></pre> <p>Then run the installer again.</p> <pre><code>bash /ssd/jetson-containers/install.sh\n</code></pre>"},{"location":"lerobot.html#b-create-lerobot-dir-on-host","title":"b. Create <code>lerobot</code> dir on host","text":"<p>As described above, we will setup the <code>lerobot</code> directory under <code>data</code> directory of <code>jetson-containers</code> for monting it inside the container so that generated data persist.</p> <pre><code>cd jetson-containers\n./packages/robots/lerobot/clone_lerobot_dir_under_data.sh\n./packages/robots/lerobot/copy_overlay_files_in_data_lerobot.sh\n</code></pre>"},{"location":"lerobot.html#c-pulseaudio-setup","title":"c. PulseAudio setup","text":"<p>LeRobot's dataset capture flow (<code>control_robot.py</code>) utilizes Speech Dispatcher to use espeak TTS, in order to give operators audio queues for notifying the status and signaling the next operation. It's actually very helpful.</p> <p>Speech Dispatcher utilizes Pulse Audio, so rather than just sharing the <code>/dev/snd</code> device when <code>docker run</code> (which is good for ALSA), we need to add the following arguments.</p> <pre><code>   --device /dev/snd \\\n   -e PULSE_SERVER=unix:${XDG_RUNTIME_DIR}/pulse/native \\\n   -v ${XDG_RUNTIME_DIR}/pulse:${XDG_RUNTIME_DIR}/pulse \\\n</code></pre> <p>This is already added to <code>run.sh</code> of <code>jetson-containers</code>, however, we need to edit <code>/etc/pulse/default.pa</code> in order to allow the root user access to the socket file.</p> <pre><code>sudo vi /etc/pulse/default.pa\n</code></pre> <p>Find the section loading <code>module-native-protomocl-unix</code> and add <code>auth-anonymous=1</code></p> <pre><code>### Load several protocols\n.ifexists module-esound-protocol-unix.so\nload-module module-esound-protocol-unix auth-anonymous=1\n.endif\nload-module module-nativ\n</code></pre> <p>Then restart PulseAudio service to make the config take effect.</p> <pre><code>pulseaudio --kill\npulseaudio --start\n</code></pre> <p>For troubleshootings or details, please check the <code>docs.md</code> of <code>speech-dispatcher</code> package.</p>"},{"location":"lerobot.html#d-set-udev-rule-for-acm-devices","title":"d. Set udev rule for ACM devices","text":"<p>It is more convenient if the lerobot programs can always find the device of leader and follower arm with unique names.</p> <p>For that, we set an udev rule so that arms always get assigned the same device name as following. This is first done on Jetson host side.</p> <ul> <li><code>/dev/ttyACM_kochleader</code>   : Leader arm</li> <li><code>/dev/ttyACM_kochfollower</code> : Follower arm</li> </ul> <p>First only connect the leader arm to Jetson and record the serial ID by running the following:</p> <pre><code>ll /dev/serial/by-id/\n</code></pre> <p>The output should look like this.</p> <pre><code>lrwxrwxrwx 1 root root 13 Sep 24 13:07 usb-ROBOTIS_OpenRB-150_BA98C8C350304A46462E3120FF121B06-if00 -&gt; ../../ttyACM1\n</code></pre> <p>Then edit the first line of <code>./99-usb-serial.rules</code> like the following.</p> <p>You can find the template of this file under <code>./packages/robots/lerobot</code> directory.</p> <pre><code>SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"2f5d\", ATTRS{idProduct}==\"2202\", ATTRS{serial}==\"BA98C8C350304A46462E3120FF121B06\", SYMLINK+=\"ttyACM_kochleader\"\nSUBSYSTEM==\"tty\", ATTRS{idVendor}==\"2f5d\", ATTRS{idProduct}==\"2202\", ATTRS{serial}==\"00000000000000000000000000000000\", SYMLINK+=\"ttyACM_kochfollower\"\n</code></pre> <p>First copy this under <code>/etc/udev/rules.d/</code> (of host).</p> <pre><code>sudo cp ./99-usb-serial.rules /etc/udev/rules.d/\n</code></pre> <p>Now disconnect the leader arm, and then only connect the follower arm to Jetson. Record the serial for the follower arm.</p> <pre><code>$ ll /dev/serial/by-id/\nlrwxrwxrwx 1 root root 13 Sep 24 13:07 usb-ROBOTIS_OpenRB-150_483F88DC50304A46462E3120FF0C081A-if00 -&gt; ../../ttyACM0\n</code></pre> <p>Repeat the same steps to record the serial to edit the lines of <code>/etc/udev/rules.d/99-usb-serial.rules</code> file.</p> <pre><code>$ sudo vi /etc/udev/rules.d/99-usb-serial.rules\n</code></pre> <p>You should have <code>/etc/udev/rules.d/99-usb-serial.rules</code> now looking like this:</p> <pre><code>SUBSYSTEM==\"tty\", ATTRS{idVendor}==\"2f5d\", ATTRS{idProduct}==\"2202\", ATTRS{serial}==\"BA98C8C350304A46462E3120FF121B06\", SYMLINK+=\"ttyACM_kochleader\"\nSUBSYSTEM==\"tty\", ATTRS{idVendor}==\"2f5d\", ATTRS{idProduct}==\"2202\", ATTRS{serial}==\"483F88DC50304A46462E3120FF0C081A\", SYMLINK+=\"ttyACM_kochfollower\"\n</code></pre> <p>Finally make sure the file is under <code>/etc/udev/rules.d/</code> (of host), and restart Jetson.</p> <pre><code>sudo ls -l /etc/udev/rules.d/99-usb-serial.rules\nsudo reboot\n</code></pre> <p>After reboot, check if we now have achieved the desired fixed simlinks names for the arms.</p> <pre><code>ls -l /dev/ttyACM*\n</code></pre> <p>You should get something like this:</p> <pre><code>crw-rw---- 1 root dialout 166, 0 Sep 24 17:20 /dev/ttyACM0\ncrw-rw---- 1 root dialout 166, 1 Sep 24 16:13 /dev/ttyACM1\nlrwxrwxrwx 1 root root         7 Sep 24 17:20 /dev/ttyACM_kochfollower -&gt; ttyACM0\nlrwxrwxrwx 1 root root         7 Sep 24 16:13 /dev/ttyACM_kochleader -&gt; ttyACM1\n</code></pre>"},{"location":"lerobot.html#e-optional-csi-cameras","title":"e. (Optional) CSI cameras","text":"<p>If you plan to use CSI cameras (not USB webcams) for data capture, you will use the new <code>--csi2webcam</code> options of <code>jetson-containers</code>, which exposes V4L2loopback devices that performs like USB webcams (MJPEG) for CSI cameras using Jetson's hardware JPEG encoder.</p> <p>This feature require some packages to be installed.</p> <pre><code>sudo apt update &amp;&amp; sudo apt install v4l2loopback-dkms v4l-utils\n</code></pre>"},{"location":"lerobot.html#f-increse-the-swap-file-size","title":"f. Increse the swap file size","text":"<p>You may ran out of memory when are setting up to perform ACT model training.</p> <pre><code>swapoff -a -v\nsudo rm /swfile\nsudo systemctl disable nvzramconfi\nsudo fallocate -l 8G /ssd/8GB.swap\nsudo chmod 600 /ssd/8GB.swap\nsudo mkswap /ssd/8GB.swap\nsudo echo \"/ssd/8GB.swap swap swap defaults 0 0\" &gt;&gt; /etc/fstab\nsudo reboot\n</code></pre>"},{"location":"lerobot.html#g-starting-the-lerobot-container","title":"g. Starting the <code>lerobot</code> container","text":"USB webcamsCSI cameras <pre><code>cd jetson-containers\n./run.sh \\\n  -v ${PWD}/data/lerobot/:/opt/lerobot/ \\\n  $(./autotag lerobot)\n</code></pre> <pre><code>cd jetson-containers\n./run.sh \\\n  --csi2webcam --csi-capture-res='1640x1232@30' --csi-output-res='640x480@30' \\\n  -v ${PWD}/data/lerobot/:/opt/lerobot/ \\\n  $(./autotag lerobot)\n</code></pre>"},{"location":"lerobot.html#work-with-real-world-robots-once-in-container","title":"Work with Real-World Robots - Once in container","text":"<p>JupyerLab tip</p> <p>Inside the <code>lerobot</code> container, JupyterLab server process starts.</p> <p>You can access with <code>http://localhost:8888/</code> (or <code>http://&lt;IP_ADDRESS&gt;:8888/</code> from other PC on the same network).</p> <p>In the <code>notebooks</code>, there are some Jupyter notebooks for each segment of the official tutorial Getting Started with Real-World Robots (<code>7_get_started_with_real_robot.md</code>).</p> <p></p> <p>Please note that some of them (like <code>notebooks/7-2_real-robot_configure-motors.ipynb</code>) can be used as a real work notebook to execute python codes and scritps convniently inside the notebook along with instructions (rather than switching to console).</p> <p>However, keep in mind that you are encouraged to always check the original official tutorial, and some operation like training is much better executed on console.</p> <p>Bash history tip</p> <p>Inside the container, on the console, you can press Up key to scroll through some of the frequently used commands pre-registered in bash history.</p>"},{"location":"lerobot.html#q-setup-audio","title":"q. Setup audio","text":"<p>Check if PulseAudio is available.</p> <pre><code>pactl info\n</code></pre> <p>If you need to set the default audio output device, use <code>set-default-sink</code>.</p> <pre><code>pactl list short sinks\npactl set-default-sink [SINK_NAME_OR_INDEX]\n</code></pre>"},{"location":"lerobot.html#1-order-and-assemble-your-koch-v11","title":"1. Order and Assemble your Koch v1.1","text":"<p>You can order the Koch v1.1 kits from ROBOTIS. (Note: they don't come with 3d printed parts)</p> <ul> <li>Follower arm</li> <li>Leader arm</li> </ul> <p>TODO:</p> <ul> <li> Document Jetson unique hardware setup</li> <li> Share custom 3D print models</li> </ul>"},{"location":"lerobot.html#2-configure-motors-calibrate-arms-teleoperate-your-koch-v11","title":"2. Configure motors, calibrate arms, teleoperate your Koch v1.1","text":"<p>Follow the Jupyter notebook <code>7-2_real-robot_configure-motors.ipynb</code>.</p>"},{"location":"lerobot.html#3-record-your-dataset-and-visualize-it","title":"3. Record your Dataset and Visualize it","text":"<p>You should mostly operate on the container's terminal.</p> <p>Follow the official document's section.</p> <p>Camera config tip</p> <p>The official document demonstrates the two camera positions, one at the top (\"phone\") and the other at directly in front facing the arm (\"laptop\").</p> <p>In our trials, this camera placement worked, but we needed to make the camera zoom-up to the scene so that they capture better spacial resolution.</p> <p>Another thing worth experimenting is the wrist cam. More to come later.</p> <p>Tip</p> <p>Following commands are registered in Bash history inside the <code>lerobot</code> container.</p> <pre><code>wandb login\nexport HF_USER=\npython lerobot/scripts/control_robot.py record \\\n  --robot-path lerobot/configs/robot/koch.yaml \\\n  --fps 30 \\\n  --root data \\\n  --repo-id ${HF_USER}/koch_test_$(date +%Y%m%d_%H%M%S) \\\n  --tags tutorial \\\n  --warmup-time-s 5 \\\n  --episode-time-s 30 \\\n  --reset-time-s 30 \\\n  --num-episodes 10\n</code></pre> <p>Tip</p> <p>If you plan to perfom training on a different machine, <code>scp</code> the dataset directory.</p> To another JetsonTo other PC <pre><code>scp -r data/lerobot/data/${HF_USER}/koch_test_01/ &lt;USER&gt;@&lt;IP&gt;:/ssd/jetson-containers/data/lerobot/data/${HF_USER}/\n</code></pre> <pre><code>scp -r data/lerobot/data/${HF_USER}/koch_test_01/ &lt;USER&gt;@&lt;IP&gt;:/home/&lt;USER&gt;/lerobot/data/${HF_USER}/\n</code></pre>"},{"location":"lerobot.html#4-train-a-policy-on-your-data","title":"4. Train a policy on your data","text":"<p>You should operate on ther container's terminal.</p> <p>Follow the official document's section.</p> <p>Tip</p> <pre><code>wandb login\nDATA_DIR=data python lerobot/scripts/train.py \\\n    dataset_repo_id=${HF_USER}/koch_test \\\n    policy=act_koch_real \\\n    env=koch_real \\\n    hydra.run.dir=outputs/train/act_koch_test \\\n    hydra.job.name=act_koch_test \\\n    device=cuda \\\n    wandb.enable=true\n</code></pre> <p>Tip</p> <p>If you perform the training on other Jetson or PC, <code>scp</code> the outputs directory content back to the orinal Jetson that has the leader and follower arm attached.</p> <pre><code>scp -r outputs/train/act_koch_test_01/ &lt;USER&gt;@&lt;IP&gt;:/ssd/jetson-containers/data/lerobot/outputs/train/\n</code></pre>"},{"location":"lerobot.html#5-evaluate-your-policy","title":"5. Evaluate your policy","text":"<p>You should operate on the container's terminal.</p> <p>Follow the official document's section.</p> <p>Tip for a. Use <code>koch.yaml</code> and our <code>record</code> function</p> <p>Modify the command in the bash history to add <code>-p</code> arugment to points to the policy checkpoint.</p> <pre><code>python lerobot/scripts/control_robot.py record \\\n  --robot-path lerobot/configs/robot/koch.yaml \\\n  --fps 30 \\\n  --root data \\\n  --repo-id ${HF_USER}/eval_koch_test_01 \\\n  --tags tutorial eval \\\n  --warmup-time-s 5 \\\n  --episode-time-s 30 \\\n  --reset-time-s 30 \\\n  --num-episodes 10 \\\n  -p outputs/train/act_koch_test/checkpoints/last/pretrained_model \\\n  --run-compute-stats 0\n</code></pre> <p>Tip for Visualize evaluation afterwards</p> <pre><code>python lerobot/scripts/visualize_dataset.py \\\n  --root data \\\n  --repo-id ${HF_USER}/eval_koch_test\n</code></pre> <p></p> <p>If everything goes well, you should see</p>"},{"location":"lerobot.html#basic-walkthrough","title":"Basic Walkthrough","text":"<p>This is from the lerobot top README.md.</p>"},{"location":"lerobot.html#visualize-datasets","title":"Visualize Datasets","text":"<p>Outside of container, first launch the rerun.io visualization tool that LeRobot uses <sup>[\u2197]</sup></p> <pre><code>pip3 install rerun-sdk\nrerun\n</code></pre> <p>Then, start the docker container to playback one of these LeRobot datasets.</p> <pre><code>jetson-containers run -w /opt/lerobot $(autotag lerobot) \\\n  python3 lerobot/scripts/visualize_dataset.py \\\n    --repo-id lerobot/pusht \\\n    --episode-index 0\n</code></pre> <p></p>"},{"location":"lerobot.html#evaluate-a-pretrained-diffusion-policy","title":"Evaluate a Pretrained Diffusion Policy","text":"<p>This will download and run a pre-trained diffusion model on the PushT environment <sup>[\u2197]</sup></p> <pre><code>jetson-containers run -w /opt/lerobot $(autotag lerobot) \\\n  python3 lerobot/scripts/eval.py \\\n    -p lerobot/diffusion_pusht \\\n    eval.n_episodes=10 \\\n    eval.batch_size=10\n</code></pre>"},{"location":"lerobot.html#train-your-own-act-policy","title":"Train your own ACT Policy","text":"<p>Next, train ACT on the Aloha manipulation environment <sup>[\u2197]</sup></p> <pre><code>jetson-containers run -w /opt/lerobot $(autotag lerobot) \\\n  python3 lerobot/scripts/train.py \\\n    policy=act \\\n    env=aloha \\\n    env.task=AlohaInsertion-v0 \\\n    dataset_repo_id=lerobot/aloha_sim_insertion_human\n</code></pre> <p>See Trossen Robotics for dual-arm ALOHA kits, and Robotis for the low-cost Koch v1.1 kit used in the LeRobot tutorial:</p> <p></p> <p>HuggingFace LeRobot - Get Started with Real-World Robots (YouTube Playlist)</p>"},{"location":"llama_vlm.html","title":"Llama 3.2 Vision","text":"<p>The latest additions to Meta's family of foundation LLMs include multimodal vision/language models (VLMs) in 11B and 90B sizes with high-resolution image inputs (1120x1120) and cross-attention with base completion and instruction-tuned chat variants:</p> <ul> <li><code>Llama-3.2-11B-Vision</code></li> <li><code>Llama-3.2-11B-Vision-Instruct</code></li> <li><code>Llama-3.2-90B-Vision</code></li> <li><code>Llama-3.2-90B-Vision-Instruct</code></li> </ul> <p>While quantization and optimization efforts are underway, we have started with running the unquantized 11B model in a container based on HuggingFace Transformers that has been updated with the latest support for Llama-3.2-Vision a jump start on trying out these exciting new multimodal models - thanks to Meta for continuing to release open Llama models!</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>12.8GB</code> for <code>llama-vision</code> container image</li> <li>Space for models (<code>&gt;25GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> <li> <p>Request access to the gated models here with your HuggingFace API key.</p> </li> </ol>"},{"location":"llama_vlm.html#code-example","title":"Code Example","text":"<p>Today Llama-3.2-11B-Vision is able to be run on Jetson AGX Orin in FP16 via HuggingFace Transformers.  Here's a simple code example from the model card for using it: </p> <pre><code>import time\nimport requests\nimport torch\n\nfrom PIL import Image\nfrom transformers import MllamaForConditionalGeneration, AutoProcessor\n\nmodel_id = \"meta-llama/Llama-3.2-11B-Vision\"\nmodel = MllamaForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16)\nprocessor = AutoProcessor.from_pretrained(model_id)\n\nprompt = \"&lt;|image|&gt;&lt;|begin_of_text|&gt;If I had to write a haiku for this one\"\nurl = \"https://llava-vl.github.io/static/images/view.jpg\"\nraw_image = Image.open(requests.get(url, stream=True).raw)\n\ninputs = processor(text=prompt, images=raw_image, return_tensors=\"pt\").to(model.device)\noutput = model.generate(**inputs, do_sample=False, max_new_tokens=32)\n</code></pre> <p></p> <pre><code>If I had to write a haiku for this one, it would be:\n\nA dock on a lake.\nA mountain in the distance.\nA long exposure.\n</code></pre> <p>Initial testing seems that Llama-3.2-Vision has more conversational abilities than VLMs typically retain after VQA alignment.  This llama_vision.py script has interactive completion and image loading to avoid re-loading the model.  It can be launched from the container like this:</p> <pre><code>jetson-containers run \\\n    -e HUGGINGFACE_TOKEN=YOUR_API_KEY \\\n    $(autotag llama-vision) \\\n      python3 /opt/llama_vision.py \\\n        --model \"meta-llama/Llama-3.2-11B-Vision\" \\\n        --image \"/data/images/hoover.jpg\" \\\n        --prompt \"I'm out in the\" \\\n        --max-new-tokens 32 \\\n        --interactive\n</code></pre> <p>After processing the initial image, it will ask you to submit another prompt or image:</p> <pre><code>total 4.8346s (39 tokens, 8.07 tokens/sec)\n\nEnter prompt or image path/URL:\n\n&gt;&gt; \n</code></pre> <p>We will update this page and container as support for the Llama-3.2-Vision architecture is added to quantization APIs like MLC and llama.cpp for GGUF, which will reduce the memory and latency.</p>"},{"location":"nerf.html","title":"Nerfstudio - Neural Reconstruction","text":"<p>Nerfstudio is a tool for training and visualizing Neural Radiance Fields (NeRF) models for 3D volumetric reconstruction from a collection of images taken of a scene or environment. It's designed to be easy to use and starts generating imagery at the outset, which becomes further refined as training progresses. Neural reconstruction is being applied to edge devices and robotics like with FruitNeRF. This tutorial will guide you through the process of training high-quality NeRF models using nerfstudio on Jetson.</p> <p></p> <p>Special thanks to Johnny N\u00fa\u00f1ez Cano for porting the NeRF stack to Jetson! See NeRF in 2023: Theory and Practice by It-Jim for useful background info.</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>17.6GB</code> for <code>nerfstudio</code> container image</li> <li>Space for models and datasets (<code>&gt;5GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"nerf.html#start-container","title":"Start Container","text":"<p>Use this command to automatically run, build, or pull a compatible container image for nerfstudio:</p> <p><pre><code>jetson-containers run $(autotag nerfstudio)\n</code></pre> To mount your own directories into the container, use the <code>-v</code> or <code>--volume</code> flags:</p> <pre><code>jetson-containers run -v /path/on/host:/path/in/container $(autotag nerfstudio)\n</code></pre> <p>Later a webserver will be reachable at <code>http://localhost:7007</code>.  The official docs include command-line usage and models <sup>[\u2197]</sup></p>"},{"location":"nerf.html#training-your-first-model","title":"Training Your First Model","text":"<p>It's recommended to train a test scene first by following these steps from the getting started tutorial:</p> <pre><code># Download some test data\nns-download-data nerfstudio --capture-name=poster\n\n# Train model\nns-train nerfacto --data /data/nerfstudio/models\n</code></pre> <p>After it loads, you can navigate your browser to <code>http://localhost:7007</code> (or substitute your Jetson's IP address)</p> <p></p> <p>The 3D visualizations will begin rendering after the first training step, and progressively become more refined.  This process typically takes an hour on Jetson AGX Orin to complete the default setting of 30,000 training steps.  The model checkpoints are saved under the mounted <code>jetson-containers/data/nerfstudio</code> directory, and generate the same output quality as before.</p>"},{"location":"nerf.html#fruitnerf","title":"FruitNeRF","text":"<p>The creators behind FruitNeRF fused NeRFs with segmentation for more accurate counting of fruits in 3D.  By training a semantic neural radiance field on both RGB and segmentation labels from SAM, the model is robust to occlusions, double-counting, and mistaken or undesirable fruits. </p> <p>Extended sampling of the volumetric grid can then blob objects for pose estimation, occupancy mapping, and navigation. Such approaches are promising for combining the strengths of NeRFs for 3D reconstruction, segmentation VITs for open-vocabulary classification, and Jetson's onboard compute for high-definition environmental scanning and perception while in the field.</p>"},{"location":"nerf.html#data-preparation","title":"Data Preparation","text":"<p>We'll use the FruitNeRF datasets as an example of training NeRFs on custom data with nerfstudio.  To train a NeRF model on a different scene, you first need to capture a set of images and corresponding camera poses (which can be estimated with the included photogrammetry and image registration tools like COLMAP and OpenSFM as shown here)</p> <p>The images should be taken from different viewpoints of the scene you want to model, structured in the nerfstudio dataset format.  For FruitNeRF, you can just download and extract pre-recorded real or synthetic data from here:</p> Real DataSynthetic data <pre><code>cd /data/nerfstudio\nwget https://zenodo.org/records/10869455/files/FruitNeRF_Real.zip\nunzip FruitNeRF_Real.zip\nmv FruitNeRF_Dataset/tree_01/semantics_sam FruitNeRF_Dataset/tree_01/semantics\n</code></pre> <pre><code>cd /data/nerfstudio\nwget https://zenodo.org/records/10869455/files/FruitNeRF_Synthetic.zip\nunzip FruitNeRF_Synthetic.zip\nmv FruitNeRF_Dataset/tree_01/semantics_sam FruitNeRF_Dataset/tree_01/semantics\n</code></pre>"},{"location":"nerf.html#training-a-fruitnerf-model","title":"Training a FruitNeRF Model","text":"<p>This command will generate a NeRF on the first tree (there are multiple trees to try from each dataset).  You can find the complete usage information and models on the official GitHub repo: <code>github.com/meyerls/FruitNeRF</code></p> <pre><code>ns-train nerfacto \\\n  --data /data/nerfstudio/FruitNeRF_Dataset/tree_01 \\\n  --output-dir /data/nerfstudio/models \\\n  --pipeline.datamanager.camera-res-scale-factor 0.5\n</code></pre> <p>The memory usage depends on the number of images in the dataset and their resolution, so if you're running low on memory try adjusting the scaling factors and image sampling parameters. The simulated data has a lower resolution and can run at full size.</p> <p></p>"},{"location":"openvla.html","title":"OpenVLA - Vision/Language Action Models for Embodied Robotics","text":"<p>Fine Tuning and Deployment Guide</p> <p>The tutorials's goal is to provide optimized quantization and inference for deploying VLA models, along with reference fine-tuning workflows for adapting models for new robots, tasks, and environments.  Rigorous performance and accuracy validation is applied in a self-contained sim environment with scenario generation and domain randomization (MimicGen).  Future phases will include sim2real with Isaac Lab and ROS2 integration, study of related models like CrossFormer and optimizations to the neural architecture for realtime performance. </p> <p>\u2705 Quantization and inference optimizations for VLA models \u2705 Accuracy validation of the original OpenVLA-7B weights \u2705 Reference fine-tuning workflow with synthetic data generation \u2705 On-device training with LoRA's on Jetson AGX Orin and full fine-tuning on A100/H100 instances \u2705 85% accuracy on an example block-stacking task with domain randomization \u2705 Sample datasets and test models for reproducing results </p> <p>Thank you to OpenVLA, Open X-Embodiment, MimicGen, Robosuite and many others with related work for sharing their promising research, models, and tools for advancing physical AI and robotics.</p>"},{"location":"openvla.html#vla-architecture","title":"VLA Architecture","text":"<p>OpenVLA is a vision/language action model for embodied robotics and behavioral learning built on LLM/VLMs (this base model is a Prismatic VLM using Llama-7B, DINOv2, and SigLIP).  Instead of image captioning or visual question/answering, VLA models generate action tokens from camera images and natural language instructions that are used for controlling the robot.  Action tokens are discrete token ID's reserved from the text tokenizer's vocabulary that map to continuous values, normalized against the range of motion of each robot. These real-valued tokens are more efficient and accurate than the model outputting numerical data as text in JSON or Pydantic formats, where each digit, decimal point, separator, and whitespace takes an additional token to generate.  Other hybrid vision/language models like Florence-2 have adopted similar approaches for continuous-domain prediction using Transformers.</p> <p>Each action token generated by the model represents a degree-of-freedom of the output coordinate space (i.e. xyz, rotation pose), or a component of the robot that can be controlled (like the gripper). OpenVLA-7B was trained on the Open X-Embodiment dataset for manipulation, with a 7-DoF action space consisting of <code>(delta xyz, delta roll/pitch/yaw, gripper)</code>.  The position and rotation are relative changes to the end-effector (EEF) pose, with an external inverse kinematics (IK) solution like cuMotion solving joint constraints specific to each robotic arm.  The gripper dimension is an absolute control between 0 (open) and 1 (closed) that does not recieve further scaling/normalization.</p> <p></p> <p>OpenVLA reserves 256 of the least-frequently used tokens out of the Llama-7B vocabulary for action values, which gives it 8-bit resolution over the controls.  It has an input image resolution of 224x224 to stacked DINOv2/SigLIP vision encoders that are projected to ~275 input tokens (plus the text prompt), and outputs 7 tokens mapped to <code>(\u0394pos, \u0394rotation, gripper)</code> coordinates.</p>"},{"location":"openvla.html#quantization","title":"Quantization","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>22GB</code> for <code>nano_llm</code> container image</li> <li>Space for models and datasets (<code>&gt;15GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol> <p>Support for OpenVLA has been added to NanoLLM on top of its streaming VLM pipeline with INT4/FP8 quantization using MLC and vision encoders in FP16 with TensorRT.  First we'll test the model on BridgeData V2, one of the top weighted datasets from the Open X-Embodiment collection.  The model was trained on this data and is used to confirm that the quantization and inference are working correctly during deployment.  This is what the dataset looks like, courtesy of their website:</p> <p>The following command starts the container, downloads the dataset and model (if needed), quantizes it on the first run, and measures the accuracy of the action values against the groundtruth from the dataset using normalized mean-squared error (NRMSE) to unbias the varying ranges each dimension of the action space can have.  We extracted a 100-episode subset of the original Bridge data here on HuggingFace Hub, so you don't need to download the entire ~400GB dataset just for these tests.</p> INT4FP8FP16 <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.vision.vla --api mlc \\\n    --model openvla/openvla-7b \\\n    --quantization q4f16_ft \\\n    --dataset dusty-nv/bridge_orig_ep100 \\\n    --dataset-type rlds \\\n    --max-episodes 10 \\\n    --save-stats /data/benchmarks/openvla_bridge_int4.json\n</code></pre> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.vision.vla --api mlc \\\n    --model openvla/openvla-7b \\\n    --quantization q8f16_ft \\\n    --dataset dusty-nv/bridge_orig_ep100 \\\n    --dataset-type rlds \\\n    --max-episodes 10 \\\n    --save-stats /data/benchmarks/openvla_bridge_fp8.json\n</code></pre> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.vision.vla --api hf \\\n    --model openvla/openvla-7b \\\n    --dataset dusty-nv/bridge_orig_ep100 \\\n    --dataset-type rlds \\\n    --max-episodes 10 \\\n    --save-stats /data/benchmarks/openvla_bridge_fp16.json\n</code></pre> Quantization Accuracy Latency FPS FP16 95.3% 840 ms 1.19 FP8 95.2% 471 ms 2.12 INT4 90.1% 336 ms 2.97 <p>These results were run on Jetson AGX Orin 64GB with JetPack 6, and we will see later with our fine-tuned model the INT4 accuracy match FP8/FP16.</p> <p>Each frame, the 7D action vector predicted by the model is printed along with the groundtruth, along with the accuracy, latency, and framerate for that frame.  The numbers printed after <code>~</code> are the averages of those so far, with the last value reported being the mean over the entire dataset processed.  </p> <pre><code># INT4\nstep 355  [-0.02692  0.00776 -0.00299  0.08160  0.07292  0.04791  0.99608]  accuracy 0.8466 ~0.9017  time=336.2 ms  fps=2.96 ~2.97\ngt   355  [-0.02387  0.00760 -0.00318  0.15965  0.07707  0.03281  1.00000]\n\n# FP8\nstep 355  [-0.02392  0.00767 -0.00310  0.08160  0.07692  0.03217  0.99608]  accuracy 0.9982 ~0.9523  time=469.7 ms  fps=2.13 ~2.12\ngt   355  [-0.02387  0.00760 -0.00318  0.15965  0.07707  0.03281  1.00000]\n\n# FP16\nstep 355  [-0.02392  0.00767 -0.00310  0.08160  0.07692  0.03217  0.99608]  accuracy 0.9982 ~0.9531  time=842.3 ms  fps=1.19 ~1.18\ngt   355  [-0.02387  0.00760 -0.00318  0.15965  0.07707  0.03281  1.00000]\n</code></pre> <p>The per-frame metrics and averages can be saved with the <code>--save-stats</code> argument, and in the interests of time you can cap the amount of episodes processed with <code>--max-episodes</code>.  As mentioned above, the Bridge dataset used was included in the training dataset, and further below we run this again on data we generated not from the training dataset with significant variation. This tool can also load other datasets in RLDS/TFDS format from Open X-Embodiment, and HDF5 from Robomimic/MimicGen.  You can also create your own agents and scripts using the exposed APIs from the coding examples below.</p>"},{"location":"openvla.html#inference-api","title":"Inference API","text":"<p>The code is simple for running VLA inference on camera streams using the NanoLLM library in the container:</p> VLA on Video<pre><code>from nano_llm import NanoLLM\nfrom nano_llm.plugins import VideoSource\n\n# load vision/language action model\nmodel = NanoLLM.from_pretrained(model, quantization='q4f16_ft')\ncamera = VideoSource(video_source, cuda_stream=0)\n\nassert(model.vla)  # make sure this is a VLA\n\nwhile True:\n    # returns a cudaImage, np.ndarray, or torch.Tensor on the GPU\n    image = camera.capture()\n\n    if image is None: # in case of timeout, keep trying\n        continue\n\n    # returns a np.ndarray or torch.Tensor with vla.dof elements\n    # for OpenVLA, this is (\u0394x, \u0394y, \u0394z, \u0394roll, \u0394pitch, \u0394yaw, gripper)\n    actions = model.vla.predict_action(\n        image, \n        instruction=\"pick up the nearest object\", \n        action_space=\"normalized\",\n        return_tensors='np',\n    )\n\n    # send the actions to your robot controller or IK solver\n    ...\n</code></pre> <p>VLA models are also supported in Agent Studio, which includes the simulator components as well.</p>"},{"location":"openvla.html#online-validation","title":"Online Validation","text":"<p>Given the challenging task domain, dynamic feedback loops, and computational demands for sim/training/inference, using VLAs for language-guided dexterous manipulation involves a significant increase in complexity over baseline usage of LLMs and VLMs.  To go from predicting logits at the token level to actions consistently correct enough over an extended series of frames to form useful behaviors, it's important to cross-check outputs and measure accuracy at each stage of the training/inference workflow to be able to identify the source of potential regressions when they occur.  </p> <p>Unlike typical applications in supervised learning, the metrics for end-task completion and success aren't measured from static pre-recorded datasets that don't account for the temporal domain and feedback from physical interactions along with compounding errors - they require online validation, either in simulation or real-world tests.  </p> <p> Closing the Sim-to-Real Gap: Training Spot Quadruped Locomotion with NVIDIA Isaac Lab</p> <p>During training the token classification accuracy is measured from the groundtruth action labels (i.e. how many action tokens were predicted exactly right), with the model optimizing to minimize this loss (as is normal for LLMs).  Action accuracy in the continuous domain is also is also measured during training from the L1 error of the detokenized real-valued outputs.  Continuous action accuracy trends slightly higher than token classification accuracy, as the later does not provide any reward for being closer to the desired result.  In practice, these should be &gt;95% accurate at this level for completing tasks successfully in similar environments.  To achieve that high degree of accuracy, it seems intentional in the work and related research to overfit the model by training it for many epochs (upwards of 30 epochs on the same 900K episodes for OpenVLA).  Transformers are known to recall specific knowledge from few training examples, and are sensitive to overfitting and forgetting previously learned information.  As such, LLMs are normally only trained for a few epochs at most to preserved their zero-shot capabilities and ability to generatize to out-of-distribution inputs.  During the fine-tuning part of this project, we characterize the impacts on model accuracy and task success from the number of distinct training episodes versus the number of epochs over repeated data.</p> <p>The actual task success rate doesn't get measured until the inference stage, when it is either connected to a simulator or physically tested in a series of time-consuming trials under similar conditions.  We integrated MimicGen directly with the OpenVLA training scripts for an endless source of unseen data, but encountered gradient instabilities after the model had received a significant number of episodes.</p>"},{"location":"openvla.html#simulation-with-mimicgen","title":"Simulation with MimicGen","text":"<p>MimicGen creates randomized episodes from as few as 10 teleoperated examples by utilizing scene graph information and task/subtask metadata about which objects in the environment are targets of the current subtask, in order to interpolate the original teloperated trajectories into their new random locations and poses.  This generates large amounts of unique training data to improve robustness, without needing large amounts of human effort for the robot learning new skills and behaviors.    </p> <p>MimicGen is built on the Robomimic and Robosuite simulators and are able to run onboard Jetson headlessly alongside the VLA, simplifying the setup for reproducibility.  The RoboCasa project is built on MimicGen and being integrated with NVIDIA Omniverse and OSMO, and in future work we'd use Isaac Lab for scalability, more accurate physics, and photorealistic rendering.</p> <p>MimicGen includes 12 tasks like block stacking, pick and place, assembly, and kitchen scenarios.  And each type of task has variants increasing in difficulty as learning progresses, which would be interesting to compare curated approaches to the purely random sequencing that OpenVLA uses with Open X-Embodiment.  In this phase of the tutorial, we focus on the block stacking task to understand the training requirements and runtime performance needed to master a new task with success rates of &gt;75-80%, similar to the paper.  This will help inform scaling to multiple behaviors and more complex scenarios that vary significantly from in-distribution examples like the MimicGen environments (as evidenced by the original OpenVLA weights scoring zero successes in them).</p>"},{"location":"openvla.html#data-generation","title":"Data Generation","text":"<p>We built MimicGen containers for Jetson from a fork of the code with some patches for aarch64+igpu along with enhancements like generation of natural language labels with random variations for the relevant tasks, along with additional domain randomization for the colors/materials of objects (these environments were added as <code>Stack_D2</code>, <code>Stack_D3</code>, and <code>Stack_D4</code>).  For training OpenVLA, the images and labels are saved to disk, whereas later inference is done with online simulation to measure the task success rate.  To that effect we integrated MimicGen with Agent Studio for interactively testing the models and quickly dropping in components like ASR for verbally commanding the robot.</p> <p>Online Training</p> <p>There's initial support for direct integration of MimicGen in this fork of OpenVLA for live simulation and validation during training and endless episodes without repeating epochs.  The models experienced spiking gradients later into LoRA's, and should try again with lower learning rates or by similarly integrating MimicGen into their full fine-tuning script using FDSP for increasing the batch size on dGPU.  </p> <p>This command will generate the specified number of training episodes, saved in Robomimic HDF5 format. We provide the rendered datasets for these on HuggingFace Hub with 1000 and 2500 episodes.  OpenVLA suggests only needing 10-150 episodes for fine-tuning and data-efficient adaptation, which perhaps performs similarly in comparable spaces, but we ultimately found insufficient for the MimicGen environments.   </p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m mimicgen.generate \\\n      --tasks Stack_D4 \\\n      --episodes 100 \\\n      --output /data/datasets/mimicgen \\\n      --cameras agentview \\\n      --camera-width 224 \\\n      --camera-height 224\n</code></pre> <p>The HDF5 dataset will be saved to <code>/data/datasets/mimicgen/demo_src_stack_task_D4/demo.hdf5</code> (which is in a mounted volume under your <code>jetson-containers/data</code> directory outside of the container), along with a video of sample episodes that were rendered:</p> <p>This video is actually of Stack_D2 to avoid subjecting everyone to flickering colors.  Stack_D4 is used for training and generates blocks with random colors and positions each frame, along with language labels augmented through the random combination of various nouns, adjectives, and verbs that form the instruction (<code>Stack the red block on the green block</code>, <code>Put the little cube on top</code>).  Stack_D3 randomizes colors/positions each frame, and instructions each episode.  Stack_D2 does them all per-episode (which is typically used at runtime). Since OpenVLA uses a single frame at a time with no temporal aspect during training, applying domain randomization per-frame as opposed to per-episode is feasible provides more variance in the dataset.  The block-stacking episodes typically come out to be around ~110 frames each, and take around 10-15 seconds to generate per episode on Jetson AGX Orin with per-frame domain randomization, and 5 seconds per episode without domain randomization.</p> <p>The agentview camera looks onward from the front of the scene towards the robot. There are others available like sideview and eye_in_hand (wrist view) - we tried using the onboard wrist camera, but found the model would too easily veer off track and get 'lost' offscreen.  It may be possible for wrist-only to work should the dataset add examples of the robot recovering and returning to a wider vantage point.  Other VIT-based embodied models like Octo and CrossFormer use both cameras, and is a future experiment with VLA's based on multi-image VLM's like VILA.</p>"},{"location":"openvla.html#rlds-conversion","title":"RLDS Conversion","text":"<p>OpenVLA uses datasets in RLDS format (which is based on TFDS), so we provide a converter from HDF5.  This extra step can also be time-consuming for a large number of epiodes, like those used here.  This is one of the reasons we desire to run MimicGen online with training and performed the initial integration directly with OpenVLA.  Unless you are generating different data, you can skip this and use the MimicGen datasets that we uploaded here in RLDS format.</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.datasets \\\n        --dataset /data/datasets/mimicgen/demo_src_stack_task_D4/demo.hdf5 \\\n        --dataset-type mimicgen \\\n        --convert rlds \\\n        --remap-keys agentview:image \\\n        --output /data/datasets/mimicgen/rlds/stack_d4_ep2500\n</code></pre> <p>This will create a set of tfrecord files under the output directory that are able to be loaded by the OpenVLA training scripts.</p>"},{"location":"openvla.html#fine-tuning","title":"Fine Tuning","text":"<p>A primary objective of this project is to characterize the training needed to adapt the model to different robots and tasks.  Our development primarily consisted of running test LoRA's onboard Jetson AGX Orin 64GB and debugging issues locally, and when the results were encouraging to perform a full fine-tuning with FDSP on multiple A100/H100's from spot instance providers like Brev.dev, Vast.ai, and RunPod.  Full fine-tuning on 2x Jetson AGX Orin's was attempted with FDSP, but ran out of memory with all settings that we tried. We provide the test models we trained on HuggingFace for others to try in the inference + sim setup below.  Below are the training GPU configurations that were used, along with their batch sizes that maximized memory usage:</p> Batch Size FPS $/hr Jetson AGX Orin 64GB 8 (LoRA) 1.57 - 2x A100 SMX4 80GB 48 13.07 ~$1.50 8x H100 NVL 94GB 256 92.4 ~$25 <p>The rental fees are ballpark averages over the spot instances available with these GPUs at the time of writing, and becomes quite reasonable when used alongside a Jetson repurposed for training daily test LoRA's on a reduced amount of data.  Training until convergence on Jetson and 2xA100 took roughly 24-36 hours depending on the amount of data and number of epochs.  We kept to &lt;5 epochs for the full fine-tunes in an attempt to prevent the afformentioned overfitting, instead opting to increase the number of episodes.</p> <p>Below we provide the steps to run the OpenVLA LoRA training on Jetson, and for the dGPU systems refer to Fully Fine-Tuning OpenVLA.  Typically you will launch a spot instance with your provider of choice in a CUDA or PyTorch container, then install the OpenVLA repo and its dependencies with pip, and download your dataset to the system before launching the command (or create a bundled container with it all included to save time).  Here's the WandB Dashboard from the full fine-tuning runs that you can inspect, comparing a fewer number of episodes for more epochs, versus a larger number of episodes trained for fewer epochs:</p>"},{"location":"openvla.html#on-device-lora","title":"On-Device LoRA","text":"<p>The OpenVLA repo provides working training scripts for LoRA/qLoRA and multi-node multi-GPU full fine-tunes using PyTorch FDSP.  It was not difficult to go in a make changes and enhancements, some of which we have done for our purposes of on-device training in this fork.  Overall we found the process to be more similar than not to training other vision DNNs, just with larger datasets and rigorous validation required of the data pipeline that all the coordinate spaces and transformations matched up at every step of the sim\u2192training\u2192inference workflow.</p> <p>We built an OpenVLA container for JetPack that runs the LoRA training, which you can find the specific documentation about from the OpenVLA readme (it's also recommended to read their paper which includes many insights into the training process).</p> <pre><code>jetson-containers run $(autotag openvla) \\\n  torchrun --standalone --nnodes 1 --nproc-per-node 1 vla-scripts/finetune.py \\\n      --vla_path openvla/openvla-7b \\\n      --data_root_dir /data/datasets/mimicgen/rlds \\\n      --dataset_name stack_d4_ep2500 \\\n      --run_root_dir /data/models/openvla \\\n      --lora_rank 32 \\\n      --batch_size 8 \\\n      --grad_accumulation_steps 2 \\\n      --learning_rate 5e-4 \\\n      --image_aug False \\\n      --save_steps 250 \\\n      --epochs 5\n</code></pre> <p>This will start a TensorBoard server on port 6006 to monitor the training progress.  Typically you would set the script running for more epochs than you intend to actually run, so that you can instead stop when the model converges (typically occurring with a loss below 0.5 and token accuracy &gt;95%). This script was adapted so that if you interrupt training by pressing <code>Ctrl+D</code> from the terminal, it will gracefully stop early and still merge the LoRA weights before exiting.  If training is terminated otherwise, we added a <code>merge.py</code> script that you should run afterwards get the model ready for inference.</p>"},{"location":"openvla.html#validation","title":"Validation","text":"<p>Now that we have trained our test model (or you can download one from here), let's re-validate it again like we did above on the original OpenVLA model, but this time on unseen data from MimicGen with a different random seed (<code>dusty-nv/mimicgen-stack_d4-ep100</code>).  These commands will download and run the fully fine-tuned checkpoint (on 2500 episodes for 4 epochs) that we released to (<code>dusty-nv/openvla-7b-mimicgen</code>.  If you trained your own model, you can substitute the local path to the HF checkpoint.</p> INT4FP8FP16 <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.vision.vla --api mlc \\\n    --model dusty-nv/openvla-7b-mimicgen \\\n    --quantization q4f16_ft \\\n    --dataset dusty-nv/bridge_orig_ep100 \\\n    --dataset-type rlds \\\n    --max-episodes 10 \\\n    --save-stats /data/benchmarks/openvla_mimicgen_int4.json\n</code></pre> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.vision.vla --api mlc \\\n    --model dusty-nv/openvla-7b-mimicgen \\\n    --quantization q8f16_ft \\\n    --dataset dusty-nv/bridge_orig_ep100 \\\n    --dataset-type rlds \\\n    --max-episodes 10 \\\n    --save-stats /data/benchmarks/openvla_mimicgen_fp8.json\n</code></pre> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.vision.vla --api hf \\\n    --model dusty-nv/openvla-7b-mimicgen \\\n    --dataset dusty-nv/bridge_orig_ep100 \\\n    --dataset-type rlds \\\n    --max-episodes 10 \\\n    --save-stats /data/benchmarks/openvla_mimicgen_fp16.json\n</code></pre> <p>The results from this are collated in the next section along with the end-task success rates.  Time to see it in action!    </p>"},{"location":"openvla.html#inference-simulation","title":"Inference + Simulation","text":"<p>To measure how well our model actually performs at completing the task, we spin up a MimicGen environment in Agent Studio that's connected to the VLA model.  It counts the number of successful episodes by checking the reward issued by the sim, which is not used by the model but signals when the task was completed.  We use a horizon of 200 frames for evaluation, after which it is deemed to be a failure.  </p> INT4FP8FP16 <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.studio --load OpenVLA-MimicGen-INT4\n</code></pre> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.studio --load OpenVLA-MimicGen-FP8\n</code></pre> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.studio --load OpenVLA-MimicGen-FP16\n</code></pre> <p>To start the benchmarking, connect the output of the <code>AutoPrompt</code> node to the <code>OpenVLA</code> node.  On its own it will run forever - we did it for 100 episodes each, which can take several hours since the sim operates in lock step with the model (future experiments will train on actions accumulated from multiple timesteps and also reduce the model size to improve performance).</p> Quantization Train Accuracy Val Accuracy Task Success Avg Frames Latency FPS FP16 96.5% 85.4% 86% 132 827 ms 1.20 FP8 96.2% 85.1% 85% 131 467 ms 2.14 INT4 95.4% 84.4% 84% 138 335 ms 2.98 <p>This is using the model fine-tuned on 2500 episodes for 4 epochs, and although the task may have been simple, is evidence of achieving the sought-after success rates of ~85%.  Quantization has a negligible ~1% impact while scaling performance almost linearly.  The average number of frames is how long it took the robot to complete the task, which efficiency is another important end-metric to evalulate models by (consider the source teleop episodes were ~110 frames long, and we realized after that these averages include failed episodes during evaluation). The training dataset <code>dusty-nv/bridge_orig_ep2500</code> was used to measure the action Train Accuracy, while the previously unused and distinct <code>dusty-nv/bridge_orig_ep100</code> was used for Validation Accuracy.</p>"},{"location":"openvla.html#episodes-vs-epochs","title":"Episodes vs Epochs","text":"<p>Upon measuring the success rates of the other fine-tuned models that were trained on fewer episodes for more epochs, we can see the impact of increasing the size of the dataset:</p> Episodes Epochs Frames Task Success Avg Frames 500 10 550K 23% 186 1000 6 660K 48% 165 2500 4 1.1M 84% 138 <p>That isn't to say that the 2500-episode model still isn't overfit - it is after learning to always stack the smallr block and essentially ignore the instructions.  More task diversity in the training is required, which we can do now that we have the workflow.  And we want to branch out into real-world domains instead of test environments in simulation to prove model viability.  The amount of variance and data required to achieve generalist models in the challenging problem space of manipulation does raise interesting questions about purely random task learning versus more guided and curated approaches that ramp up in complexity as training progresses.</p>"},{"location":"openvla.html#future-research","title":"Future Research","text":"<p>Embodied agents are an exciting emerging area at the forefront of robotics and physical AI, with many promising avenues to investigate further.  Follow-up publications to OpenVLA include Embodied-CoT and CrossFormer, along with others sure to be in development.  In addition to proceeding to physical testing, these areas of interest we'll experiment with as research progresses:</p> <ul> <li>Smaller LLM with higher-resolution vision encoder(s)</li> <li>Multiple image inputs - multiple cameras and/or timesteps</li> <li>Action states from previous frame(s) as input</li> <li>Training on consecutive actions for larger timesteps</li> <li>Similar test model for UGV's in sim</li> <li>Using Isaac Lab and Robocasa</li> <li>sim2real deployment with ROS2</li> <li>Trade-offs of VLA vs VIT-based approaches</li> </ul>"},{"location":"research.html","title":"Jetson AI Lab Research Group","text":"<p>The Jetson AI Lab Research Group is a global collective for advancing open-source Edge ML, open to anyone to join and collaborate with others from the community and leverage each other's work. Our goal is using advanced AI for good in real-world applications in accessible and responsible ways. By coordinating together as a group, we can keep up with the rapidly evolving pace of AI and more quickly arrive at deploying intelligent multimodal agents and autonomous robots into the field.</p> <p>There are virtual meetings that anyone is welcome to join, offline discussion on the Jetson Projects forum, and guidelines for upstreaming open-source contributions. </p> <p>Upcoming Meetings</p> <p>Regular Series Begins: 2nd Tuesday of every month starting September 9, 2025</p> <p>We are pleased to announce the resumption of the Jetson AI Lab Research Group Community calls. The primary objective of these calls remains consistent with our past meetings. We aim to discuss emerging trends, share insights on exciting projects undertaken by members of this group, address any issues or roadblocks, and explore new frameworks and libraries, among other topics.</p> <p>We will share more details during the call. We look forward to reconnecting with all of you.</p> <p>Join the Discord server to stay involved and receive the latest updates.</p>"},{"location":"research.html#research-topics","title":"Research Topics","text":"<p>Our group explores cutting-edge research areas in edge AI and robotics. These topics represent our current focus areas and will evolve as the state-of-the-art progresses and new challenges emerge:</p> \u2022 Controller LLMs for dynamic pipeline code generation \u2022 Fine-tuning LLM/VLM onboard Jetson AGX Orin 64GB \u2022 HomeAssistant.io integration for smart home [1] [2] \u2022 Continuous multi-image VLM streaming and change detection \u2022 Recurrent LLM architectures (Mamba, RKVW, etc) [1] \u2022 Lightweight low-memory streaming ASR/TTS models \u2022 Diffusion models for image processing and enhancement \u2022 Time Series Forecasting with Transformers [1] [2] \u2022 Guidance, grammars, and guardrails for constrained output \u2022 Inline LLM function calling / plugins from API definitions \u2022 ML DevOps, edge deployment, and orchestration \u2022 Robotics, IoT, and cyberphysical systems integration"},{"location":"research.html#contribution-guidelines","title":"Contribution Guidelines","text":"<p>When experiments are successful, we package the results in ways that make them easily reusable for others to integrate into their own projects. We encourage contributions through multiple channels:</p> <ul> <li>Open-source libraries &amp; code on GitHub</li> <li>Models on HuggingFace Hub</li> <li>Containers provided by jetson-containers</li> <li>Discussions on the Jetson Projects forum</li> <li>Documentation &amp; tutorials on Jetson AI Lab</li> <li>Hackster.io for hardware-centric builds</li> </ul> <p>Ongoing technical discussions are encouraged to occur on the forums, Discord, or GitHub Issues, with status updates on projects given during the meetings.</p>"},{"location":"research.html#meeting-schedule","title":"Meeting Schedule","text":"<p>Regular Series: 2nd Tuesday of every month starting September 9, 2025</p> <p>We hold virtual meetings that anyone is welcome to join and speak during. We discuss the latest updates and experiments that we want to explore. Please remain courteous to others during the calls. We'll stick around after for anyone who has questions or didn't get the chance to be heard.</p> <p>Next Meeting: Tuesday, September 9<sup>th</sup>, 2025</p> <ul> <li>Time: 9:00 AM PST / 12:00 PM EST / 5:00 PM GMT</li> <li>Platform: Microsoft Teams</li> <li>Meeting Link: Join Meeting</li> <li>Meeting ID: <code>288 976 487 014 3</code></li> <li>Passcode: <code>6oG2uK2A</code></li> </ul> <p>Future Meetings</p> <ul> <li>October 14<sup>th</sup>, 2025 - 2nd Tuesday</li> <li>November 11<sup>th</sup>, 2025 - 2nd Tuesday  </li> <li>December 9<sup>th</sup>, 2025 - 2nd Tuesday</li> <li>January 13<sup>th</sup>, 2026 - 2nd Tuesday</li> </ul> <p>Please join the <code>#events</code> channel on the Discord server for the latest info. The meetings will be recorded so anyone unable to attend live can watch them after.</p>"},{"location":"research.html#past-meetings","title":"Past Meetings","text":"<p>Our meeting recordings and resources are available for review and reference.</p> <p>Recordings Archive</p> Microsoft Teams: (August 2025 - present) DateTopicLink Sep 9, 2025 Jetson Community Back-to-School call TBD Aug 13, 2025 Jetson Community Q3 Kick-of call &amp; new format Recording Google Drive: (October 2024 - August 2025) <p>Jetson AI Lab Research Group Recordings</p> Youtube Developer Channel: (April 2024 - October 2024) October 15, 2024 <p>Topics Covered:</p> <ul> <li>HuggingFace LeRobot (Chitoku Yato)</li> <li>Stanley H1 Humanoid (Dave Niewinski)</li> </ul> October 1, 2024 <p>Topics Covered:</p> <ul> <li>ReMEmbR: Long-Horizon Memory for Navigation (Abrar Anwar)</li> <li>Diffusion Policies, Shape LLM, 3D Encoders</li> </ul> September 17, 2024 <p>Topics Covered:</p> <ul> <li>NASA JPL - ROSA (Rob Royce &amp; Shehryar Khattak)</li> <li>LeRobot Walkthrough (Chitoku Yato)</li> <li>VLM Agent in Isaac Sim/ROS (Kabilan Kb)</li> </ul> September 3, 2024 <p>Topics Covered:</p> <ul> <li>Edge NeRF's and nerfstudio (Johnny N\u00fa\u00f1ez Cano)</li> <li>Review of OpenVLA results (Dustin Franklin)</li> <li>Oculus Interface for Jetson (Al Costa)</li> <li>TAO Toolkit 5.5</li> </ul> August 20, 2024 <p>Topics Covered:</p> <ul> <li>GenAI ROS Nodes for VLM (Khannah Shaltiel)</li> <li>Isaac Sim and Orin Nano with Hardware-in-the-Loop (Kabilan Kb)</li> <li>Miru Edge Deployment Infrastructure (Vedant Nair)</li> </ul> August 6, 2024 <p>Topics Covered:</p> <ul> <li>OpenVLA Fine-Tuning</li> <li>Gemma-2-2b (Asier Arranz)</li> <li>Ultralytics YOLOv8 (Lakshantha Dissanayake)</li> </ul> July 23, 2024 <p>Topics Covered:</p> <ul> <li>Llama-3 Function &amp; Specs</li> <li>OpenVLA with MimicGen</li> <li>Phi-3 Vision via ONNX (Jambo Chen)</li> <li>OStream GenRunner (Kerry Shih)</li> </ul> July 9, 2024 <p>Topics Covered:</p> <ul> <li>OpenVLA Quantization (openvla.github.io)</li> <li>visualnav-transformer (robodhruv/visualnav-transformer)</li> <li>Florence-2, Holoscan, Grammars (Nigel Nelson, florence-2-vision)</li> <li>LLaMa-Factory (hiyouga/LLaMA-Factory)</li> </ul> June 25, 2024 <p>Topics Covered:</p> <ul> <li>Function Calling in Agent Studio</li> <li>Jetson Copilot (Chitoku Yato)</li> <li>Jetson Platform Services (Sammy Ochoa)</li> <li>On-device Fine-tuning (Nurgaliyev Shakhizat)</li> </ul> June 11, 2024 <p>Topics Covered:</p> <ul> <li>Agent Studio</li> <li>HomeAssistant 2024.6</li> <li>AWS IoT Greengrass (Romil Shah)</li> </ul> May 29, 2024 <p>Topics Covered:</p> <ul> <li>OpenAI-style Tools with NousResearch/Hermes-2-Pro-Llama-3-8B</li> <li>Jetson Copilot with jetrag</li> <li>whisper_trt for Orin Nano</li> </ul> May 15, 2024 <p>Topics Covered:</p> <ul> <li>VILA-1.5 on Video Sequences</li> <li>Voicecraft Container (Martin Cerven)</li> <li>JetBot / Nanosaur Updates for Orin Nano (Chitoku Yato &amp; Raffaello Bonghi)</li> <li>Controller LLM &amp; Advanced Function Calling (<code>NousResearch/Hermes-2-Pro-Llama-3-8B</code>)</li> <li>RAG Samples with LlamaIndex (Chitoku Yato)</li> </ul> May 1, 2024 <p>Topics Covered:</p> <ul> <li>Function Calling with Llama-3</li> <li>Home Assistant / Wyoming (Mieszko Syty)</li> <li>Smart Sorting / Recycling (Alvaro Costa)</li> </ul> April 17, 2024 <p>Topics Covered:</p> <ul> <li>Ollama Support for Jetson Devices <li>Home Assistant Integration</li> <li><code>jetson-container</code> Updates</li> <li>Upgrading JetBot with Orin Nano</li> April 3, 2024 <p>Project Kickoffs:</p> <ul> <li>Home Assistant Integration</li> <li>Agent Controller LLM <li>ML DevOps, Containers, Core Inferencing</li>"},{"location":"research.html#team-members","title":"Team Members","text":"<p>Our research group brings together experts from academia, industry, and the open-source community who are passionate about advancing edge AI and robotics. Below are some of our sustaining members who have been actively contributing to generative AI in edge computing:</p>"},{"location":"research.html#core-contributors","title":"Core Contributors","text":"Dustin Franklin, NVIDIA                  Principal Engineer | Pittsburgh, PA                 (jetson-inference, jetson-containers)                           Nurgaliyev Shakhizat                  Institute of Smart Systems and AI | Kazakhstan                 (Assistive Devices, Vision2Audio, HPC)                           Kris Kersey, Kersey Fabrications                  Embedded Software Engineer | Atlanta, GA                 (The OASIS Project, AR/VR, 3D Fabrication)                           Johnny N\u00fa\u00f1ez Cano                  PhD Researcher in CV/AI | Barcelona, Spain                 (Recurrent LLMs, Pose &amp; Behavior Analysis)                           Doruk S\u00f6nmez, ConnectTech                  Intelligent Video Analytics Engineer | Turkey                 (NVIDIA DLI Certified Instructor, IVA, VLM)                           Akash James, Spark Cognition                  AI Architect, UC Berkeley Researcher | Oakland                 (NVIDIA AI Ambassador, Personal Assistants)                           Mieszko Syty, FutureProofHomes                  AI/ML Engineer | Warsaw, Poland                 (LLM, ML DevOps, Edge AI, Home Assistant, Nexus: AI Home Assistant, jetson-containers)                           Jim Benson, JetsonHacks                  DIY Extraordinaire | Los Angeles, CA                 (AI in Education, RACECAR/J)                           Chitoku Yato, NVIDIA                  Jetson AI DevTech | Santa Clara, CA                 (JetBot, JetRacer, MinDisk, Containers)                           Dana Sheahen, NVIDIA                  DLI Curriculum Developer | Santa Clara, CA                 (AI in Education, Jetson AI Fundamentals)                           Sammy Ochoa, NVIDIA                  Jetson AI DevTech | Austin, TX                 (Metropolis Microservices)                           John Welsh, NVIDIA                  (NanoOWL, NanoSAM, JetBot, JetRacer, torch2trt, trt_pose, Knowledge Distillation)                           Dave Niewinski Dave's Armoury | Waterloo, Ontario                 (GLaDOS, Fetch, Offroad La-Z-Boy, KUKA Bot)                           Gary Hilgemann, REBOTNIX                  CEO &amp; AI Roboticist | L\u00fcnen, Germany                 (GUSTAV, SPIKE, VisionTools, GenAI)                           Elaine Wu, Seeed Studio                  AI &amp; Robotics Partnerships | Shenzhen, China                 (reComputer, YOLOv8, LocalJARVIS, Voice Bot)                           Patty Delafuente, NVIDIA                  Data Scientist &amp; UMBC PhD Student | MD                 (AI in Education, DLI Robotics Teaching Kit)                           Song Han, MIT HAN Lab NVIDIA Research | Cambridge, MA                 (Efficient Large Models, AWQ, VILA)                           Bryan Hughes, Mimzy AI                  Founder, Entrepreneur | SF Bay Area                 (Multimodal Assistants, AI at the Edge)                           Tianqi Chen, CMU Catalyst OctoML, CTO | Seattle, WA                 (MLC, Apache TVM, XGBoost)                           Michael Gr\u00fcner, RidgeRun                  Team Lead / Architect | Costa Rica                 (Embedded Vision &amp; AI, Multimedia)                           Jesse Flot, CMU Robotics Academy                  Co-Director | Pittsburgh, PA                 (Applied AI &amp; Robotics, Autonomy Foundations)                           Paul DeCarlo, Microsoft                  Professor | University of Houston                 (Azure IoT, Video Analytics, Microsoft JARVIS)                           Mike Hansen, Nabu Casa                  Voice Engineering Lead | Urbandale, Iowa                 (Home Assistant, Piper TTS, Wyoming)              Lakshantha Dissanayake, Ultralytics                  Embedded CV Engineer | Vancouver, BC                 (YOLOv8, TensorRT, DeepStream)                           Kerry Shih, GenAI Nerds                  Founder, CEO | Los Angeles, CA                 (Gapi)                           Ziad Youssfi, CMU                  ECE Professor | Pittsburgh, PA                 (ML in Robotics &amp; Embedded Systems)                           Walter Lucetti, Stereolabs                  Robotics &amp; Vision Engineer | Paris, France                 (MyzharBot, ROS2, GStreamer)                           Raffaello Bonghi, NVIDIA                  AI &amp; Robotics Engineer | Manchester, UK                 (Nanosaur, Panther, jetson-stats)                           Alvaro Costa, TeknTrash                  CEO &amp; Founder | Epsom, UK                 (Stipra Dataset, Industrial Automation)                           David Pearson, ConnectTech                  Embedded Systems Engineer | Ontario, CA                 (Edge AI Systems, Vision/Language Models)                           Jason Seawall, Numurus                  CEO | Seattle, WA                 (NEPI, Edge AI &amp; Automation)                           Martin Cerven                  AI Researcher | Germany                 (Autonomous Robotics, Voicecraft)                           Romil Shah, Amazon                  GenAI IIoT @ AWS | San Jose, CA                 (<code>aws-samples/genai-at-edge</code>)                           Kabilan Kb, Roboticist                  NVIDIA DLI Ambassador | Tamil Nadu, IN                 (ROS2 Tutorials, Autonomous Wheelchair)"},{"location":"robopoint.html","title":"Tutorial - RoboPoint VLM for Robotic Manipulation","text":"<p>RoboPoint is a general vision/language model that has been specifially tuned for spatially-aware tasks and referencing the relative locations of objects during language-guided robot navigation, manipulation, and augmented reality (AR) assistance. </p> <p></p> <p>Credits</p> <p>Thank you to Manuel Schweiger for creating this tutorial, along with the University of Washington, NVIDIA, Allen Institute and Universidad Catolica San Pablo. </p> <p>This tutorial provides a demo application for robotic manipulation using a Vision-Language Model (VLM) pipeline combined with a Large Language Model (LLM) to articulate manipulators using natural language. The RoboPoint inference pipeline generates 2D action points, which can be projected to 3D targets using depth maps or renowned algorithms like OpenCV solvePNP. The computed 3D targets can be fed into motion planning and deployed to real hardware or simulation environments like Isaac Sim. Future phases will include ROS2 integration with an Isaac Sim pipeline and the implementation of quantization methods.</p> <p>In this tutorial we will guide you through:</p> <p> Setting up the environment using jetson-containers   Connecting a Boston Dynamics Spot with Arm to RoboPoint VLM  Issuing commands using natural language prompts  Executing pick-and-place operations  </p>"},{"location":"robopoint.html#robopoint-vlm-for-embodied-ai","title":"RoboPoint VLM for Embodied AI","text":"<p>From rearranging objects on a table to putting groceries into shelves, robots must plan precise action points to perform tasks accurately and reliably. In spite of the recent adoption of vision language models (VLMs) to control robot behavior, VLMs struggle to precisely articulate robot actions using language. We introduce an automatic synthetic data generation pipeline that instruction-tunes VLMs to robotic domains and needs. </p>"},{"location":"robopoint.html#robopoint-pipeline","title":"RoboPoint Pipeline","text":"<p>Source: RoboPoint Paper</p> <p>An RGB image is rendered from a procedurally generated 3D scene. We compute spatial relations from the camera's perspective and generate affordances by sampling points within object masks and object-surface intersections. These instruction-point pairs fine-tune the language model. During deployment, RoboPoint predicts 2D action points from an image and instruction, which are projected into 3D using a depth map. The robot then navigates to these 3D targets with a motion planner. For more information please refer the official paper and project</p>"},{"location":"robopoint.html#advantages-of-the-proposed-architecture","title":"Advantages of the proposed architecture","text":"<p>One key advantage of this architecture is its efficiency. The process of projecting 2D action points into 3D poses is both fast and computationally lightweight. This ensures smooth robotic manipulation, enabling rapid execution of even complex, long-term, and sequential commands.</p>"},{"location":"robopoint.html#1-setting-up-the-environment-with-jetson-containers","title":"1. Setting up the environment with <code>jetson-containers</code>","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one the following version of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>25GB</code> for <code>robopoint-v1-vicuna-v1.5-13b</code> LLM</li> <li><code>5.3GB</code> for <code>robopoint</code> container image</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> <li> <p>Take a look at the container README</p> </li> <li> <p>Run the RoboPoint Container</p> <pre><code>jetson-containers $(autotag robopoint)\n</code></pre> </li> </ol>"},{"location":"robopoint.html#2-gradio-demo-application","title":"2. Gradio Demo Application","text":"<p>The project includes a Gradio demo application, packaged within the provided container. To access the interface and execute commands, simply open a web browser and navigate to <code>http://jetson-ip:7860/</code>. You should see a Gradio WebApp with Demo Examples as shown here.</p> <p></p>"},{"location":"robopoint.html#3-boston-dynamics-spot-deployment","title":"3. Boston Dynamics Spot Deployment","text":"<p>Connect the RoboPoint VLM to a Boston Dynamics Spot with Arm for mobile manipulation scenarios. The inference is performed using the Gradio API. The results are then parsed, projected into 3D and sent to the inverse kinematics (IK) solver of the Boston Dynamics Spot API. The required steps are outlined below.</p> <p>Disclaimer:  Use at your own risk</p> <p>Please note that controlling the robot to grasp an object involves moving parts that may cause damage or harm to people or property. Ensure that the operating environment is clear of obstacles and that all personnel maintain a safe distance from the robot during operation. Always follow safety guidelines and protocols provided by the robot manufacturer.</p> <p>What we will do</p> <ol> <li> <p>Setup your Python Spot SDK environment: <code>Spot SDK</code></p> </li> <li> <p>Deploy the  <code>RoboPoint jetson-container</code> </p> </li> <li> <p>Use the RoboPoint Spot example to execute the following steps:</p> <pre><code>pip3 install -r requirements.txt\npython3 robopoint_spot_example.py -i frontleft -l \"pick the object next to the ball\" -g \"http://jetson-ip:7860\"\n</code></pre> <p>a. Connect to the robot and acquire a lease to control the robot</p> <p>b. Use the gradio inference API to predict 2D action points</p> <p>c. Project the 2D action points to a 3D pose using the robots API</p> <p>d. Run the motion planning</p> <p>e. Execute the grasp command</p> </li> </ol> <p>Work in Progress</p> <ul> <li>ROS2 Integration</li> <li>Isaac Sim Integration</li> <li>Ask questions in <code>#vla</code> on Discord or <code>jetson-containers/issues</code></li> </ul>"},{"location":"robopoint.html#optional-no-robot-at-hand-demo-script-with-camera-input","title":"Optional: No robot at hand? Demo script with camera input","text":"<p>The Gradio inference API enables seamless command execution for other robots or testing purposes. It simplifies integration and allows for quick deployment across different robotic platforms or testing purpose. We provide a convenient demo script to test the API and inference with images or a live camera input. Start the RoboPoint container  and execute the <code>client.py</code>demo script.  Run <code>python3 client.py --help</code> for input parameter details.</p> <pre><code>python3 client.py --request 'Find free space between the plates in the sink' --camera 0\n</code></pre> <p>You will receive an <code>output_image.jpg</code> with the predicted 2D action points, and the coordinates will be logged to the command line. Use this result to verify the inference on your images.</p>"},{"location":"ros.html","title":"ROS2 Nodes for Generative AI","text":"<p>The <code>ros2_nanollm</code> package provides ROS2 nodes for running optimized LLM's and VLM's locally inside a container.  These are built on NanoLLM and ROS2 Humble for deploying generative AI models onboard your robot with Jetson.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)\u26a0\ufe0f</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>22GB</code> for <code>nano_llm:humble</code> container image</li> <li>Space for models (<code>&gt;10GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"ros.html#running-the-live-demo","title":"Running the Live Demo","text":"<p>Recommended</p> <p>Before you start, please review NanoVLM and Live LLaVa demos.  For primary documentation, view ROS2 NanoLLM.</p> <ol> <li> <p>Ensure you have a camera device connected</p> <pre><code>ls /dev/video*\n</code></pre> </li> <li> <p>Use the <code>jetson-containers run</code> and <code>autotag</code> commands to automatically pull or build a compatible container image. </p> <pre><code>jetson-containers run $(autotag nano_llm:humble) \\\n    ros2 launch ros2_nanollm camera_input_example.launch.py\n</code></pre> <p>This command will start the launch file of the container. </p> </li> </ol> <p>By default this will load the <code>Efficient-Large-Model/Llama-3-VILA1.5-8B</code> VLM and publish the image captions and overlay to topics that can be subscribed to by your other nodes, or visualized with RViz or Foxglove.  Refer to the <code>ros2_nanollm</code> repo for documentation on the input/output topics that are exposed.</p>"},{"location":"ros.html#build-your-own-ros-nodes","title":"Build your own ROS Nodes","text":"<p>To build your own ROS2 node using LLM or VLM, first create a ROS 2 workspace and package in a directory mounted to the container (following the ROS 2 Humble Documentation).  Your src folder should then look like this: </p> <pre><code>    \u2514\u2500\u2500 src    \n        \u2514\u2500\u2500 your-package-name\n            \u251c\u2500\u2500 launch     \n                    \u2514\u2500\u2500 camera_input.launch.py\n            \u251c\u2500\u2500 resource\n                    \u2514\u2500\u2500 your-package-name\n            \u251c\u2500\u2500 your-package-name\n                    \u2514\u2500\u2500 __init__.py \n                    \u2514\u2500\u2500 your-node-name_py.py\n            \u251c\u2500\u2500 test\n                    \u2514\u2500\u2500 test_copyright.py\n                    \u2514\u2500\u2500 test_flake8.py\n                    \u2514\u2500\u2500 test_pep257.py\n            \u251c\u2500\u2500 package.xml\n            \u251c\u2500\u2500 setup.cfg\n            \u251c\u2500\u2500 setup.py\n            \u2514\u2500\u2500 README.md\n</code></pre> <p>We will create the launch folder, as well as the camera_input.launch.py and your-node-name_py.py files in later steps. </p>"},{"location":"ros.html#editing-the-setup","title":"Editing the Setup","text":"<p>Let\u2019s begin by editing the <code>setup.py</code> file. At the top of the file, add </p> <pre><code>from glob import glob \n</code></pre> <p>In the setup method, find the <code>data_files=[]</code> line, and make sure it looks like this: </p> <pre><code>data_files=[\n       ('share/ament_index/resource_index/packages',\n           ['resource/' + package_name]),\n       ('share/' + package_name, ['package.xml']),\n   ('share/' + package_name, glob('launch/*.launch.py')),\n   ],\n</code></pre> <p>Edit the maintainer line with your name. Edit the maintainer email to your email. Edit the description line to describe your package. </p> <pre><code>maintainer='kshaltiel', \nmaintainter_email='kshaltiel@nvidia.com', \ndescription='YOUR DESCRIPTION',  \n</code></pre> <p>Find the <code>console_scripts</code> line in the entry_points method. Edit the inside to be: </p> <pre><code>'your-node-name_py = your-package-name.your-node-name_py:main'\n</code></pre> <p>For example:  <pre><code>entry_points={\n       'console_scripts': [\n       'nano_llm_py = ros2_nanollm.nano_llm_py:main'\n       ],\n   },\n</code></pre> All done for this file!</p>"},{"location":"ros.html#creating-the-node","title":"Creating the Node","text":"<p>Inside your package, under the folder that shares your package's name and contains the <code>__init__.py</code> file, create a file named after your node. For NanoLLM, this file would be called <code>nano_llm_py.py</code>. </p> <p>Paste the following code into the empty file: </p> <pre><code>import rclpy \nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nfrom PIL import Image as im\nfrom MODEL_NAME import NECESSARY_MODULES\n\nclass Your_Model_Subscriber(Node):\n\n    def __init__(self):\n        super().__init__('your_model_subscriber')\n\n        #EDIT PARAMETERS HERE \n        self.declare_parameter('param1', \"param1_value\") \n        self.declare_parameter('param2', \"param2_value\")\n\n        # Subscriber for input query\n        self.query_subscription = self.create_subscription(\n            String,\n            'input_query',\n            self.query_listener_callback,\n            10)\n        self.query_subscription  # prevent unused variable warning\n\n        # Subscriber for input image\n        self.image_subscription = self.create_subscription(\n            Image,\n            'input_image',\n            self.image_listener_callback,\n            10)\n        self.image_subscription  # prevent unused variable warning\n\n        # To convert ROS image message to OpenCV image\n        self.cv_br = CvBridge() \n\n        #LOAD THE MODEL\n        self.model = INSERT_MODEL.from_pretrained(\"PATH-TO-MODEL\")\n\n        #chatHistory var \n        self.chat_history = ChatHistory(self.model)\n\n        ##  PUBLISHER\n        self.output_publisher = self.create_publisher(String, 'output', 10)\n        self.query = \"Describe the image.\"\n\n    def query_listener_callback(self, msg):\n        self.query = msg.data\n\n    def image_listener_callback(self, data): \n        input_query = self.query\n\n        # call model with input_query and input_image \n        cv_img = self.cv_br.imgmsg_to_cv2(data, 'rgb8')\n        PIL_img = im.fromarray(cv_img)\n\n        # Parsing input text prompt\n        prompt = input_query.strip(\"][()\")\n        text = prompt.split(',')\n        self.get_logger().info('Your query: %s' % text) #prints the query\n\n        #chat history \n        self.chat_history.append('user', image=PIL_img)\n        self.chat_history.append('user', prompt, use_cache=True)\n        embedding, _ = self.chat_history.embed_chat()\n\n    #GENERATE OUTPUT\n        output = self.model.generate(\n            inputs=embedding,\n            kv_cache=self.chat_history.kv_cache,\n            min_new_tokens = 10,\n            streaming = False, \n            do_sample = True,\n        )\n\n        output_msg = String()\n        output_msg.data = output\n        self.output_publisher.publish(output_msg)\n        self.get_logger().info(f\"Published output: {output}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n\n    your_model_subscriber = Your_Model_Subscriber()\n\n    rclpy.spin(your_model_subscriber)\n\n    # Destroy the node explicitly\n    # (optional - otherwise it will be done automatically\n    # when the garbage collector destroys the node object)\n    nano_llm_subscriber.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n</code></pre> <p>Edit the import statement at the top of the file to import the necessary modules from the model. </p> <p>Next, edit the class name and name inside the <code>__init__()</code> function to reflect the model that will be used. </p> <p>Find the comment that reads <code>#EDIT PARAMETERS HERE</code>. Declare all parameters except for the model name following the format in the file. Under the <code>#LOAD THE MODEL</code> comment, include the path to the model. </p> <p>Lastly, edit the generate method under the <code>GENERATE OUTPUT</code> comment to include any additional parameters. </p> <p>All done for this file!</p>"},{"location":"ros.html#creating-the-launch-file","title":"Creating the Launch File","text":"<p>Inside your package, create the launch folder. Create your launch file inside of it. </p> <pre><code>mkdir launch\ncd launch \ntouch camera_input.launch.py\n</code></pre> <p>You can edit this file externally, and it will update within the container. Paste the following code into the empty file. </p> <pre><code>from launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.substitutions import LaunchConfiguration\nfrom launch.actions import DeclareLaunchArgument\n\ndef generate_launch_description():\n    launch_args = [\n        DeclareLaunchArgument( \n            'param1',\n            default_value='param1_default',\n            description='Description of param1'),\n        DeclareLaunchArgument(\n            'param2',\n            default_value='param2_default',\n            description='Description of param2'),\n    ]\n\n\n    #Your model parameters \n    param1 = LaunchConfiguration('param1')\n    param2 = LaunchConfiguration('param2')\n\n    #camera node for camera input\n    cam2image_node = Node(\n            package='image_tools',\n            executable='cam2image',\n            remappings=[('image', 'input_image')],\n    )\n\n    #model node\n    model_node = Node(\n            package='your-package-name', #make sure your package is named this\n            executable='your-node-name_py', \n            parameters=[{\n                'param1': param1, \n                'param2': param2,\n            }]\n    )\n\n    final_launch_description = launch_args + [cam2image_node] + [model_node]\n\n    return LaunchDescription(final_launch_description)\n</code></pre> <p>Find the required parameters for your model. You can view this by looking at the Model API for your specific model and taking note to how the model is called. For example, NanoLLM retrieves models through the following: </p> <pre><code>model = NanoLLM.from_pretrained(\n   \"meta-llama/Llama-3-8b-hf\",  # HuggingFace repo/model name, or path to HF model checkpoint\n   api='mlc',                   # supported APIs are: mlc, awq, hf\n   quantization='q4f16_ft'      # q4f16_ft, q4f16_1, q8f16_0 for MLC, or path to AWQ weights\n)\n</code></pre> <p>The parameters for NanoLLM would be the model name, api, and quantization. </p> <p>In the <code>generate_launch_description</code> function, edit the <code>DeclareLaunchArgument</code> to accomodate for all parameters except the model name. For NanoLLM, this would look like: </p> <pre><code>def generate_launch_description():\n    launch_args = [\n        DeclareLaunchArgument( \n            'api',\n            default_value='mlc',\n            description='The model backend to use'),\n        DeclareLaunchArgument(\n            'quantization',\n            default_value='q4f16_ft',\n            description='The quantization method to use'),\n    ]\n</code></pre> <p>Then edit the lines under <code>#Your model Parameters</code> to match the parameters of your model, again excluding the model name. Lastly, fill in the code under the <code>#model node</code> comment with your package name, the name of your node file, and all of your parameters, this time including the model. </p> <p>All done for this file!</p>"},{"location":"tensorrt_llm.html","title":"TensorRT-LLM for Jetson","text":"<p>TensorRT-LLM is a high-performance LLM inference library with advanced quantization, attention kernels, and paged KV caching.  Initial support for building TensorRT-LLM from source for JetPack 6.1 has been included in the <code>v0.12.0-jetson</code> branch of the TensorRT-LLM repo for Jetson AGX Orin.</p> <p></p> <p>We've provided pre-compiled TensorRT-LLM wheels and containers along with this guide for <code>TensorRT-LLM Deployment on Jetson Orin</code></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin *Support for other Orin devices is currently undergoing testing.</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6.1 (L4T r36.4)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>18.5GB</code> for <code>tensorrt_llm</code> container image</li> <li>Space for models (<code>&gt;10GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tensorrt_llm.html#building-tensorrt-llm-engine-for-llama","title":"Building TensorRT-LLM Engine for Llama","text":"<p>You can find the steps for converting Llama to TensorRT-LLM under <code>examples/llama</code> in the repo, and also in the documentation.  This script will automate the process for Llama-7B with INT4 quantization applied, and run some generation and performance checks on the model:</p> <pre><code>jetson-containers run \\\n  -e HUGGINGFACE_TOKEN=YOUR_API_KEY \\\n  -e FORCE_BUILD=on \\\n  dustynv/tensorrt_llm:0.12-r36.4.0 \\\n    /opt/TensorRT-LLM/llama.sh\n</code></pre> <p>There are many such conversion procedures outlined in the TensorRT-LLM examples for different model architectures.  </p>"},{"location":"tensorrt_llm.html#openai-api-endpoint","title":"OpenAI API Endpoint","text":"<p>TensorRT-LLM has programming APIs for Python and C++ available, but it also includes an example server endpoint for the OpenAI protocol that makes it easy to substitute for other local or cloud model backends.  </p> <p>This will start the TensorRT-LLM container with the server and model that you built above:</p> <pre><code>jetson-containers run \\\n  dustynv/tensorrt_llm:0.12-r36.4.0 \\\n  python3 /opt/TensorRT-LLM/examples/apps/openai_server.py \\\n    /data/models/tensorrt_llm/Llama-2-7b-chat-hf-gptq\n</code></pre> <p>Then you can make chat completion requests against it from practically any language or from any connected device.  This example shows a simple way of testing it initially from another terminal with curl:</p> <pre><code>curl http://localhost:8000/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": &lt;model_name&gt;,\n        \"prompt\": \"Where is New York?\",\n        \"max_tokens\": 16,\n        \"temperature\": 0\n    }'\n</code></pre> <p>Or the code included with openai_client.py will handle these requests using the standard <code>openai-python</code> package that can be installed outside of the container or on another machine.</p> <pre><code>jetson-containers run \\\n  --workdir /opt/TensorRT-LLM/examples/apps \\\n  dustynv/tensorrt_llm:0.12-r36.4.0 \\\n    python3 openai_client.py --prompt \"Where is New York?\" --api chat\n</code></pre> <p>The patches in the branch above for TensorRT-LLM 0.12 are a preview release for Jetson AGX Orin, and we'll continue with validating and testing the various settings in TensorRT-LLM.  If you need any support, please post to the Jetson Developer Forums..</p>"},{"location":"tips_ram-optimization.html","title":"RAM Optimization","text":"<p>Running a LLM requires a huge RAM space.</p> <p>Especially if you are on Jetson Orin Nano that only has 8GB of RAM, it is crucial to leave as much RAM space available for models. </p> <p>Here we share a couple of ways to optimize the system RAM usage. </p>"},{"location":"tips_ram-optimization.html#disabling-the-desktop-gui","title":"Disabling the Desktop GUI","text":"<p>If you use your Jetson remotely through SSH, you can disable the Ubuntu desktop GUI. This will free up extra memory that the window manager and desktop uses (around ~800MB for Unity/GNOME).</p> <p>You can disable the desktop temporarily, run commands in the console, and then re-start the desktop when desired:</p> <pre><code>$ sudo init 3     # stop the desktop\n# log your user back into the console (Ctrl+Alt+F1, F2, ect)\n$ sudo init 5     # restart the desktop\n</code></pre> <p>If you wish to make this persistent across reboots, you can use the following commands to change the boot-up behavior:</p> <ul> <li> <p>To disable desktop on boot</p> <pre><code>sudo systemctl set-default multi-user.target\n</code></pre> </li> <li> <p>To enable desktop on boot</p> <pre><code>sudo systemctl set-default graphical.target\n</code></pre> </li> </ul>"},{"location":"tips_ram-optimization.html#disabling-misc-services","title":"Disabling misc services","text":"<pre><code>sudo systemctl disable nvargus-daemon.service\n</code></pre>"},{"location":"tips_ram-optimization.html#mounting-swap","title":"Mounting Swap","text":"<p>If you're building containers or working with large models, it's advisable to mount SWAP (typically correlated with the amount of memory in the board). Run these commands to disable ZRAM and create a swap file:</p> <p>If you have NVMe SSD storage available, it's preferred to allocate the swap file on the NVMe SSD.</p> <pre><code>sudo systemctl disable nvzramconfig\nsudo fallocate -l 16G /ssd/16GB.swap\nsudo mkswap /ssd/16GB.swap\nsudo swapon /ssd/16GB.swap\n</code></pre> <p>Then add the following line to the end of /etc/fstab to make the change persistent:</p> <pre><code>/ssd/16GB.swap  none  swap  sw 0  0\n</code></pre>"},{"location":"tips_ssd-docker.html","title":"Tips - SSD + Docker","text":"<p>Once you have your Jetson set up by flashing the latest Jetson Linux (L4T) BSP on it or by flashing the SD card with the whole JetPack image, before embarking on testing out all the great generative AI application using <code>jetson-containers</code>, you want to make sure you have a huge storage space for all the containers and the models you will download.</p> <p>We are going to show how you can install SSD on your Jetson, and set it up for Docker.</p>"},{"location":"tips_ssd-docker.html#ssd","title":"SSD","text":""},{"location":"tips_ssd-docker.html#physical-installation","title":"Physical installation","text":"<ol> <li>Unplug power and any peripherals from the Jetson developer kit.</li> <li>Physically install an NVMe SSD card on the carrier board of your Jetson developer kit, making sure to properly seat the connector and secure with the screw.</li> <li>Reconnect any peripherals, and then reconnect the power supply to turn on the Jetson developer kit.</li> <li> <p>Once the system is up, verify that your Jetson identifies a new memory controller on PCI bus:</p> <pre><code>lspci\n</code></pre> <p>The output should look like the following:</p> <pre><code>0007:01:00.0 Non-Volatile memory controller: Marvell Technology Group Ltd. Device 1322 (rev 02)\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#format-and-set-up-auto-mount","title":"Format and set up auto-mount","text":"<ol> <li> <p>Run <code>lsblk</code> to find the device name.</p> <pre><code>lsblk\n</code></pre> <p>The output should look like the following:</p> <pre><code>NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nloop0          7:0    0    16M  1 loop\nmmcblk1      179:0    0  59.5G  0 disk\n\u251c\u2500mmcblk1p1  179:1    0    58G  0 part /\n\u251c\u2500mmcblk1p2  179:2    0   128M  0 part\n\u251c\u2500mmcblk1p3  179:3    0   768K  0 part\n\u251c\u2500mmcblk1p4  179:4    0  31.6M  0 part\n\u251c\u2500mmcblk1p5  179:5    0   128M  0 part\n\u251c\u2500mmcblk1p6  179:6    0   768K  0 part\n\u251c\u2500mmcblk1p7  179:7    0  31.6M  0 part\n\u251c\u2500mmcblk1p8  179:8    0    80M  0 part\n\u251c\u2500mmcblk1p9  179:9    0   512K  0 part\n\u251c\u2500mmcblk1p10 179:10   0    64M  0 part\n\u251c\u2500mmcblk1p11 179:11   0    80M  0 part\n\u251c\u2500mmcblk1p12 179:12   0   512K  0 part\n\u251c\u2500mmcblk1p13 179:13   0    64M  0 part\n\u2514\u2500mmcblk1p14 179:14   0 879.5M  0 part\nzram0        251:0    0   1.8G  0 disk [SWAP]\nzram1        251:1    0   1.8G  0 disk [SWAP]\nzram2        251:2    0   1.8G  0 disk [SWAP]\nzram3        251:3    0   1.8G  0 disk [SWAP]\nnvme0n1      259:0    0 238.5G  0 disk\n</code></pre> <p>Identify the device corresponding to your SSD. In this case, it is <code>nvme0n1</code>.</p> </li> <li> <p>Format the SSD, create a mount point, and mount it to the filesystem.</p> <pre><code>sudo mkfs.ext4 /dev/nvme0n1\n</code></pre> <p>You can choose any name for the mount point directory. We use <code>/ssd</code> here, but in <code>jetson-containers</code>' setup.md documentation, <code>/mnt</code> is used.</p> <pre><code>sudo mkdir /ssd\n</code></pre> <pre><code>sudo mount /dev/nvme0n1 /ssd\n</code></pre> </li> <li> <p>In order to ensure that the mount persists after boot, add an entry to the <code>fstab</code> file:</p> <p>First, identify the UUID for your SSD:</p> <pre><code>lsblk -f\n</code></pre> <p>Then, add a new entry to the <code>fstab</code> file:</p> <pre><code>sudo vi /etc/fstab\n</code></pre> <p>Insert the following line, replacing the UUID with the value found from <code>lsblk -f</code>:</p> <pre><code>UUID=************-****-****-****-******** /ssd/ ext4 defaults 0 2\n</code></pre> </li> <li> <p>Finally, change the ownership of the <code>/ssd</code> directory.</p> <pre><code>sudo chown ${USER}:${USER} /ssd\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#docker","title":"Docker","text":"<ol> <li> <p>Install <code>nvidia-container</code> package.</p> <p>Note: If you used an NVIDIA-supplied SD card image to flash your SD card, all necessary JetPack components (including <code>nvidia-containers</code>) and Docker are already pre-installed, so this step can be skipped.</p> <pre><code>sudo apt update\nsudo apt install -y nvidia-container\n</code></pre> <p>JetPack 6.x users</p> <p>If you flash  **Jetson Linux (L4T) R36.x (JetPack 6.x) on your Jetson using SDK Manager, and install <code>nvidia-container</code> using <code>apt</code>, on JetPack 6.x it no longer automatically installs Docker.</p> <p>Therefore, you need to run the following to manually install Docker and set it up.</p> <pre><code>sudo apt update\nsudo apt install -y nvidia-container curl\ncurl https://get.docker.com | sh &amp;&amp; sudo systemctl --now enable docker\nsudo nvidia-ctk runtime configure --runtime=docker\n</code></pre> </li> <li> <p>Restart the Docker service and add your user to the <code>docker</code> group, so that you don't need to use the command with <code>sudo</code>.</p> <pre><code>sudo systemctl restart docker\nsudo usermod -aG docker $USER\nnewgrp docker\n</code></pre> </li> <li> <p>Add default runtime in <code>/etc/docker/daemon.json</code></p> One-linerManual <pre><code>sudo apt install -y jq\nsudo jq '. + {\"default-runtime\": \"nvidia\"}' /etc/docker/daemon.json | \\\n  sudo tee /etc/docker/daemon.json.tmp &amp;&amp; \\\n  sudo mv /etc/docker/daemon.json.tmp /etc/docker/daemon.json\n</code></pre> <p>Use your editor to open the following JSON file.</p> <pre><code>sudo vi /etc/docker/daemon.json\n</code></pre> <p>Insert the <code>\"default-runtime\": \"nvidia\"</code> line as following:</p> <pre><code>{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"default-runtime\": \"nvidia\"\n}\n</code></pre> </li> <li> <p>Restart Docker</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#migrate-docker-directory-to-ssd","title":"Migrate Docker directory to SSD","text":"<p>Now that the SSD is installed and available on your device, you can use the extra storage capacity to hold the storage-demanding Docker directory.</p> <ol> <li> <p>Stop the Docker service.</p> <pre><code>sudo systemctl stop docker\n</code></pre> </li> <li> <p>Move the existing Docker folder</p> <pre><code>sudo du -csh /var/lib/docker/ &amp;&amp; \\\n    sudo mkdir /ssd/docker &amp;&amp; \\\n    sudo rsync -axPS /var/lib/docker/ /ssd/docker/ &amp;&amp; \\\n    sudo du -csh  /ssd/docker/\n</code></pre> </li> <li> <p>Edit <code>/etc/docker/daemon.json</code></p> <pre><code>sudo vi /etc/docker/daemon.json\n</code></pre> <p>Insert <code>\"data-root\"</code> line like the following.</p> <pre><code>{\n    \"runtimes\": {\n        \"nvidia\": {\n            \"path\": \"nvidia-container-runtime\",\n            \"runtimeArgs\": []\n        }\n    },\n    \"default-runtime\": \"nvidia\",\n    \"data-root\": \"/ssd/docker\"\n}\n</code></pre> </li> <li> <p>Rename the old Docker data directory</p> <pre><code>sudo mv /var/lib/docker /var/lib/docker.old\n</code></pre> </li> <li> <p>Restart the docker daemon</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; \\\n    sudo systemctl restart docker &amp;&amp; \\\n    sudo journalctl -u docker\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#test-docker-on-ssd","title":"Test Docker on SSD","text":"<ol> <li> <p>[Terminal 1] First, open a terminal to monitor the disk usage while pulling a Docker image.</p> <pre><code>watch -n1 df\n</code></pre> </li> <li> <p>[Terminal 2] Next, open a new terminal and start Docker pull.</p> <pre><code>docker pull ubuntu:22.04\n</code></pre> </li> <li> <p>[Terminal 1] Observe that the disk usage on <code>/ssd</code> goes up as the container image is downloaded and extracted.</p> <pre><code>~$ docker image ls\nREPOSITORY                  TAG       IMAGE ID       CREATED        SIZE\nubuntu                      22.04     981912c48e9a   4 months ago   69.2MB\n</code></pre> </li> </ol>"},{"location":"tips_ssd-docker.html#final-verification","title":"Final Verification","text":"<p>Reboot your Jetson, and verify that you observe the following:</p> <pre><code>~$ sudo blkid | grep nvme\n/dev/nvme0n1: UUID=\"9fc06de1-7cf3-43e2-928a-53a9c03fc5d8\" TYPE=\"ext4\"\n\n~$ df -h\nFilesystem      Size  Used Avail Use% Mounted on\n/dev/mmcblk1p1  116G   18G   94G  16% /\nnone            3.5G     0  3.5G   0% /dev\ntmpfs           3.6G  108K  3.6G   1% /dev/shm\ntmpfs           734M   35M  699M   5% /run\ntmpfs           5.0M  4.0K  5.0M   1% /run/lock\ntmpfs           3.6G     0  3.6G   0% /sys/fs/cgroup\ntmpfs           734M   88K  734M   1% /run/user/1000\n/dev/nvme0n1    458G  824M  434G   1% /ssd\n\n~$ docker info | grep Root\n Docker Root Dir: /ssd/docker\n\n~$ sudo ls -l /ssd/docker/\ntotal 44\ndrwx--x--x  4 root root 4096 Mar 22 11:44 buildkit\ndrwx--x---  2 root root 4096 Mar 22 11:44 containers\ndrwx------  3 root root 4096 Mar 22 11:44 image\ndrwxr-x---  3 root root 4096 Mar 22 11:44 network\ndrwx--x--- 13 root root 4096 Mar 22 16:20 overlay2\ndrwx------  4 root root 4096 Mar 22 11:44 plugins\ndrwx------  2 root root 4096 Mar 22 16:19 runtimes\ndrwx------  2 root root 4096 Mar 22 11:44 swarm\ndrwx------  2 root root 4096 Mar 22 16:20 tmp\ndrwx------  2 root root 4096 Mar 22 11:44 trust\ndrwx-----x  2 root root 4096 Mar 22 16:19 volumes\n\n~$ sudo du -chs /ssd/docker/\n752M    /ssd/docker/\n752M    total\n\n~$ docker info | grep -e \"Runtime\" -e \"Root\"\n Runtimes: io.containerd.runtime.v1.linux nvidia runc io.containerd.runc.v2\n Default Runtime: nvidia\n Docker Root Dir: /ssd/docker\n</code></pre> <p>Your Jetson is now set up with the SSD!</p>"},{"location":"try.html","title":"Try","text":"<p>Jump to NVIDIA Jetson Store.</p>"},{"location":"tutorial-intro.html","title":"Tutorial - Introduction","text":""},{"location":"tutorial-intro.html#overview","title":"Overview","text":"<p>Our tutorials are divided into categories roughly based on model modality, the type of data to be processed or generated.</p>"},{"location":"tutorial-intro.html#text-llm","title":"Text (LLM)","text":"text-generation-webui Interact with a local AI assistant by running a LLM with oobabooga's text-generaton-webui Ollama Get started effortlessly deploying GGUF models for chat and web UI llamaspeak Talk live with Llama using Riva ASR/TTS, and chat about images with Llava! NanoLLM Optimized inferencing library for LLMs, multimodal agents, and speech. Small LLM (SLM) Deploy Small Language Models (SLM) with reduced memory usage and higher throughput. API Examples Learn how to write Python code for doing LLM inference using popular APIs."},{"location":"tutorial-intro.html#text-vision-vlm","title":"Text + Vision (VLM)","text":"<p>Give your locally running LLM an access to vision!</p> LLaVA Different ways to run LLaVa vision/language model on Jetson for visual understanding. Live LLaVA Run multimodal models interactively on live video streams over a repeating set of prompts. NanoVLM Use mini vision/language models and the optimized multimodal pipeline for live streaming. Llama 3.2 Vision Run Meta's multimodal Llama-3.2-11B-Vision model on Orin with HuggingFace Transformers."},{"location":"tutorial-intro.html#vision-transformers","title":"Vision Transformers","text":"EfficientVIT MIT Han Lab's EfficientViT, Multi-Scale Linear Attention for High-Resolution Dense Prediction NanoOWL OWL-ViT optimized to run real-time on Jetson with NVIDIA TensorRT NanoSAM NanoSAM, SAM model variant capable of running in real-time on Jetson SAM Meta's SAM, Segment Anything model TAM TAM, Track-Anything model, is an interactive tool for video object tracking and segmentation"},{"location":"tutorial-intro.html#image-generation","title":"Image Generation","text":"Cosmos Cosmos is a world model development platform that consists of world foundation models, tokenizers and video processing pipeline to accelerate the development of Physical AI at Robotics &amp; AV labs. Genesis Genesis is a physics platform designed for general-purpose Robotics/Embodied AI/Physical AI applications. Flux + ComfyUI Set up and run the ComfyUI with Flux model for image generation on Jetson Orin. Stable Diffusion Run AUTOMATIC1111's <code>stable-diffusion-webui</code> to generate images from prompts SDXL Ensemble pipeline consisting of a base model and refiner with enhanced image generation. nerfstudio Experience neural reconstruction and rendering with nerfstudio and onboard training."},{"location":"tutorial-intro.html#audio","title":"Audio","text":"Whisper OpenAI's Whisper, pre-trained model for automatic speech recognition (ASR) AudioCraft Meta's AudioCraft, to produce high-quality audio and music Voicecraft Interactive speech editing and zero shot TTS"},{"location":"tutorial-intro.html#rag-vector-database","title":"RAG &amp; Vector Database","text":"NanoDB Interactive demo to witness the impact of Vector Database that handles multimodal data LlamaIndex Realize RAG (Retrieval Augmented Generation) so that an LLM can work with your documents LlamaIndex Reference application for building your own local AI assistants using LLM, RAG, and VectorDB"},{"location":"tutorial-intro.html#api-integrations","title":"API Integrations","text":"ROS2 Nodes Optimized LLM and VLM provided as ROS2 nodes for robotics Holoscan SDK Use the Holoscan-SDK to run high-throughput, low-latency edge AI pipelines Jetson Platform Services Quickly build microservice driven vision applications with Jetson Platform Services Gapi Workflows Integrating generative AI into real world environments Gapi Micro Services Wrapping models and code to participate in systems Ultralytics YOLOv8 Run Ultralytics YOLOv8 on Jetson with NVIDIA TensorRT."},{"location":"tutorial-intro.html#about-nvidia-jetson","title":"About NVIDIA Jetson","text":"<p>Note</p> <p>We are mainly targeting Jetson Orin generation devices for deploying the latest LLMs and generative AI models.</p> Jetson AGX Orin 64GB Developer Kit Jetson AGX Orin Developer Kit Jetson Orin Nano Developer Kit GPU 2048-core NVIDIA Ampere architecture GPU with 64 Tensor Cores 1024-core NVIDIA Ampere architecture GPU with 32 Tensor Cores RAM(CPU+GPU) 64GB 32GB 8GB Storage 64GB eMMC (+ NVMe SSD) microSD card (+ NVMe SSD)"},{"location":"tutorial_api-examples.html","title":"Tutorial - API Examples","text":"<p>It's good to know the code for generating text with LLM inference, and ancillary things like tokenization, chat templates, and prompting.  On this page we give Python examples of running various LLM APIs, and their benchmarks.</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)\u26a0\ufe0f</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35) JetPack 6 (L4T r36)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>22GB</code> for <code>l4t-text-generation</code> container image</li> <li>Space for models (<code>&gt;10GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_api-examples.html#transformers","title":"Transformers","text":"<p>The HuggingFace Transformers API is the de-facto API that models are released for, often serving as the reference implementation.  It's not terribly fast, but it does have broad model support, and also supports quantization (AutoGPTQ, AWQ).  This uses streaming:</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\nfrom threading import Thread\n\nmodel_name='meta-llama/Llama-2-7b-chat-hf'\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map='cuda')\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nstreamer = TextIteratorStreamer(tokenizer)\n\nprompt = [{'role': 'user', 'content': 'Can I get a recipe for French Onion soup?'}]\ninputs = tokenizer.apply_chat_template(\n    prompt,\n    add_generation_prompt=True,\n    return_tensors='pt'\n).to(model.device)\n\nThread(target=lambda: model.generate(inputs, max_new_tokens=256, streamer=streamer)).start()\n\nfor text in streamer:\n    print(text, end='', flush=True)\n</code></pre> <p>To run this (it can be found here), you can mount a directory containing the script or your jetson-containers directory:</p> <pre><code>jetson-containers run --volume $PWD/packages/llm:/mount --workdir /mount \\\n  $(autotag l4t-text-generation) \\\n    python3 transformers/test.py\n</code></pre> <p>We use the <code>l4t-text-generation</code> container because it includes the quantization libraries in addition to Transformers, for running the quanztized versions of the models like <code>TheBloke/Llama-2-7B-Chat-GPTQ</code></p>"},{"location":"tutorial_api-examples.html#benchmarks","title":"Benchmarks","text":"<p>The <code>huggingface-benchmark.py</code> script will benchmark the models:</p> <pre><code>./run.sh --volume $PWD/packages/llm/transformers:/mount --workdir /mount \\\n  $(./autotag l4t-text-generation) \\\n    python3 huggingface-benchmark.py --model meta-llama/Llama-2-7b-chat-hf\n</code></pre> <pre><code>* meta-llama/Llama-2-7b-chat-hf  AVG = 20.7077 seconds,  6.2 tokens/sec  memory=10173.45 MB\n* TheBloke/Llama-2-7B-Chat-GPTQ  AVG = 12.3922 seconds, 10.3 tokens/sec  memory=7023.36 MB\n* TheBloke/Llama-2-7B-Chat-AWQ   AVG = 11.4667 seconds, 11.2 tokens/sec  memory=4662.34 MB\n</code></pre>"},{"location":"tutorial_api-examples.html#nanollm","title":"NanoLLM","text":"<p>The <code>NanoLLM</code> library uses the optimized MLC/TVM library for inference, like on the Benchmarks page:</p> <p></p> &gt; NanoLLM Reference Documentation<pre><code>from nano_llm import NanoLLM, ChatHistory, ChatTemplates\n\n# load model\nmodel = NanoLLM.from_pretrained(\n    model='meta-llama/Meta-Llama-3-8B-Instruct', \n    quantization='q4f16_ft', \n    api='mlc'\n)\n\n# create the chat history\nchat_history = ChatHistory(model, system_prompt=\"You are a helpful and friendly AI assistant.\")\n\nwhile True:\n    # enter the user query from terminal\n    print('&gt;&gt; ', end='', flush=True)\n    prompt = input().strip()\n\n    # add user prompt and generate chat tokens/embeddings\n    chat_history.append(role='user', msg=prompt)\n    embedding, position = chat_history.embed_chat()\n\n    # generate bot reply\n    reply = model.generate(\n        embedding, \n        streaming=True, \n        kv_cache=chat_history.kv_cache,\n        stop_tokens=chat_history.template.stop,\n        max_new_tokens=256,\n    )\n\n    # append the output stream to the chat history\n    bot_reply = chat_history.append(role='bot', text='')\n\n    for token in reply:\n        bot_reply.text += token\n        print(token, end='', flush=True)\n\n    print('\\n')\n\n    # save the inter-request KV cache \n    chat_history.kv_cache = reply.kv_cache\n</code></pre> <p>This example keeps an interactive chat running with text being entered from the terminal.  You can start it like this:</p> <pre><code>jetson-containers run \\\n  --env HUGGINGFACE_TOKEN=hf_abc123def \\\n  $(autotag nano_llm) \\\n    python3 -m nano_llm.chat.example\n</code></pre> <p>Or for easy editing from the host device, copy the source into your own script and mount it into the container with the <code>--volume</code> flag.  And for authenticated models, request access through HuggingFace (like with Llama) and substitute your account's API token above.</p>"},{"location":"tutorial_audiocraft.html","title":"Tutorial - AudioCraft","text":"<p>Let's run Meta's AudioCraft, to produce high-quality audio and music on Jetson!</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>10.7 GB</code> for <code>audiocraft</code> container image</li> <li>Space for checkpoints</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_audiocraft.html#how-to-start","title":"How to start","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run $(autotag audiocraft)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the Jupyter Lab server.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:8888</code>.</p> <p>The default password for Jupyter Lab is <code>nvidia</code>.</p>"},{"location":"tutorial_audiocraft.html#run-jupyter-notebooks","title":"Run Jupyter notebooks","text":"<p>AudioCraft repo comes with demo Jupyter notebooks.</p> <p>On Jupyter Lab navigation pane on the left, double-click <code>demos</code> folder. </p> <p></p>"},{"location":"tutorial_audiocraft.html#audiogen-demo","title":"AudioGen demo","text":"<p>Run cells with <code>Shift + Enter</code>, first one will download models, which can take some time.</p> <p>Info</p> <p>You may encounter an error message like the following when executing the first cell, but you can keep going. <pre><code>A matching Triton is not available, some optimizations will not be enabled.\nError caught was: No module named 'triton'\n</code></pre></p> <p>In the Audio Continuation cells, you can generate continuation based on text, while in Text-conditional Generation you can generate audio based just on text descriptions.</p> <p>You can also use your own audio as prompt, and use text descriptions to generate continuation: <pre><code>prompt_waveform, prompt_sr = torchaudio.load(\"../assets/sirens_and_a_humming_engine_approach_and_pass.mp3\") # you can upload your own audio\nprompt_duration = 2\nprompt_waveform = prompt_waveform[..., :int(prompt_duration * prompt_sr)]\noutput = model.generate_continuation(prompt_waveform.expand(3, -1, -1), prompt_sample_rate=prompt_sr,descriptions=[\n        'Subway train blowing its horn',   # text descriptions for continuation\n        'Horse neighing furiously',\n        'Cat hissing'\n], progress=True)\ndisplay_audio(output, sample_rate=16000)\n</code></pre></p>"},{"location":"tutorial_audiocraft.html#musicgen-and-magnet-demos","title":"MusicGen and MAGNeT demos","text":"<p>The two other jupyter notebooks are similar to AuidioGen, where you can generate continuation or generate audio, while using models trained to generate music.</p>"},{"location":"tutorial_comfyui_flux.html","title":"ComfyUI and Flux on Jetson Orin","text":"<p>Hey there, fellow developer! \ud83d\udc4b I'm excited to share with you our latest project: Flux, an open-source model for image generation. Here at NVIDIA, we're pushing the boundaries to make Flux work seamlessly across all platforms, including our Jetson Orin devices. While we're still fine-tuning the model for the Jetson Orin Nano, we've already got it running smoothly on the Jetson AGX Orin.</p> <p>In this tutorial, I'm going to walk you through every step needed to get Flux up and running on your Jetson Orin, even if you've just flashed your system. Follow along, and you should have no trouble getting everything set up. And hey, if something doesn't work out, reach out to me\u2014I\u2019ll keep this guide updated to make sure it's always on point.</p> <p></p> <p>So, let's dive in and get Flux running on your Jetson!</p>"},{"location":"tutorial_comfyui_flux.html#1-install-miniconda-and-create-a-python-310-environment","title":"1. Install Miniconda and Create a Python 3.10 Environment","text":"<p>First things first, you'll need to install Miniconda on your Jetson Orin and create a Python 3.10 environment called <code>comfyui</code>. This will ensure all dependencies are handled properly within an isolated environment.</p> <pre><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-aarch64.sh\nchmod +x Miniconda3-latest-Linux-aarch64.sh\n./Miniconda3-latest-Linux-aarch64.sh\n\nconda update conda\n\nconda create -n comfyui python=3.10\nconda activate comfyui\n</code></pre>"},{"location":"tutorial_comfyui_flux.html#2-install-cuda-cudnn-and-tensorrt","title":"2. Install CUDA, cuDNN, and TensorRT","text":"<p>Once your environment is set up, install CUDA 12.4 along with the necessary cuDNN and TensorRT libraries to ensure compatibility and optimal performance on your Jetson Orin.</p> <pre><code>wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/arm64/cuda-keyring_1.1-1_all.deb\nsudo dpkg -i cuda-keyring_1.1-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cuda-toolkit-12-4 cuda-compat-12-4\nsudo apt-get install cudnn python3-libnvinfer python3-libnvinfer-dev tensorrt\n</code></pre>"},{"location":"tutorial_comfyui_flux.html#3-verify-and-configure-cuda","title":"3. Verify and Configure CUDA","text":"<p>After installing CUDA, you'll want to verify that the correct version (12.4) is being used and make this change permanent in your environment.</p> <pre><code>ls -l /usr/local | grep cuda\nsudo ln -s /usr/local/cuda-12.4 /usr/local/cuda\n\nexport PATH=/usr/local/cuda/bin:$PATH\nnvcc --version\n\necho 'export PATH=/usr/local/cuda/bin:$PATH' &gt;&gt; ~/.bashrc\necho 'export CUDA_HOME=/usr/local/cuda' &gt;&gt; ~/.bashrc\necho 'export CUDA_PATH=/usr/local/cuda' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"tutorial_comfyui_flux.html#4-compile-and-install-bitsandbytes-with-cuda-support","title":"4. Compile and Install <code>bitsandbytes</code> with CUDA Support","text":"<p>Now it\u2019s time to compile and install <code>bitsandbytes</code> with CUDA support. This involves cloning the repository, configuring the build with CMake, compiling using all available cores, and installing the resulting package.</p> <pre><code>export BNB_CUDA_VERSION=124\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.4/lib64:$LD_LIBRARY_PATH\n\ngit clone https://github.com/timdettmers/bitsandbytes.git\ncd bitsandbytes\n\nmkdir -p build\ncd build\ncmake .. -DCOMPUTE_BACKEND=cuda -DCUDA_TOOLKIT_ROOT_DIR=/usr/local/cuda-12.4\nmake -j$(nproc)\n\ncd ..\npython setup.py install\n</code></pre> <p>Verify the installation by importing the package in Python:</p> <pre><code>python\n&gt;&gt;&gt; import bitsandbytes as bnb\n&gt;&gt;&gt; print(bnb.__version__)\n</code></pre>"},{"location":"tutorial_comfyui_flux.html#5-install-pytorch-torchvision-and-torchaudio","title":"5. Install PyTorch, TorchVision, and TorchAudio","text":"<p>Next up, install the essential libraries <code>PyTorch</code>, <code>torchvision</code>, and <code>torchaudio</code> for Jetson Orin. You can always check for the latest links here.</p> <pre><code>pip install http://jetson.webredirect.org/jp6/cu124/+f/5fe/ee5f5d1a75229/torch-2.3.0-cp310-cp310-linux_aarch64.whl\npip install http://jetson.webredirect.org/jp6/cu124/+f/988/cb71323efff87/torchvision-0.18.0a0+6043bc2-cp310-cp310-linux_aarch64.whl\npip install http://jetson.webredirect.org/jp6/cu124/+f/0aa/a066463c02b4a/torchaudio-2.3.0+952ea74-cp310-cp310-linux_aarch64.whl\n</code></pre>"},{"location":"tutorial_comfyui_flux.html#6-clone-the-comfyui-repository","title":"6. Clone the ComfyUI Repository","text":"<p>Clone the ComfyUI repository from GitHub to get the necessary source code.</p> <pre><code>git clone https://github.com/comfyanonymous/ComfyUI.git\ncd ComfyUI\n</code></pre>"},{"location":"tutorial_comfyui_flux.html#7-update-dependencies","title":"7. Update Dependencies","text":"<p>Make sure all the necessary dependencies are installed by running the <code>requirements.txt</code> file.</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"tutorial_comfyui_flux.html#8-resolve-issues-with-numpy","title":"8. Resolve Issues with NumPy","text":"<p>If you encounter issues with NumPy, downgrade to a version below 2.0 to avoid compatibility problems.</p> <pre><code>pip install \"numpy&lt;2\"\n</code></pre>"},{"location":"tutorial_comfyui_flux.html#9-run-comfyui","title":"9. Run ComfyUI","text":"<p>Finally, run ComfyUI to ensure everything is set up correctly.</p> <pre><code>python main.py\n</code></pre> <p>Great! Now that you\u2019ve got ComfyUI up and running, let's load the workflow to start using the Flux model. </p> <ul> <li>Download the workflow file using this link. And load it from the ComfyUI interface.</li> <li>You\u2019ll need to download the Flux Schnell model <code>flux1-schnell.safetensors</code> and vae <code>ae.safetensors</code> from Hugging Face and place the model in the <code>models/unet</code> folder and vae in <code>models/vae</code> within ComfyUI.</li> <li>Download <code>clip_l.safetensors</code> and <code>t5xxl_fp8_e4m3fn.safetensors</code> from Stability's Hugging Face and place them inside <code>models/clip</code> folder.</li> </ul> <p>Alright, you're all set to launch your first run! Head over to the URL provided by ComfyUI (127.0.0.1:8188) on your Jetson AGX Orin, and hit that Queue Prompt button. The first time might take a little longer as the model loads, but after that, each generation should take around 21 seconds. Plus, you can queue up multiple prompts and let it generate images for hours!!</p> <p>Happy generating! \ud83c\udf89</p> <p>ASIER \ud83d\ude80</p> <p>Some examples: </p> <p> </p>"},{"location":"tutorial_distillation.html","title":"CLIP model distillation","text":"<p>See \"Jetson Introduction to Knowledge Distillation\" repo's README.md.</p> <p>https://github.com/NVIDIA-AI-IOT/jetson-intro-to-distillation</p>"},{"location":"tutorial_gapi_microservices.html","title":"Tutorial - Gapi Micro Services","text":"<p>A Micro Service is a process that runs a wrapper python script that integrates your custom code/models so they can integrate into Gapi Workflows.</p> <p></p> <p>You can run a Micro Service wherever you like and connect it to a Gapi Server via the streaming, hybrid binary+json message protocol.</p> <p></p> <p>There are some out-of-the-box \u201cCommunity Micro Services\" that we integrate, test and pack into Docker images. When you run them, they auto integrate, load NVIDIA layers correctly and offer logging to the host system.</p> <p></p> <p>Even better, create your own! Just implement an on_message Python handler to process requests and respond. The rest is handled for you.</p> <p></p> <p>Think of a Micro Service as a simple wrapper to some code or model you have. It works the same as any other Node inside a Workflow. When it's your Micro Service Node's turn your on_message function will be called. Your script gets the rolling Transaction data for context and you then publish your data directly back into the flow.</p> <p></p>"},{"location":"tutorial_gapi_microservices.html#running-the-community-micro-services","title":"Running the Community Micro Services","text":"<p>Requirements for Community Micro Services</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB) Jetson Orin Nano (4GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>Sufficient storage space.</p> <ul> <li>Literally 4KB for your own Micro Service</li> <li>Anywhere from ~4GB to ~10GB for each Community Micro Service</li> </ul> </li> </ol> <pre><code>#1 Login and go to the Micro Services tab\n#2 Follow the instructions in the blue box on that page to download your custom configuration\n#3 Then follow the instructions below that for installing the Micro Service you want\n</code></pre> <p>Example of instruction page: </p> <p></p>"},{"location":"tutorial_gapi_microservices.html#congrats-you-can-go-through-the-workflow-tips-now","title":"Congrats! You Can Go Through the Workflow Tips Now","text":""},{"location":"tutorial_gapi_microservices.html#creating-your-own-micro-service","title":"Creating Your Own Micro Service","text":"<p>The entire Micro Service zip file is just 4KB with 4 files:</p> <ul> <li>message_handler.py: for you to respond</li> <li>message.py: for the streaming binary/json protocol</li> <li>gapi-ms: as entry point and handler)</li> <li>requirements.txt: defines just asyncio + websockets</li> </ul> <p>Full documentation here: Gapi Micro Service Docs. Synopsis below...</p> <pre><code>#1 Create logical Micro Service in UI and copy the key\n#2 Download the zip file from the UI\n#3 python gapi-ms.py ws://0.0.0.0:8090/gapi-ws [MICROSERVICE_KEY]\n#4 Refresh the UI to confirm it's online\n#5 Edit the message_handler.py to handle binary+json input and change the output\n#6 Add a Micro Service Node to a Workflow and tie it to your Micro Service. Hit Test.\n</code></pre>"},{"location":"tutorial_gapi_workflows.html","title":"Tutorial - Gapi","text":"<p>Gapi is an embeddable API gateway that creates streaming integrations between AI micro services and the systems that users leverage everyday.</p> <p>The project's goal is to accelerate the speed of creating pilots and demos of Jetson AI Lab achievements into real world environments</p> <p>\"On Device\" generative AI doesn't mean it has to live on an island!</p> <p></p> <ul> <li>Workflow engine with low code UI with dozens of open integrations and customizable clients for mobile web and desktop.</li> <li>Micro service framework for wrapping Jetson containers (Ollama, Whisper, Piper TTS, etc. are done, with more coming). Or wrap your own models/code and integrate it into Workflows.</li> <li>Real-time, hybrid, binary+json messaging smoothens intra-service calls and reduced latency. </li> <li>A fast path to proving generative AI value to stakeholders in their actual environment.</li> </ul>"},{"location":"tutorial_gapi_workflows.html#gapi-server","title":"Gapi Server","text":"<p>Embeddable API gateway software that runs in the background with a low code workflow UI for testing. The server is a message hub and state machine for workflow 'nodes' that talk to Micro Services. Think of it as connective-tissue for applications.</p> <p></p> <p>A Micro Service is a process that runs some wrapper python scripts that integrates custom code/models into Workflows using a streaming API.</p> <ul> <li> <p>Gapi Server can run on any Jetson Orin or really any computer as the Micro Services connect outbound over secure web sockets. It doesn't use any GPU resources. There is a also a little demo version to skip the Server install (but you'll still need to run your own Micro Services).</p> </li> <li> <p>Gapi Project Page</p> </li> <li>Gapi Github</li> <li>Gapi Docs</li> <li>Gapi Hosted Demo</li> </ul> <p>What you need to run Gapi Server on Jetson</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin NX (8GB) Jetson Orin Nano (8GB) Jetson Orin Nano (4GB)</p> </li> <li> <p>Docker</p> </li> <li> <p>Sufficient storage space.</p> <ul> <li>Size: ~1.3GB</li> </ul> </li> </ol> <p>Gapi Server will run on other environments. Email us at support@GenAINerds.com if that's something you think is worthwhile.</p> <p>Explaining the Steps:</p> <ul> <li>1) On the Docker host, create working dir for persistant data</li> <li>2) Download configuration files</li> <li>3) Unzip</li> <li>4) Pull Docker image, create container and start the process (will return console to you)</li> </ul> <p>Copy and Run the Steps:</p> <pre><code>mkdir ~/gapiData &amp;&amp; cd ~/gapiData\ncurl -L https://raw.githubusercontent.com/genai-nerds/Gapi/main/gapiConfigs.zip -o gapiConfigs.zip\nunzip -q gapiConfigs.zip\ndocker run -d --name gapi --network host -v ~/gapiData:/opt/gapi/vdata genainerds/gapi:arm64 /bin/bash -c \"cd /opt/gapi/bin &amp;&amp; ./startGapi.sh\"\necho \"You may need to hit Enter now. Afterwards the Docker container 'gapi' should be running\"\n</code></pre> <p>Troubleshooting:</p> <ul> <li>Keep in mind all data read or written is in ~/gapiData</li> <li>Look at ~/gapiData/gapi.log to see what happened (if say the docker run command doesn't work)</li> <li>gapiServerConfig.json has all the initial setup</li> </ul> <p>NOTE: You will need to run some Micro Services before doing anything meaningful, so please review the mini tour below but don't do any of it in the UI untill you complete the setup (instructions at the bottom)</p>"},{"location":"tutorial_gapi_workflows.html#ui","title":"UI","text":"<ul> <li>Browse in: http://[host-device-ip]:8090</li> <li>User: root</li> <li> <p>Pass: !gapi2024</p> </li> <li> <p>Change password in Settings! Docs shows how to add SSL cert.</p> </li> </ul>"},{"location":"tutorial_gapi_workflows.html#tips-use-case-templates","title":"Tips &amp; Use Case Templates","text":"<p> When you login there will be an array of Tip Workflows that have notes and explain core concepts.</p> <p>Tips:</p> <ul> <li>Hello World: Basics plus it literally says hello</li> <li>Run a Local LLM: Play with Ollama graphically and connect it to other systems</li> <li>Streaming Speech to Text: PiperTTS</li> <li>Querying a Vector Database: Query a simple set of vectorized text documents</li> <li>Variables, Flow and Logic: Understand how to setup more robust workflows</li> <li>Calling Workflows from Outside Gapi: Configure Webhooks</li> <li>Workflows Calling Your Code: Micro Service Nodes that invoke your code</li> <li>Communications: 3rd party communications like Slack (IM), Twilio (SMS), SendGrid (EMAIL)</li> </ul>"},{"location":"tutorial_gapi_workflows.html#workflows","title":"Workflows","text":"<p>Workflows visually connect the execution and flow of data between Nodes. </p> <p>A Transaction (or single firing) has \"Rolling Input\" data it accumulates as Node to Node steps each talk to Micro Services and APIs. All Nodes enjoy variables and flow control using familiar json and javascript concepts.</p> <p></p> <p>Each Node can append or reference the rolling data in the Transaction while making decisions along the way.</p> <p></p> <p>Watch live Transactions as they start from clients, webhooks and published messages from Micro Services with visual feedback and debugging.</p>"},{"location":"tutorial_gapi_workflows.html#apis-to-business-systems","title":"APIs to Business Systems","text":"<p>Gapi can help smoothly integrate generative AI into systems that people already use everyday via APIs. It has the streaming API to Micro Services plus the state management and chops to handle the outward (webhook) style APIs to existing systems.</p> <p>Our hope is to rally contributions by the community to keep growing the out-of-the-box/tested Nodes but there is a DIY one as well to manually map what you need into your Workflows.</p> <p>Some of the out-of-the-box API Nodes: Slack (IM), Twilio (SMS), SendGrid (Email), Service Now (Ticketing), DIY Webhook</p>"},{"location":"tutorial_gapi_workflows.html#micro-services","title":"Micro Services","text":"<p>There are community published Micro Services as well as custom ones you can make yourself. Gapi Server becomes most useful when leveraging them so please follow the How To below.</p> <p>Current Community Micro Services:</p> <ul> <li>Whisper</li> <li>Ollama</li> <li>Vector</li> <li>Text to Speech</li> <li>Img to Text</li> </ul> <p>Complete the Setup: How To Run and/or Create Micro Services</p>"},{"location":"tutorial_gapi_workflows.html#support-contribute","title":"Support / Contribute","text":"<p>Gapi is a project from the GenAI Nerds and hosted on Github.</p> <ul> <li>Ask a question, support@GenAINerds.com or</li> <li>Say hello, hello@GenAINerds.com</li> <li>Contribute/create tickets on Github</li> </ul>"},{"location":"tutorial_holoscan.html","title":"Tutorial - Holoscan SDK","text":"<p>The Holoscan SDK is part of NVIDIA Holoscan, the AI sensor processing platform that combines hardware systems for low-latency sensor and network connectivity, optimized libraries for data processing and AI, and core microservices to run streaming, imaging, and other applications, from embedded to edge to cloud.</p>"},{"location":"tutorial_holoscan.html#holoscan-sdk-core-concepts","title":"Holoscan SDK - Core Concepts","text":"<p>A Holoscan SDK <code>Application</code> is composed of <code>Fragments</code>, each of which runs a graph of <code>Operators</code>. The implementation of that graph is sometimes referred to as a pipeline or workflow, which can be visualized below: </p>"},{"location":"tutorial_holoscan.html#holoscan-sdk-getting-started-on-jetson","title":"Holoscan SDK - Getting Started on Jetson","text":"<p>The best place to get started using the Holoscan SDK is the HoloHub repo. This is a central repository for the NVIDIA Holoscan AI sensor processing community to share apps and extensions.</p> <p>So, let's walk through how to run the Surgical Tool Tracking example application from HoloHub!</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>13.7 GB</code> for <code>efficientvit</code> container image</li> <li><code>850 Mb</code> for Tool Tracking ONNX model + example video</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_holoscan.html#launching-a-holoscan-compatible-container","title":"Launching a Holoscan-compatible Container","text":"<p>Use the <code>jetson-containers run</code> and <code>autotag</code> commands to automatically pull or build the Holoscan container.</p> <p>Use the <code>-v</code> option to mount HoloHub's <code>./build</code> and <code>./data</code> directories to the local Holoscan package directory so builds and data are cached across runs</p> <pre><code>jetson-containers run \\\n    -v ./packages/holoscan/holohub/data:/opt/nvidia/holohub/data \\\n    -v ./packages/holoscan/holohub/build:/opt/nvidia/holohub/build \\\n    $(autotag holoscan)\n</code></pre>"},{"location":"tutorial_holoscan.html#running-holohubs-endoscopy-tool-tracking-app","title":"Running HoloHub's Endoscopy Tool Tracking App","text":"<p>An example application from HoloHub is the Endoscopy Tool Tracking application. This sample application demonstrates how the Holoscan SDK can be used to build an efficient pipeline that streams a video feed, preprocesses the data, runs inference using TensorRT, post-processes the data, and renders the video feed with the inference overlays.</p> <p></p>"},{"location":"tutorial_holoscan.html#building-the-app","title":"Building The App","text":"<p>The Holoscan SDK uses CMake to build C++ applications and also leverages CMake to pull and build app dependencies. So, regardless of whether an application is implemented using C++ or Python, many apps will still require that you \"build\" them first.</p> <p>The Endoscopy Tool Tracking App has both a Python and C++ implementation. Building this app creates the C++ application program, pulls in an example video, and builds the TensorRT engine used for inference.</p> <p>Go to the HoloHub directory <pre><code>cd /opt/nvidia/holohub\n</code></pre> Build the app using the 'run' script <pre><code>./run build endoscopy_tool_tracking\n</code></pre></p>"},{"location":"tutorial_holoscan.html#running-the-python-app","title":"Running The Python App","text":"<p>First, add the Holoscan SDK and the HoloHub build directory to your PYTHONPATH environment variable. <pre><code>export HOLOHUB_BUILD_PATH=/opt/nvidia/holohub/build/endoscopy_tool_tracking\nexport PYTHONPATH=$PYTHONPATH:$HOLOHUB_BUILD_PATH/python/lib:/opt/nvidia/holoscan/python/lib\n</code></pre> Next, run the application using Python! <pre><code>python3 /opt/nvidia/holohub/applications/endoscopy_tool_tracking/python/endoscopy_tool_tracking.python --data /opt/nvidia/holohub/data/endoscopy/\n</code></pre></p> <p>Shortly after launching the application, you will see the HoloViz window that visualizes the sample video and the model's outputs:</p> <p></p>"},{"location":"tutorial_holoscan.html#running-the-c-app","title":"Running The C++ App","text":"<p>The C++ app can be run using the run script by specifying the app name: <pre><code>./run launch endoscopy_tool_tracking\n</code></pre></p>"},{"location":"tutorial_holoscan.html#next-steps","title":"Next Steps","text":"<p>Congratulations! You've successfully run a Holoscan SDK application!</p> <p>To dive deeper into the Holoscan SDK, explore the SDK's documentation on Core Concepts, Holoscan by Example, and Creating an Application.</p>"},{"location":"tutorial_jetson-copilot.html","title":"Tutorial - Jetson Copilot","text":"<p>Jetson Copilot is a reference application for a local AI assistant, which demonstrates;</p> <ul> <li>Running open-source LLMs (large language models) on device</li> <li>RAG (retrieval-augmented generation) to let LLM have access to your locally indexed knowledge</li> </ul> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin 64GB Developer Kit Jetson AGX Orin (32GB) Developer Kit Jetson Orin Nano 8GB Developer Kit</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6 GB</code> for <code>jetrag</code> container image</li> <li>About <code>4 GB</code> for downloading some default models (<code>llama3</code> and <code>mxbai-embed-large</code>)</li> </ul> </li> </ol> <p>Info</p> <p>To run Jetson Copilot, you do not need to have <code>jetson-containers</code> on your system. It uses the <code>jetrag</code> container image that is being managed and built on <code>jetson-containers</code>.</p>"},{"location":"tutorial_jetson-copilot.html#getting-started","title":"\ud83c\udfc3 Getting started","text":""},{"location":"tutorial_jetson-copilot.html#first-time-setup","title":"First time setup","text":"<p>If this is your first time to run Jetson Copilot on your Jetson, first run <code>setup.sh</code> to ensure you have all the necessary software installed and the environment set up. </p> <pre><code>git clone https://github.com/NVIDIA-AI-IOT/jetson-copilot/\ncd jetson-copilot\n./setup_environment.sh\n</code></pre> <p>It will install the following, if not yet.</p> <ul> <li>Chromium web browser</li> <li>Docker</li> </ul>"},{"location":"tutorial_jetson-copilot.html#how-to-start-jetson-copilot","title":"How to start Jetson Copilot","text":"<pre><code>cd jetson-copilot\n./launch_jetson_copilot.sh\n</code></pre> <p>This will start a Docker container and start a Ollama server and Streamlit app inside the container. It will shows the URL on the console in order to access the web app hosted on your Jetson.</p> <p>With your web browser on Jetson, open the Local URL (<code>http://localhost:8501</code>).Or on a PC connected on the same network as on your Jetson, access the Network URL.</p> <pre><code>Local URL: http://localhost:8501\nNetwork URL: http://10.110.50.252:8501 \n</code></pre> <p>Info</p> <p>You will need the Internet connection on Jetson when it launches for the first time, as it will pull the container image (and download the default LLM and embedding model when web UI starts for the first time).</p> <p>When you access the web UI for the first time, it will dowload the default LLM (<code>llama3</code>) and the embedding model (<code>mxbai-embed-large</code>).</p> <p>Tips</p> <p>If you are on Ubuntu Desktop, a frameless Chromium window will pop up to access the web app, to make it look like an independent application. You need to close the window as stopping the container on the console won't shutdown Chromium.</p> <p></p>"},{"location":"tutorial_jetson-copilot.html#how-to-use-jetson-copilot","title":"\ud83d\udcd6 How to use Jetson Copilot","text":""},{"location":"tutorial_jetson-copilot.html#0-interact-with-the-plain-llama3-8b","title":"0. Interact with the plain Llama3 (8b)","text":"<p>You can use Jetson Copilot just to interact with a LLM withut enabling RAG feature.</p> <p>By default, Llama3 (8b) model is downloaded when running for the first time and use as the default LLM.</p> <p>You will be surprized how much a model like Llama3 is capable, but may soon find limitations as it does not have information prior to its cutoff date nor know anything about your specific subject matter.</p>"},{"location":"tutorial_jetson-copilot.html#1-ask-jetson-related-question-using-pre-built-index","title":"1. Ask Jetson related question using pre-built index","text":"<p>On the side panel, you can toggle \"Use RAG\" on to enable RAG pipeline. The LLM will have an access to a custom knowledge/index that is selected under \"Index\".</p> <p>As a sample, a pre-build index \"<code>_L4T_README</code>\" is provided. This is built on all the README text files that supplied in the \"L4T-README\" folder on the Jetson desktop.</p> <p>It is mounted as <code>/media/&lt;USER_NAME&gt;/L4T-README/</code> once you execute <code>udisksctl mount -b /dev/disk/by-label/L4T-README</code>.</p> <p>You can ask questions like:</p> <pre><code>What IP address does Jetson gets assigned when connected to a PC via a USB cable in USB Device Mode?\n</code></pre>"},{"location":"tutorial_jetson-copilot.html#2-build-your-own-index-based-on-your-documents","title":"2. Build your own index based on your documents","text":"<p>You can build your own index based on your local and/or online documents.</p> <p>First, on the console (or on the desktop) create a directory under <code>Documents</code> directory to store your documents.</p> <pre><code>cd jetson-copilot\nmkdir Documents/Jetson-Orin-Nano\ncd Documents/Jetson-Orin-Nano\nwget https://developer.nvidia.com/downloads/assets/embedded/secure/jetson/orin_nano/docs/jetson_orin_nano_devkit_carrier_board_specification_sp.pdf\n</code></pre> <p>Now back on the web UI, open the side bar, toggle on \"Use RAG\", then click on \"\u2795Build a new index\" to jump to a \"Build Index\" page.</p> <p>Give a name for the Index you are to build. (e.g. \"JON Carrier Board\") Type in the field and hit <code>Enter</code> key, then it will check and show what path will be created for your index.</p> <p></p> <p>And then from the drop select box under \"Local documents\", select the directory you created and saved your documents in. (e.g. <code>/opt/jetson_copilot/Documents/Jetson-Orin-Nano</code>).</p> <p>It will show the summary of files found in the selected directory.</p> <p></p> <p>If you want to rather only or additionally supply URLs for the online docuemnts to be ingested, fill the text area with one URL per a line. You can skip this if you are building your index only based on your local documents.</p> <p>Info</p> <p>On the sidebar, make sure <code>mxbai-embed-large</code> is selected for the embedding model.</p> <p>Use of OpenAI embedding models is not well supported and needs more testing.</p> <p>Finally, hit \"Build Index\" button. It will show the progress in the drop-down \"status container\", so you can check the status by clicking on it. Once done, it will show the summary of your index and time it took.</p> <p>You can go back to the home screen to now select the index you just built.</p>"},{"location":"tutorial_jetson-copilot.html#3-test-different-llm-or-embedding-model","title":"3. Test different LLM or Embedding model","text":"<p>TODO</p>"},{"location":"tutorial_jetson-copilot.html#development","title":"\ud83c\udfd7\ufe0f Development","text":"<p>Streamlit based web app is very easy to develop.</p> <p>On web UI, at the top-right of the screen to choose \"Always rerun\" to automatically update your app every time you change the source codes.</p> <p>See Streamlit Documentation for the detail.</p>"},{"location":"tutorial_jetson-copilot.html#manually-run-streamlit-app-inside-the-container","title":"Manually run streamlit app inside the container","text":"<p>In case you make more fundamental changes, you can also manually run streamlit app.</p> <pre><code>cd jetson-copilot\n./launch_dev.sh\n</code></pre> <p>Once in container;</p> <pre><code>streamlit run app.py\n</code></pre>"},{"location":"tutorial_jetson-copilot.html#directory-structure","title":"\ud83e\uddf1 Directory structure","text":"<pre><code>\u2514\u2500\u2500 jetson-copilot\n    \u251c\u2500\u2500 launch_jetson_copilot.sh\n    \u251c\u2500\u2500 setup_environment.sh\n    \u251c\u2500\u2500 Documents \n    \u2502   \u2514\u2500\u2500 your_abc_docs\n    \u251c\u2500\u2500 Indexes\n    \u2502   \u251c\u2500\u2500 _L4T_README\n    \u2502   \u2514\u2500\u2500 your_abc_index\n    \u251c\u2500\u2500 logs\n    \u2502   \u251c\u2500\u2500 container.log\n    \u2502   \u2514\u2500\u2500 ollama.log\n    \u251c\u2500\u2500 ollama_models\n    \u2514\u2500\u2500 Streamlit_app\n        \u251c\u2500\u2500 app.py\n        \u251c\u2500\u2500 build_index.py\n        \u2514\u2500\u2500 download_model.py\n</code></pre> <p>Following directories inside the <code>jetson-copilot</code> directory are mounted in the Docker container.</p> Directory Name Description <code>Docuemtns</code> Directory to store your documents to be indexed <code>Indexes</code> Directory to store pre-built (or built-by-you) indexes for LLM to perform RAG on <code>logs</code> Directory for the app to store log files <code>ollama_models</code> Directory for the ollama server to store download models <code>stremlit_app</code> Directory for Python scripts to make up the web app"},{"location":"tutorial_jetson-copilot.html#troubleshooting","title":"\ud83d\udcab Troubleshooting","text":"<p>If you find any issue, please check GitHub Issues of the Jetson Copilot repo.</p>"},{"location":"tutorial_jps.html","title":"Tutorial - Jetson Platform Services","text":"<p>Jetson Plaform Services (JPS) provide a platform to simplify development, deployment and management of Edge AI applications on NVIDIA Jetson. JPS is a modular &amp; extensible architecture for developers to distill large complex applications into smaller modular microservice with APIs to integrate into other apps &amp; services. At its core are a collection of AI services leveraging generative AI, deep learning, and analytics, which provide state of the art capabilities including video analytics, video understanding and summarization, text based prompting, zero shot detection and spatio temporal analysis of object movement. </p> <p> VLM Alert Workflow built with JPS</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> </li> </ol> <p>To get started with Jetson Platform Services, follow the quickstart guide to install and setup JPS. Then explore the reference workflows to learn how to use DeepStream, Analytics, Generative AI and more with JPS:</p> <p>1) Quick Start Guide 2) AI Powered Network Video Recorder 3) Zero Shot Detection with NanoOWL 4) Visual Language Model Alerts </p> <p>The reference workflows demonstrate how to use the microservices provided in JPS to build full end to end systems on your Jetson. </p> <p> VLM Alert Workflow Architecture</p> <p>View the links below to learn more about Jetson Platform Services: VLM Alert Blog JPS Product Page JPS Documentation  VLM Alert Demo Video</p>"},{"location":"tutorial_live-llava.html","title":"Tutorial - Live LLaVA","text":"<p>Recommended</p> <p>Follow the NanoVLM tutorial first to familiarize yourself with vision/language models, and see Agent Studio for in interactive pipeline editor built from live VLMs.</p> <p>This multimodal agent runs a vision-language model on a live camera feed or video stream, repeatedly applying the same prompts to it:</p> <p></p> <p>It uses models like LLaVA or VILA and has been quantized with 4-bit precision.  This runs an optimized multimodal pipeline from the <code>NanoLLM</code> library, including running the CLIP/SigLIP vision encoder in TensorRT, event filters and alerts, and multimodal RAG (see the NanoVLM page for benchmarks)</p> <p></p>"},{"location":"tutorial_live-llava.html#running-the-live-llava-demo","title":"Running the Live Llava Demo","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)\u26a0\ufe0f</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>22GB</code> for <code>nano_llm</code> container image</li> <li>Space for models (<code>&gt;10GB</code>)</li> </ul> </li> <li> <p>Follow the chat-based LLaVA and NanoVLM tutorials first.</p> </li> <li> <p>Supported vision/language models:</p> <ul> <li><code>liuhaotian/llava-v1.5-7b</code>, <code>liuhaotian/llava-v1.5-13b</code>, <code>liuhaotian/llava-v1.6-vicuna-7b</code>, <code>liuhaotian/llava-v1.6-vicuna-13b</code></li> <li><code>Efficient-Large-Model/VILA-2.7b</code>,<code>Efficient-Large-Model/VILA-7b</code>, <code>Efficient-Large-Model/VILA-13b</code></li> <li><code>Efficient-Large-Model/VILA1.5-3b</code>,<code>Efficient-Large-Model/Llama-3-VILA1.5-8B</code>, <code>Efficient-Large-Model/VILA1.5-13b</code></li> <li><code>VILA-2.7b</code>, <code>VILA1.5-3b</code>, <code>VILA-7b</code>, <code>Llava-7b</code>, and <code>Obsidian-3B</code> can run on Orin Nano 8GB</li> </ul> </li> </ol> <p>The VideoQuery agent applies prompts to the incoming video feed with the VLM.  Navigate your browser to <code>https://&lt;IP_ADDRESS&gt;:8050</code> after launching it with your camera (Chrome is recommended with <code>chrome://flags#enable-webrtc-hide-local-ips-with-mdns</code> disabled)</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.agents.video_query --api=mlc \\\n    --model Efficient-Large-Model/VILA1.5-3b \\\n    --max-context-len 256 \\\n    --max-new-tokens 32 \\\n    --video-input /dev/video0 \\\n    --video-output webrtc://@:8554/output\n</code></pre> <p>This uses <code>jetson_utils</code> for video I/O, and for options related to protocols and file formats, see Camera Streaming and Multimedia.  In the example above, it captures a V4L2 USB webcam connected to the Jetson (under the device <code>/dev/video0</code>) and outputs a WebRTC stream.</p>"},{"location":"tutorial_live-llava.html#processing-a-video-file-or-stream","title":"Processing a Video File or Stream","text":"<p>The example above was running on a live camera, but you can also read and write a video file or network stream by substituting the path or URL to the <code>--video-input</code> and <code>--video-output</code> command-line arguments like this:</p> <pre><code>jetson-containers run \\\n  -v /path/to/your/videos:/mount\n  $(autotag nano_llm) \\\n    python3 -m nano_llm.agents.video_query --api=mlc \\\n      --model Efficient-Large-Model/VILA1.5-3b \\\n      --max-context-len 256 \\\n      --max-new-tokens 32 \\\n      --video-input /mount/my_video.mp4 \\\n      --video-output /mount/output.mp4 \\\n      --prompt \"What does the weather look like?\"\n</code></pre> <p>This example processes and pre-recorded video (in MP4, MKV, AVI, FLV formats with H.264/H.265 encoding), but it also can input/output live network streams like RTP, RTSP, and WebRTC using Jetson's hardware-accelerated video codecs.</p>"},{"location":"tutorial_live-llava.html#nanodb-integration","title":"NanoDB Integration","text":"<p>If you launch the VideoQuery agent with the <code>--nanodb</code> flag along with a path to your NanoDB database, it will perform reverse-image search on the incoming feed against the database by re-using the CLIP embeddings generated by the VLM.</p> <p>To enable this mode, first follow the NanoDB tutorial to download, index, and test the database.  Then launch VideoQuery like this:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.agents.video_query --api=mlc \\\n    --model Efficient-Large-Model/VILA1.5-3b \\\n    --max-context-len 256 \\\n    --max-new-tokens 32 \\\n    --video-input /dev/video0 \\\n    --video-output webrtc://@:8554/output \\\n    --nanodb /data/nanodb/coco/2017\n</code></pre> <p>You can also tag incoming images and add them to the database using the web UI, for one-shot recognition tasks:</p>"},{"location":"tutorial_live-llava.html#video-vila","title":"Video VILA","text":"<p>The VILA-1.5 family of models can understand multiple images per query, enabling video search/summarization, action &amp; behavior analysis, change detection, and other temporal-based vision functions.  The <code>vision/video.py</code> example keeps a rolling history of frames:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.vision.video \\\n    --model Efficient-Large-Model/VILA1.5-3b \\\n    --max-images 8 \\\n    --max-new-tokens 48 \\\n    --video-input /data/my_video.mp4 \\\n    --video-output /data/my_output.mp4 \\\n    --prompt 'What changes occurred in the video?'\n</code></pre> <p></p>"},{"location":"tutorial_live-llava.html#python-code","title":"Python Code","text":"<p>For a simplified code example of doing live VLM streaming from Python, see here in the NanoLLM docs. </p> <p>You can use this to implement customized prompting techniques and integrate with other vision pipelines.  This code applies the same set of prompts to the latest image from the video feed.  See here for the version that does multi-image queries on video sequences.</p>"},{"location":"tutorial_live-llava.html#walkthrough-videos","title":"Walkthrough Videos","text":""},{"location":"tutorial_llamaindex.html","title":"Tutorial - LlamaIndex","text":"<p>Let's use LlamaIndex, to realize RAG (Retrieval Augmented Generation) so that an LLM can work with your documents!</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin 64GB Developer Kit Jetson AGX Orin (32GB) Developer Kit Jetson Orin Nano 8GB Developer Kit</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>5.5 GB</code> for <code>llama-index</code> container image</li> <li>Space for checkpoints</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_llamaindex.html#how-to-start-a-container-with-samples","title":"How to start a container with samples","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run $(autotag llama-index:samples)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the Jupyter Lab server.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:8888</code>.</p> <p>The default password for Jupyter Lab is <code>nvidia</code>.</p> <p>You can follow along <code>LlamaIndex_Local-Models_L4T.ipynb</code> (which is based on the official LlamaIndex tutorial). </p> <p></p>"},{"location":"tutorial_llamaspeak.html","title":"Tutorial - llamaspeak","text":"<p>Talk live with Llama using streaming ASR/TTS, and chat about images with Llava!</p> <p></p> <ul> <li>The <code>NanoLLM</code> library provides optimized inference for LLM and speech models.</li> <li>It's recommended to run JetPack 6.0 to be able to run the latest containers.</li> </ul> <p>The <code>WebChat</code> agent has responsive conversational abilities and multimodal support for chatting about images with vision/language models, including overlapping ASR/LLM/TTS generation and verbal interruptability.</p>"},{"location":"tutorial_llamaspeak.html#running-llamaspeak","title":"Running llamaspeak","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>22GB</code> for <code>nano_llm</code> container image</li> <li>Space for models (<code>&gt;10GB</code>)</li> </ul> </li> <li> <p>Start the Riva server first and test the ASR examples.</p> </li> </ol> <pre><code>jetson-containers run --env HUGGINGFACE_TOKEN=hf_xyz123abc456 \\\n  $(autotag nano_llm) \\\n  python3 -m nano_llm.agents.web_chat --api=mlc \\\n    --model meta-llama/Meta-Llama-3-8B-Instruct \\\n    --asr=riva --tts=piper\n</code></pre> <p>This will start llamaspeak with text LLM and ASR/TTS enabled.  You can then navigate your browser to <code>https://IP_ADDRESS:8050</code> <ul> <li>The default port is 8050, but can be changed with <code>--web-port</code> (and <code>--ws-port</code> for the websocket port)</li> <li>During bot replies, the TTS model will pause output if you speak a few words in the mic to interrupt it.</li> <li>Request access to the Llama models on HuggingFace and substitute your account's API token above.</li> </ul> </p> <p>The code and docs for the <code>WebAgent</code> that runs llamaspeak can be found in the NanoLLM library.  This block diagram shows the speech pipeline with interleaved model generation, user interruption, and streaming I/O:</p> <p></p>"},{"location":"tutorial_llamaspeak.html#multimodality","title":"Multimodality","text":"<p>If you load a multimodal vision/language model instead, you can drag images into the chat and ask questions about them:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.agents.web_chat --api=mlc \\\n    --model Efficient-Large-Model/VILA-7b \\\n    --asr=riva --tts=piper\n</code></pre> <p>For more info about the supported vision/language models, see the NanoVLM page.</p>"},{"location":"tutorial_llamaspeak.html#function-calling","title":"Function Calling","text":"<p>There's the ability to define functions from Python that the bot has access to and can invoke based on the chat flow:</p> <p>This works by using the <code>bot_function()</code> decorator and adding the API description's to the system prompt:</p> <pre><code>from nano_llm import NanoLLM, ChatHistory, BotFunctions, bot_function\nfrom datetime import datetime\n\n@bot_function\ndef DATE():\n    \"\"\" Returns the current date. \"\"\"\n    return datetime.now().strftime(\"%A, %B %-m %Y\")\n\n@bot_function\ndef TIME():\n    \"\"\" Returns the current time. \"\"\"\n    return datetime.now().strftime(\"%-I:%M %p\")\n\nsystem_prompt = \"You are a helpful and friendly AI assistant.\" + BotFunctions.generate_docs()\n</code></pre> <p>The system prompt can be autogenerated from the Python docstrings embedded in the functions themselves, and can include parameters that the bot can supply (for example, selectively saving relevant user info to a vector database for RAG like is shown in the video).  </p> <p>For more information about this topic, see the Function Calling section of the NanoLLM documentation.  </p>"},{"location":"tutorial_llava.html","title":"Tutorial - LLaVA","text":"<p>LLaVA is a popular multimodal vision/language model that you can run locally on Jetson to answer questions about image prompts and queries.  Llava uses the CLIP vision encoder to transform images into the same embedding space as its LLM (which is the same as Llama architecture).  Below we cover different methods to run Llava on Jetson, with increasingly optimized performance:</p> <ol> <li>Chat with Llava using <code>text-generation-webui</code></li> <li>Run from the terminal with <code>llava.serve.cli</code></li> <li>Quantized GGUF models with <code>llama.cpp</code></li> <li>Optimized Multimodal Pipeline with <code>NanoVLM</code></li> </ol> Llava-13B (Jetson AGX Orin) Quantization Tokens/sec Memory <code>text-generation-webui</code> 4-bit (GPTQ) 2.3 9.7 GB <code>llava.serve.cli</code> FP16 (None) 4.2 27.7 GB <code>llama.cpp</code> 4-bit (Q4_K) 10.1 9.2 GB <code>NanoVLM</code> 4-bit (MLC) 21.1 8.7 GB <p>In addition to Llava, the <code>NanoVLM</code> pipeline supports VILA and mini vision models that run on Orin Nano as well.</p> <p></p>"},{"location":"tutorial_llava.html#1-chat-with-llava-using-text-generation-webui","title":"1. Chat with Llava using <code>text-generation-webui</code>","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.2GB</code> for <code>text-generation-webui</code> container image</li> <li>Space for models<ul> <li>CLIP model : <code>1.7GB</code></li> <li>Llava-v1.5-13B-GPTQ model : <code>7.25GB</code></li> </ul> </li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_llava.html#download-model","title":"Download Model","text":"<pre><code>jetson-containers run --workdir=/opt/text-generation-webui $(autotag text-generation-webui) \\\n  python3 download-model.py --output=/data/models/text-generation-webui \\\n    TheBloke/llava-v1.5-13B-GPTQ\n</code></pre>"},{"location":"tutorial_llava.html#start-web-ui-with-multimodal-extension","title":"Start Web UI with Multimodal Extension","text":"<pre><code>jetson-containers run --workdir=/opt/text-generation-webui $(autotag text-generation-webui) \\\n  python3 server.py --listen \\\n    --model-dir /data/models/text-generation-webui \\\n    --model TheBloke_llava-v1.5-13B-GPTQ \\\n    --multimodal-pipeline llava-v1.5-13b \\\n    --loader autogptq \\\n    --disable_exllama \\\n    --verbose\n</code></pre> <p>Go to Chat tab, drag and drop an image into the Drop Image Here area, and your question in the text area and hit Generate:</p> <p></p>"},{"location":"tutorial_llava.html#result","title":"Result","text":""},{"location":"tutorial_llava.html#2-run-from-the-terminal-with-llavaservecli","title":"2. Run from the terminal with <code>llava.serve.cli</code>","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.1GB</code> for <code>llava</code> container</li> <li><code>14GB</code> for Llava-7B (or <code>26GB</code> for Llava-13B)</li> </ul> </li> </ol> <p>This example uses the upstream Llava repo to run the original, unquantized Llava models from the command-line.  It uses more memory due to using FP16 precision, and is provided mostly as a reference for debugging.  See the Llava container readme for more info.</p>"},{"location":"tutorial_llava.html#llava-v15-7b","title":"llava-v1.5-7b","text":"<pre><code>jetson-containers run $(autotag llava) \\\n  python3 -m llava.serve.cli \\\n    --model-path liuhaotian/llava-v1.5-7b \\\n    --image-file /data/images/hoover.jpg\n</code></pre>"},{"location":"tutorial_llava.html#llava-v15-13b","title":"llava-v1.5-13b","text":"<pre><code>jetson-containers run $(autotag llava) \\\n  python3 -m llava.serve.cli \\\n    --model-path liuhaotian/llava-v1.5-13b \\\n    --image-file /data/images/hoover.jpg\n</code></pre> <p>Unquantized 13B may run only on Jetson AGX Orin 64GB due to memory requirements.</p>"},{"location":"tutorial_llava.html#3-quantized-gguf-models-with-llamacpp","title":"3. Quantized GGUF models with <code>llama.cpp</code>","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> </ol> <p>llama.cpp is one of the faster LLM API's, and can apply a variety of quantization methods to Llava to reduce its memory usage and runtime.  Despite its name, it uses CUDA.  There are pre-quantized versions of Llava-1.5 available in GGUF format for 4-bit and 5-bit:</p> <ul> <li>mys/ggml_llava-v1.5-7b</li> <li>mys/ggml_llava-v1.5-13b</li> </ul> <pre><code>jetson-containers run --workdir=/opt/llama.cpp/bin $(autotag llama_cpp:gguf) \\\n  /bin/bash -c './llava-cli \\\n    --model $(huggingface-downloader mys/ggml_llava-v1.5-13b/ggml-model-q4_k.gguf) \\\n    --mmproj $(huggingface-downloader mys/ggml_llava-v1.5-13b/mmproj-model-f16.gguf) \\\n    --n-gpu-layers 999 \\\n    --image /data/images/hoover.jpg \\\n    --prompt \"What does the sign say\"'\n</code></pre> Quantization Bits Response Tokens/sec Memory <code>Q4_K</code> 4 The sign says \"Hoover Dam, Exit 9.\" 10.17 9.2 GB <code>Q5_K</code> 5 The sign says \"Hoover Dam exit 9.\" 9.73 10.4 GB <p>A lower temperature like 0.1 is recommended for better quality (<code>--temp 0.1</code>), and if you omit <code>--prompt</code> it will describe the image:</p> <pre><code>jetson-containers run --workdir=/opt/llama.cpp/bin $(autotag llama_cpp:gguf) \\\n  /bin/bash -c './llava-cli \\\n    --model $(huggingface-downloader mys/ggml_llava-v1.5-13b/ggml-model-q4_k.gguf) \\\n    --mmproj $(huggingface-downloader mys/ggml_llava-v1.5-13b/mmproj-model-f16.gguf) \\\n    --n-gpu-layers 999 \\\n    --image /data/images/lake.jpg'\n\nIn this image, a small wooden pier extends out into a calm lake, surrounded by tall trees and mountains. The pier seems to be the only access point to the lake. The serene scene includes a few boats scattered across the water, with one near the pier and the others further away. The overall atmosphere suggests a peaceful and tranquil setting, perfect for relaxation and enjoying nature.\n</code></pre> <p>You can put your own images in the mounted <code>jetson-containers/data</code> directory.  The C++ code for llava-cli can be found here.  The llama-cpp-python bindings also support Llava, however they are slower from Python (potentially handling of the tokens) </p>"},{"location":"tutorial_llava.html#4-optimized-multimodal-pipeline-with-nanovlm","title":"4. Optimized Multimodal Pipeline with <code>NanoVLM</code>","text":"<p>What's Next</p> <p>This section got too long and was moved to the NanoVLM page - check it out there for performance optimizations, mini VLMs, and live streaming!</p> <p></p> <p></p>"},{"location":"tutorial_minigpt4.html","title":"Tutorial - MiniGPT-4","text":"<p>Give your locally running LLM an access to vision, by running MiniGPT-4 on Jetson!</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>5.8GB</code> for container image</li> <li>Space for pre-quantized MiniGPT-4 model</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_minigpt4.html#start-minigpt4-container-with-models","title":"Start <code>minigpt4</code> container with models","text":"<p>To start the MiniGPT4 container and webserver with the recommended models, run this command:</p> <pre><code>jetson-containers run $(autotag minigpt4) /bin/bash -c 'cd /opt/minigpt4.cpp/minigpt4 &amp;&amp; python3 webui.py \\\n  $(huggingface-downloader --type=dataset maknee/minigpt4-13b-ggml/minigpt4-13B-f16.bin) \\\n  $(huggingface-downloader --type=dataset maknee/ggml-vicuna-v0-quantized/ggml-vicuna-13B-v0-q5_k.bin)'\n</code></pre> <p>Then, open your web browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code>.</p>"},{"location":"tutorial_minigpt4.html#results","title":"Results","text":""},{"location":"tutorial_mmj.html","title":"Tutorial mmj","text":"<p>17# First steps with Metropolis Microservices for Jetson</p> <p>NVIDIA Metropolis Microservices for Jetson simplifies the development of vision AI applications, offering a suite of customizable, cloud-native tools. Before diving into this tutorial, ensure you've filled out the Metropolis Microservices for Jetson Early Access form to gain the necessary access to launch the services. This step is crucial as it enables you to utilize all the features and capabilities discussed in this guide.</p> <p>Perfect for both newcomers and experts, this tutorial provides straightforward steps to kick-start your edge AI projects. Whether you're a student or an ecosystem partner working on a use case, this guide offers a straightforward start for every skill level.</p> <p></p>"},{"location":"tutorial_mmj.html#0-install-nvidia-jetson-services","title":"0. Install NVIDIA Jetson Services:","text":"<p>Ok, let's start by installing NVIDIA Jetson Services: <pre><code>sudo apt install nvidia-jetson-services\n</code></pre></p> <p>Let's add some performance hacks that will be needed to run the demo faster and without streaming artifacts:</p> <ul> <li> <p>If you don't have the Orin at max performance, you can use these two commands, a reboot is needed after: <pre><code>sudo nvpmodel -m 0 \nsudo jetson_clocks\n</code></pre></p> </li> <li> <p>After these two commands, a reboot is needed if your Jetson wasn't already in high-performance mode. These are optional, but they fine-tune your network buffers to ensure smoother streaming by optimizing how much data can be sent and received: <pre><code>sudo sysctl -w net.core.rmem_default=2129920\nsudo sysctl -w net.core.rmem_max=10000000\nsudo sysctl -w net.core.wmem_max=2000000\n</code></pre></p> </li> </ul>"},{"location":"tutorial_mmj.html#1-download-nvidia-cli-for-jetson","title":"1. Download NVIDIA CLI for Jetson","text":"<p>Download NGC for ARM64 from the NGC for CLI site:  <pre><code>unzip ngccli_arm64.zip\nchmod u+x ngc-cli/ngc\necho \"export PATH=\\\"\\$PATH:$(pwd)/ngc-cli\\\"\" &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile\nngc config set\n</code></pre> Here it will ask for your API Key, and the organization name, to get those you need to login into NGC and generate an API key here.</p> <p>You should then paste the API key and use the organization name you are using. You can also press [Enter] to select the default values for the remaining options. After this, you should get the message:</p> <pre><code>Successfully saved NGC configuration to /home/jetson/.ngc/config\n</code></pre> <p>Then, login with the same API key: <pre><code>sudo docker login nvcr.io -u \"\\$oauthtoken\" -p &lt;NGC-API-KEY&gt;\n</code></pre></p> <p>Now launch the Redis and Ingress services, as we need them for this tutorial. </p> <pre><code>sudo systemctl start jetson-redis\nsudo systemctl start jetson-ingress\n</code></pre>"},{"location":"tutorial_mmj.html#2-download-and-launch-nvstreamer","title":"2. Download and launch NVStreamer","text":""},{"location":"tutorial_mmj.html#_1","title":"Tutorial mmj","text":"<p>First, we need to install NVStreamer, an app that streams the videos MMJs will need to run AI on them. Follow this NVStreamer Link (In the top-left, click Download files.zip)</p> <p><pre><code>unzip files.zip\nrm files.zip\ntar -xvf nvstreamer.tar.gz\ncd nvstreamer\n</code></pre> Launch it: <pre><code>sudo docker compose -f compose_nvstreamer.yaml up -d  --force-recreate\n</code></pre></p>"},{"location":"tutorial_mmj.html#3-download-ai_nvr-and-launch","title":"3. Download AI_NVR and launch:","text":""},{"location":"tutorial_mmj.html#_2","title":"Tutorial mmj","text":"<p>AI NVR (NGC) Link (Top-left -&gt; Download files.zip)</p> <pre><code>unzip files.zip\nrm files.zip\ntar -xvf ai_nvr.tar.gz\nsudo cp ai_nvr/config/ai-nvr-nginx.conf /opt/nvidia/jetson/services/ingress/config/\ncd ai_nvr\nsudo docker compose -f compose_agx.yaml up -d --force-recreate\n</code></pre>"},{"location":"tutorial_mmj.html#4-download-some-sample-videos-and-upload-them-to-nvstreamer","title":"4. Download some sample videos and upload them to NVStreamer","text":"<p>Download them from here.</p> <p><pre><code>unzip files.zip\n</code></pre> Ok, now, this is important to understand, there are 2 web interfaces:</p> <ol> <li>The NVStream Streamer Dashboard, running in: http://localhost:31000</li> <li>The NVStreamer Camera Management Dashboard, running in: http://localhost:31000</li> </ol> <p>So, first we need to upload the file in the Streamer interface, it looks like this:</p> <p></p> <p>There, go to File Upload, and drag and drop the file in the upload squared area.</p> <p>After uploading it, go to the Dashboad option of the left menu, and copy the RTSP URL of the video you just uploaded, you will need it for the Camera Management Dashboard.</p> <p>Now jump to the Camera Management Dashboard (http://localhost:30080/vst), it looks like this:</p> <p></p> <p>Go to the Camera Management option of the menu, then use the Add device manually option, and paste the RTSP URL, add the name of your video to the Name and Location text boxes, so it will be displayed on top of the stream.</p> <p>Finally, click in the Live Streams option of the left menu, and you should be able to watch your video stream.</p> <p></p>"},{"location":"tutorial_mmj.html#5-watch-rtsp-ai-processed-streaming-from-vlc","title":"5. Watch RTSP AI processed streaming from VLC","text":"<p>Open VLC from another computer (localhost doesn't work here), and point to your Jetson Orin's IP address (you should be in the same network, or not having a firewal to access).</p> <p>The easiest way to get Jetson's ip is launching: <pre><code>ifconfig\n</code></pre> And checking the IP of the interface (usually wlan0, inet IP).</p> <p>Then go to rtsp://[JETSON_IP]:8555/ds-test using VLC like this:</p> <p></p>"},{"location":"tutorial_mmj.html#6-android-app","title":"6. Android app","text":"<p>There is an Android app that allows you to track events and create areas of interest to monitor, you can find it on Google Play as AI NVR.</p> <p></p> <p>Here is a quick walkthough where you can see how to:</p> <ul> <li>Add the IP address of the Jetson</li> <li>Track current events</li> <li>Add new areas of interest</li> <li>Add tripwire to track the flux and direction of events</li> </ul> <p></p>"},{"location":"tutorial_nano-llm.html","title":"NanoLLM - Optimized LLM Inference","text":"<p><code>NanoLLM</code> is a lightweight, high-performance library using optimized inferencing APIs for quantized LLM\u2019s, multimodality, speech services, vector databases with RAG, and web frontends like Agent Studio.</p> <p></p> <p>It provides similar APIs to HuggingFace, backed by highly-optimized inference libraries and quantization tools:</p> NanoLLM Reference Documentation<pre><code>from nano_llm import NanoLLM\n\nmodel = NanoLLM.from_pretrained(\n   \"meta-llama/Meta-Llama-3-8B-Instruct\",  # HuggingFace repo/model name, or path to HF model checkpoint\n   api='mlc',                              # supported APIs are: mlc, awq, hf\n   api_token='hf_abc123def',               # HuggingFace API key for authenticated models ($HUGGINGFACE_TOKEN)\n   quantization='q4f16_ft'                 # q4f16_ft, q4f16_1, q8f16_0 for MLC, or path to AWQ weights\n)\n\nresponse = model.generate(\"Once upon a time,\", max_new_tokens=128)\n\nfor token in response:\n   print(token, end='', flush=True)\n</code></pre>"},{"location":"tutorial_nano-llm.html#containers","title":"Containers","text":"<p>To test a chat session with Llama from the command-line, install <code>jetson-containers</code> and run NanoLLM like this:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> Llama CLIAgent Studio <pre><code>jetson-containers run \\\n  --env HUGGINGFACE_TOKEN=hf_abc123def \\\n  $(autotag nano_llm) \\\n    python3 -m nano_llm.chat --api mlc \\\n      --model meta-llama/Meta-Llama-3-8B-Instruct \\\n      --prompt \"Can you tell me a joke about llamas?\"\n</code></pre> <pre><code>jetson-containers run \\\n  --env HUGGINGFACE_TOKEN=hf_abc123def \\\n  $(autotag nano_llm) \\\n    python3 -m nano_llm.studio\n</code></pre> <p>If you haven't already, request access to the Llama models on HuggingFace and substitute your account's API token above.</p>"},{"location":"tutorial_nano-llm.html#resources","title":"Resources","text":"<p>Here's an index of the various tutorials &amp; examples using NanoLLM on Jetson AI Lab:</p> Benchmarks Benchmarking results for LLM, SLM, VLM using MLC/TVM backend. API Examples Python code examples for chat, completion, and multimodal. Documentation Reference documentation for the NanoLLM model and agent APIs. Llamaspeak Talk verbally with LLMs using low-latency ASR/TTS speech models. Small LLM (SLM) Focus on language models with reduced footprint (7B params and below) Live LLaVA Realtime live-streaming vision/language models on recurring prompts. Nano VLM Efficient multimodal pipeline with one-shot image tagging and RAG support. Agent Studio Rapidly design and experiment with creating your own automation agents. OpenVLA Robot learning with Vision/Language Action models and manipulation in simulator."},{"location":"tutorial_nano-vlm.html","title":"NanoVLM - Efficient Multimodal Pipeline","text":"<p>We saw in the previous LLaVA tutorial how to run vision-language models through tools like <code>text-generation-webui</code> and <code>llama.cpp</code>.  In a similar vein to the SLM page on Small Language Models, here we'll explore optimizing VLMs for reduced memory usage and higher performance that reaches interactive levels (like in Liva LLava).  These are great for fitting on Orin Nano and increasing the framerate.</p> <p>There are 3 model families currently supported:  Llava, VILA, and Obsidian (mini VLM)</p>"},{"location":"tutorial_nano-vlm.html#vlm-benchmarks","title":"VLM Benchmarks","text":"<p>This FPS measures the end-to-end pipeline performance for continuous streaming like with Live Llava (on yes/no question)  </p>"},{"location":"tutorial_nano-vlm.html#multimodal-chat","title":"Multimodal Chat","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)\u26a0\ufe0f</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>22GB</code> for <code>nano_llm</code> container image</li> <li>Space for models (<code>&gt;10GB</code>)</li> </ul> </li> <li> <p>Supported VLM models in <code>NanoLLM</code>:</p> <ul> <li><code>liuhaotian/llava-v1.5-7b</code>, <code>liuhaotian/llava-v1.5-13b</code>, <code>liuhaotian/llava-v1.6-vicuna-7b</code>, <code>liuhaotian/llava-v1.6-vicuna-13b</code></li> <li><code>Efficient-Large-Model/VILA-2.7b</code>,<code>Efficient-Large-Model/VILA-7b</code>, <code>Efficient-Large-Model/VILA-13b</code></li> <li><code>Efficient-Large-Model/VILA1.5-3b</code>,<code>Efficient-Large-Model/Llama-3-VILA1.5-8B</code>, <code>Efficient-Large-Model/VILA1.5-13b</code></li> <li><code>VILA-2.7b</code>, <code>VILA1.5-3b</code>, <code>VILA-7b</code>, <code>Llava-7b</code>, and <code>Obsidian-3B</code> can run on Orin Nano 8GB</li> </ul> </li> </ol> <p>The optimized <code>NanoLLM</code> library uses MLC/TVM for quantization and inference provides the highest performance.  It efficiently manages the CLIP embeddings and KV cache.  You can find Python code for the chat program used in this example here. </p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.chat --api=mlc \\\n    --model Efficient-Large-Model/VILA1.5-3b \\\n    --max-context-len 256 \\\n    --max-new-tokens 32\n</code></pre> <p>This starts an interactive console-based chat with Llava, and on the first run the model will automatically be downloaded from HuggingFace and quantized using MLC and W4A16 precision (which can take some time).  See here for command-line options.</p> <p>You'll end up at a <code>&gt;&gt; PROMPT:</code> in which you can enter the path or URL of an image file, followed by your question about the image.  You can follow-up with multiple questions about the same image.  Llava does not understand multiple images in the same chat, so when changing images, first reset the chat history by entering <code>clear</code> or <code>reset</code> as the prompt.  VILA supports multiple images (area of active research)</p>"},{"location":"tutorial_nano-vlm.html#automated-prompts","title":"Automated Prompts","text":"<p>During testing, you can specify prompts on the command-line that will run sequentially:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.chat --api=mlc \\\n    --model Efficient-Large-Model/VILA1.5-3b \\\n    --max-context-len 256 \\\n    --max-new-tokens 32 \\\n    --prompt '/data/images/hoover.jpg' \\\n    --prompt 'what does the road sign say?' \\\n    --prompt 'what kind of environment is it?' \\\n    --prompt 'reset' \\\n    --prompt '/data/images/lake.jpg' \\\n    --prompt 'please describe the scene.' \\\n    --prompt 'are there any hazards to be aware of?'\n</code></pre> <p>You can also use <code>--prompt /data/prompts/images.json</code> to run the test sequence, the results of which are in the table below.</p>"},{"location":"tutorial_nano-vlm.html#results","title":"Results","text":"<p>\u2022 \u00a0 The model responses are with 4-bit quantization enabled, and are truncated to 128 tokens for brevity. \u2022 \u00a0 These chat questions and images are from <code>/data/prompts/images.json</code> (found in jetson-containers) </p>"},{"location":"tutorial_nano-vlm.html#json","title":"JSON","text":"<p>When prompted, these models can also output in constrained JSON formats (which the LLaVA authors cover in their LLaVA-1.5 paper), and can be used to programatically query information about the image:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.chat --api=mlc \\\n    --model liuhaotian/llava-v1.5-13b \\\n    --prompt '/data/images/hoover.jpg' \\\n    --prompt 'extract any text from the image as json'\n\n{\n  \"sign\": \"Hoover Dam\",\n  \"exit\": \"2\",\n  \"distance\": \"1 1/2 mile\"\n}\n</code></pre>"},{"location":"tutorial_nano-vlm.html#web-ui","title":"Web UI","text":"<p>To use this through a web browser instead, see the llamaspeak tutorial: </p> <p></p>"},{"location":"tutorial_nano-vlm.html#live-streaming","title":"Live Streaming","text":"<p>These models can also be used with the Live Llava agent for continuous streaming - just substitute the desired model name below:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.agents.video_query --api=mlc \\\n    --model Efficient-Large-Model/VILA1.5-3b \\\n    --max-context-len 256 \\\n    --max-new-tokens 32 \\\n    --video-input /dev/video0 \\\n    --video-output webrtc://@:8554/output\n</code></pre> <p>Then navigate your browser to <code>https://&lt;IP_ADDRESS&gt;:8050</code> after launching it with your camera.  Using Chrome or Chromium is recommended for a stable WebRTC connection, with <code>chrome://flags#enable-webrtc-hide-local-ips-with-mdns</code> disabled.</p> <p>The Live Llava tutorial shows how to enable additional features like vector database integration, image tagging, and RAG.</p>"},{"location":"tutorial_nano-vlm.html#video-sequences","title":"Video Sequences","text":"<p>The VILA-1.5 family of models can understand multiple images per query, enabling video search/summarization, action &amp; behavior analysis, change detection, and other temporal-based vision functions.  By manipulating the KV cache and dropping off the last frame from the chat history, we can keep the stream rolling continuously beyond the maximum context length of the model.  The <code>vision/video.py</code> example shows how to use this:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.vision.video \\\n    --model Efficient-Large-Model/VILA1.5-3b \\\n    --max-images 8 \\\n    --max-new-tokens 48 \\\n    --video-input /data/my_video.mp4 \\\n    --video-output /data/my_output.mp4 \\\n    --prompt 'What changes occurred in the video?'\n</code></pre>"},{"location":"tutorial_nano-vlm.html#python-code","title":"Python Code","text":"<p>For a simplified code example of doing live VLM streaming from Python, see here in the NanoLLM docs. </p> <p>You can use this to implement customized prompting techniques and integrate with other vision pipelines.  This code applies the same set of prompts to the latest image from the video feed.  See here for the version that does multi-image queries on video sequences.</p>"},{"location":"tutorial_nanodb.html","title":"Tutorial - NanoDB","text":"<p>Let's run NanoDB's interactive demo to witness the impact of Vector Database that handles multimodal data.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>16GB</code> for container image</li> <li><code>40GB</code> for MS COCO dataset</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_nanodb.html#how-to-start","title":"How to start","text":""},{"location":"tutorial_nanodb.html#download-coco","title":"Download COCO","text":"<p>Just for an example, let's use MS COCO dataset:</p> <pre><code>cd jetson-containers\nmkdir -p data/datasets/coco/2017\ncd data/datasets/coco/2017\n\nwget http://images.cocodataset.org/zips/train2017.zip\nwget http://images.cocodataset.org/zips/val2017.zip\nwget http://images.cocodataset.org/zips/unlabeled2017.zip\n\nunzip train2017.zip\nunzip val2017.zip\nunzip unlabeled2017.zip\n</code></pre>"},{"location":"tutorial_nanodb.html#download-index","title":"Download Index","text":"<p>You can download a pre-indexed NanoDB that was already prepared over the COCO dataset from here:</p> <pre><code>cd jetson-containers/data\nwget https://nvidia.box.com/shared/static/icw8qhgioyj4qsk832r4nj2p9olsxoci.gz -O nanodb_coco_2017.tar.gz\ntar -xzvf nanodb_coco_2017.tar.gz\n</code></pre> <p>This allow you to skip the indexing process in the next step, and jump to starting the Web UI.</p>"},{"location":"tutorial_nanodb.html#indexing-data","title":"Indexing Data","text":"<p>If you didn't download the NanoDB index for COCO from above, we need to build the index by scanning your dataset directory:</p> <pre><code>jetson-containers run $(autotag nanodb) \\\n  python3 -m nanodb \\\n    --scan /data/datasets/coco/2017 \\\n    --path /data/nanodb/coco/2017 \\\n    --autosave --validate \n</code></pre> <p>This will take a few hours on AGX Orin.  Once the database has loaded and completed any start-up operations , it will drop down to a <code>&gt;</code> prompt from which the user can run search queries. You can quickly check the operation by typing your query on this prompt:</p> <pre><code>&gt; a girl riding a horse\n\n* index=80110   /data/datasets/coco/2017/train2017/000000393735.jpg      similarity=0.29991915822029114\n* index=158747  /data/datasets/coco/2017/unlabeled2017/000000189708.jpg  similarity=0.29254037141799927\n* index=123846  /data/datasets/coco/2017/unlabeled2017/000000026239.jpg  similarity=0.292171448469162\n* index=127338  /data/datasets/coco/2017/unlabeled2017/000000042508.jpg  similarity=0.29118549823760986\n* index=77416   /data/datasets/coco/2017/train2017/000000380634.jpg      similarity=0.28964102268218994\n* index=51992   /data/datasets/coco/2017/train2017/000000256290.jpg      similarity=0.28929752111434937\n* index=228640  /data/datasets/coco/2017/unlabeled2017/000000520381.jpg  similarity=0.28642547130584717\n* index=104819  /data/datasets/coco/2017/train2017/000000515895.jpg      similarity=0.285491943359375\n</code></pre> <p>You can press Ctrl+C to exit. For more info about the various options available, see the NanoDB container documentation.</p>"},{"location":"tutorial_nanodb.html#interactive-web-ui","title":"Interactive Web UI","text":"<p>Spin up the Gradio server:</p> <pre><code>jetson-containers run $(autotag nanodb) \\\n  python3 -m nanodb \\\n    --path /data/nanodb/coco/2017 \\\n    --server --port=7860\n</code></pre> <p>Then navigate your browser to <code>http://&lt;IP_ADDRESS&gt;:7860</code>, and you can enter text search queries as well as drag/upload images:</p> <p>To use the dark theme, navigate to <code>http://&lt;IP_ADDRESS&gt;:7860/?__theme=dark</code> instead"},{"location":"tutorial_nanoowl.html","title":"Tutorial - NanoOWL","text":"<p>Let's run NanoOWL, OWL-ViT optimized to run real-time on Jetson with NVIDIA TensorRT.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin 64GB Jetson AGX Orin (32GB) Jetson Orin Nano Orin (8GB)</p> </li> <li> <p>Running one of the following JetPack.5x</p> <p>JetPack 5.1.2 (L4T r35.4.1) JetPack 5.1.1 (L4T r35.3.1) JetPack 5.1 (L4T r35.2.1)</p> </li> <li> <p>Sufficient storage space (preferably with NVMe SSD).</p> <ul> <li><code>7.2 GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> </ol>"},{"location":"tutorial_nanoowl.html#clone-and-set-up-jetson-containers","title":"Clone and set up <code>jetson-containers</code>","text":"<pre><code>git clone https://github.com/dusty-nv/jetson-containers\ncd jetson-containers\nsudo apt update; sudo apt install -y python3-pip\npip3 install -r requirements.txt\n</code></pre>"},{"location":"tutorial_nanoowl.html#how-to-start","title":"How to start","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>cd jetson-containers\n./run.sh $(./autotag nanoowl)\n</code></pre>"},{"location":"tutorial_nanoowl.html#how-to-run-the-tree-prediction-live-camera-example","title":"How to run the tree prediction (live camera) example","text":"<ol> <li> <p>Ensure you have a camera device connected</p> <pre><code>ls /dev/video*\n</code></pre> <p>If no video device is found, exit from the container and check if you can see a video device on the host side.</p> </li> <li> <p>Launch the demo     <pre><code>cd examples/tree_demo\npython3 tree_demo.py --camera 0 --resolution 640x480 ../../data/owl_image_encoder_patch32.engine\n</code></pre></p> Option Description Example <code>--camera</code> To specify camera index (corresponds to <code>/dev/video*</code>) when multiple cameras are connected <code>1</code> <code>--resolution</code> To specify the camera open resolution in the format <code>{width}x{height}</code> <code>640x480</code> <p>Info</p> <p>If it fails to find or load the TensorRT engine file, build the TensorRT engine for the OWL-ViT vision encoder on your Jetson device.</p> <pre><code>python3 -m nanoowl.build_image_encoder_engine \\\n    data/owl_image_encoder_patch32.engine\n</code></pre> </li> <li> <p>Second, open your browser to <code>http://&lt;ip address&gt;:7860</code></p> </li> <li> <p>Type whatever prompt you like to see what works!</p> <p>Here are some examples</p> <ul> <li>Example: <code>[a face [a nose, an eye, a mouth]]</code></li> <li>Example: <code>[a face (interested, yawning / bored)]</code></li> <li>Example: <code>(indoors, outdoors)</code></li> </ul> </li> </ol>"},{"location":"tutorial_nanoowl.html#result","title":"Result","text":""},{"location":"tutorial_ollama.html","title":"Tutorial - Ollama","text":"<p>Ollama is a popular open-source tool that allows users to easily run a large language models (LLMs) locally on their own computer, serving as an accessible entry point to LLMs for many.</p> <p>It now offers out-of-the-box support for the Jetson platform with CUDA support, enabling Jetson users to seamlessly install Ollama with a single command and start using it immediately.</p> <p>In this tutorial, we introduce two installation methods: (1) the default native installation using the official Ollama installer, and (2) the Docker container method, which allows users to avoid making changes to their existing system.</p> <p></p> <ul> <li>The <code>ollama</code> client can run inside or outside container after starting the server.</li> <li>You can also run an Open WebUI server for supporting web clients.</li> <li>Supports the latest models like [gpt-oss](https://ollama.com/library/gpt-oss{:target=\"_blank\"}!</li> </ul>"},{"location":"tutorial_ollama.html#ollama-server","title":"Ollama Server","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Thor Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>7GB</code> for <code>ollama</code> container image</li> <li>Space for models (<code>&gt;5GB</code>)</li> </ul> </li> </ol>"},{"location":"tutorial_ollama.html#1-native-install","title":"(1) Native Install","text":"<p>Note</p> <p>Ollama native installer does not support Jetson AGX Thor Developer Kit yet.</p> <p>If you want to run Ollama on Jetson AGX Thor Developer Kit, checkout the following Ollama container approach.</p> <p>Ollama's official installer already support Jetson and can easily install CUDA-supporting Ollama.</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p></p> <p>It create a service to run <code>ollama serve</code> on start up, so you can start using <code>ollama</code> command right away.</p>"},{"location":"tutorial_ollama.html#example-ollama-usage","title":"Example: Ollama usage","text":"<pre><code>ollama\n</code></pre>"},{"location":"tutorial_ollama.html#example-run-a-model-on-cli","title":"Example: run a model on CLI","text":"<pre><code>ollama run llama3.2:3b\n</code></pre>"},{"location":"tutorial_ollama.html#2-docker-container-for-ollama","title":"(2) Docker container for <code>ollama</code>","text":"JetPack 7 (Jetson Thor) JetPack 6 (Jetson Orin) <p>You can use this Docker container built to run Ollama on Jetson Thor.</p> <pre><code>mkdir ~/ollama-data/\ndocker run --rm -it -v ${HOME}/ollama-data:/data ghcr.io/nvidia-ai-iot/ollama:r38.2.arm64-sbsa-cu130-24.04\n</code></pre> <p>It will take some time to pull (download) the container image.</p> <p>Once in the container, you will see something like this.</p> <pre><code>Starting ollama server\n\n\nOLLAMA_HOST   0.0.0.0\nOLLAMA_LOGS   /data/logs/ollama.log\nOLLAMA_MODELS /data/models/ollama/models\n\n\nollama server is now started, and you can run commands here like 'ollama run gemma3'\n\nroot@2a79cc8835d9:/#\n</code></pre> <p>Try running a GPT OSS (20b parameter) model by issuing a command below.</p> <pre><code>ollama run --verbose gpt-oss:20b\n</code></pre> <p>It will download 14GB weight, so it takes some time here as well.</p> <p>Once ready, it will show something like this:</p> <pre><code>root@2a79cc8835d9:/# ollama run --verbose gpt-oss:20b\npulling manifest\npulling b112e727c6f1: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  13 GB\npulling fa6710a93d78: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f 7.2 KB\npulling f60356777647: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  11 KB\npulling d8ba2f9a17b3: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f   18 B\npulling 55c108d8e936: 100% \u2595\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  489 B\nverifying sha256 digest\nwriting manifest\nsuccess\n&gt;&gt;&gt; Send a message (/? for help)\n</code></pre> <p>Try any prompt and you will get something like this.</p> <pre><code>root@c11344f6bbbc:/# ollama run --verbose gpt-oss:20b\n&gt;&gt;&gt; why is the sky blue in one sentence\nThinking...\nWe need to answer: \"why is the sky blue in one sentence\". Just one sentence. Provide explanation: Rayleigh scattering of sunlight,\nshorter wavelengths scatter more. We'll produce a single sentence. Let's give a concise explanation.\n...done thinking.\n\nThe sky looks blue because the Earth's atmosphere scatters shorter-wavelength (blue) light from the sun more efficiently than longer\nwavelengths, a phenomenon called Rayleigh scattering.\n\ntotal duration:       3.504445244s\nload duration:        225.399151ms\nprompt eval count:    76 token(s)\nprompt eval duration: 673.487645ms\nprompt eval rate:     112.85 tokens/s\neval count:           88 token(s)\neval duration:        2.603822053s\neval rate:            33.80 tokens/s\n&gt;&gt;&gt; Send a message (/? for help)\n</code></pre> <p>You can finish the session by typing <code>/bye</code>.</p> <p>We can use <code>jetson-containers</code> to run Ollama.</p> <pre><code># models cached under jetson-containers/data\njetson-containers run --name ollama $(autotag ollama)\n\n# models cached under your user's home directory\ndocker run --runtime nvidia --rm --network=host -v ~/ollama:/ollama -e OLLAMA_MODELS=/ollama dustynv/ollama:r36.2.0\n</code></pre> <p>Running either of these will start the local Ollama server as a daemon in the background.  It will save the models it downloads under your mounted <code>jetson-containers/data/models/ollama</code> directory (or another directory that you override with <code>OLLAMA_MODELS</code>)</p> <p>Start the Ollama command-line chat client with your desired model (for example: <code>llama3</code>, <code>phi3</code>, <code>mistral</code>)</p> <pre><code># if running inside the same container as launched above\n/bin/ollama run phi3\n\n# if launching a new container for the client in another terminal\njetson-containers run $(autotag ollama) /bin/ollama run phi3\n</code></pre> <p>Or you can install Ollama's binaries for arm64 outside of container (without CUDA, which only the server needs)</p> <pre><code># download the latest ollama release for arm64 into /bin\nsudo wget https://github.com/ollama/ollama/releases/download/$(git ls-remote --refs --sort=\"version:refname\" --tags https://github.com/ollama/ollama | cut -d/ -f3- | sed 's/-rc.*//g' | tail -n1)/ollama-linux-arm64 -O /bin/ollama\nsudo chmod +x /bin/ollama\n\n# use the client like normal outside container\n/bin/ollama run phi3\n</code></pre>"},{"location":"tutorial_ollama.html#open-webui","title":"Open WebUI","text":"<p>To run an Open WebUI server for client browsers to connect to, use the <code>open-webui</code> container:</p> <pre><code>docker run -it --rm --network=host --add-host=host.docker.internal:host-gateway ghcr.io/open-webui/open-webui:main\n</code></pre> <p>You can then navigate your browser to <code>http://JETSON_IP:8080</code>, and create a fake account to login (these credentials are only local)</p> <p></p> <p>Ollama uses llama.cpp for inference, which various API benchmarks and comparisons are provided for on the Llava page.  It gets roughly half of peak performance versus the faster APIs like NanoLLM, but is generally considered fast enough for text chat.</p>"},{"location":"tutorial_openwebui.html","title":"Tutorial - Open WebUI","text":"<p>Open WebUI is a versatile, browser-based interface for running and managing large language models (LLMs) locally, offering Jetson developers an intuitive platform to experiment with LLMs on their devices.</p> <p>It can work with Ollama as a backend as well as other backend that is compatible with OpenAI, which can also run well on Jetson.</p> <p></p>"},{"location":"tutorial_openwebui.html#ollama-server","title":"Ollama Server","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>7GB</code> for <code>open-webui</code> container image</li> </ul> </li> </ol> <pre><code>sudo docker run -d --network=host \\\n    -v ${HOME}/open-webui:/app/backend/data \\\n    -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \\\n    --name open-webui \\\n    --restart always \\\n    ghcr.io/open-webui/open-webui:main\n</code></pre>"},{"location":"tutorial_openwebui.html#ollama-backend","title":"\ud83e\udd99 Ollama backend","text":"<p>If you have installed Ollama, you can just run the Open WebUI docker container without installing any other things.</p> <pre><code>sudo docker run -d --network=host \\\n    -v ${HOME}/open-webui:/app/backend/data \\\n    -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \\\n    --name open-webui \\\n    --restart always \\\n    ghcr.io/open-webui/open-webui:main\n</code></pre>"},{"location":"tutorial_openwebui.html#case-2-ollama-container","title":"Case 2: Ollama container","text":"<p>If you have not natively installed Ollama, you can also run <code>Ollama</code> in container using <code>jetson-containers</code>, after executing the above command to run Open WebUI container.</p> <pre><code>jetson-containers run --name ollama $(autotag ollama)\n</code></pre> <p>You need to have <code>jetson-containers</code> installed</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre>"},{"location":"tutorial_openwebui.html#case-3-docker-compose-to-launch-both-at-the-same-time","title":"Case 3: Docker Compose to launch both at the same time","text":"<p>You can save the following YML file and issue <code>docker compose up</code>.</p> docker-compose.yml <pre><code>services:\nopen-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    network_mode: \"host\"\n    environment:\n    - OLLAMA_BASE_URL=http://127.0.0.1:11434\n    volumes:\n    - \"${HOME}/open-webui:/app/backend/data\"\n\nollama:\n    image: dustynv/ollama:r36.4.0\n    container_name: ollama\n    runtime: nvidia\n    network_mode: \"host\"\n    shm_size: \"8g\"\n    volumes:\n    - \"/var/run/docker.sock:/var/run/docker.sock\"\n    - \"${HOME}/data:/data\"\n    - \"/etc/localtime:/etc/localtime:ro\"\n    - \"/etc/timezone:/etc/timezone:ro\"\n    - \"/run/jtop.sock:/run/jtop.sock\"\n</code></pre> <p>Once you save the above YAML file in a directory, issue the following.</p> <pre><code>docker compose up\n</code></pre> <p></p>"},{"location":"tutorial_openwebui.html#basic-usage","title":"Basic Usage","text":""},{"location":"tutorial_openwebui.html#step-1-access-through-web-browser","title":"Step 1. Access through Web browser","text":"<p>Open a Web browser on a PC connected to the same network as your Jetson, and access the following address.</p> <pre><code>http://&lt;IP_ADDR&gt;:8080\n</code></pre> <p>You will see a initial web page like this.</p> <p></p> <p>Click \"Get started  \".</p>"},{"location":"tutorial_openwebui.html#step-2-complete-the-account-creation-process","title":"Step 2. Complete the Account Creation Process","text":"<p>Follow the on-screen prompts to create an account.</p> <p></p> <p>Note</p> <p>Note that all account information stays local, so privacy is maintained. You can use a random email address and password for this step as it is not verified or stored externally.</p> <p>However, make sure to remember the credentials, as you\u2019ll need them to log in for future sessions.</p> <p>For more details, refer to the provided information link or instructions on the screen.</p> <p>Once you \"Create Admin Account\", you will be presented with the release notes for the latest version of Open WebUI. To proceed, click \"Okay, Let's Go!\" button.</p> <p></p> <p>Once everything is set up, you should see the following UI appear.</p> <p></p>"},{"location":"tutorial_openwebui.html#step-3-download-an-slm-model","title":"Step 3. Download an SLM model","text":"<p>To download an SLM model, click the dropdown  next to the \"Select a model\" section. Type the name of the model you want to try in \"\ud83d\udd0e Search a model\" field.</p> <p></p> <p>Once selected, you'll be prompted to download the model directly from Ollama.</p> <p></p> <p>After the download, select the newly downloaded model from the list. In our case, it was the LLaMA 3.2:2B model.</p> <p></p> <p>Tip</p> <p>After downloading your SLM model, you have the option to disconnect your Jetson unit from the internet. This allows you to validate that all subsequent interactions are powered exclusively by the local generative AI model, all running on this edge AI computer.</p>"},{"location":"tutorial_openwebui.html#step-4-start-interacting-with-the-model","title":"Step 4. Start interacting with the model","text":"<p>You can now start interacting with the model, just like you would with any other LLM chatbot.</p> <p></p>"},{"location":"tutorial_openwebui.html#usage-rag","title":"Usage - RAG","text":""},{"location":"tutorial_openwebui.html#step-1-create-a-knowledge-base","title":"Step 1. Create a knowledge base","text":"<p>To enable RAG, you need to create a knowledge base that the LLM can reference to answer queries.</p> <p>Follow these steps to create a knowledge base:</p> <ul> <li>Open the Open WebUI interface.</li> <li>Click on Workspace.</li> <li>Select Knowledge from the top menu.</li> <li>Click the \"\u2795\" icon to create a new knowledge base.</li> </ul> <p></p>"},{"location":"tutorial_openwebui.html#step-2-add-files-to-the-knowledge-base","title":"Step 2. Add files to the knowledge base","text":"<p>After providing the necessary details and clicking \"Create Knowledge\", you will be redirected to the following page.</p> <p>Here, you can add files or entire directories to build your knowledge base.</p> <p></p> <p>Select a local PDF file or other document files to upload.</p> <p></p> <p></p> <p></p>"},{"location":"tutorial_openwebui.html#step-3-create-a-custom-model-with-knowledge-base-access","title":"Step 3. Create a custom model with knowledge base access","text":"<p>To leverage the knowledge base for more accurate and context-aware responses, you need to create a model that can access this information. By linking the model to the knowledge base, the LLM can reference the stored data to provide more precise and relevant answers to user queries.</p> <p>Follow these steps to create a model with knowledge base access</p> <ul> <li>Open the Open WebUI interface.</li> <li>Click on Workspace.</li> <li>Select Model from the top menu.</li> <li>Click the \"\u2795\" icon to create a new model.</li> </ul> <p></p>"},{"location":"tutorial_openwebui.html#step-4-configure-the-model","title":"Step 4. Configure the model","text":"<p>After clicking the \"\u2795\" icon, the following screen will appear.</p> <p></p> <p>Here, you can configure the model with the following info:</p> Field What to do Example Model name Enter a name for your new model. \"Jetson Expert\" Select a base model Choose a base model from the list of models you've downloaded on your local Ollama server. \"<code>llama3.2:3b</code>\" Select Knowledge Click the button and select the newly created knowledge base from the list. \"Jetson Documents\" <p></p> <p>Once you enter necessary information, click \"Save &amp; Create\" button.</p> <p></p> <p>You will be taken back to the Models tab in the Workspace.</p> <p></p>"},{"location":"tutorial_openwebui.html#step-5-chat-with-your-custom-model","title":"Step 5. Chat with your custom model","text":"<p>You can now navigate to the chat window and select the newly created model.</p> <p>Use this model to ask questions related to the uploaded documents. This will help verify if the model can successfully retrieve information from the knowledge base.</p> <p></p>"},{"location":"tutorial_openwebui.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"tutorial_openwebui.html#open-web-ui-is-not-responding","title":"Open Web UI is not responding","text":"<p>Reload the page on your web browser.</p>"},{"location":"tutorial_openwebui.html#how-to-create-a-new-account-for-open-webui-if-you-forgot-your-password","title":"How to Create a New Account for Open WebUI (If You Forgot Your Password)","text":"<p>If you need to create a new account in Open WebUI (for example, if you forgot your password), follow these steps to reset the account:</p>"},{"location":"tutorial_openwebui.html#delete-the-existing-open-webui-data-folder","title":"Delete the existing Open WebUI data folder:","text":"<p>This will remove all existing user account data and settings. Run the following command:</p> <pre><code>sudo rm -rf ${HOME}/open-webui\n</code></pre>"},{"location":"tutorial_openwebui.html#re-run-the-docker-container","title":"Re-run the Docker container","text":"<p>This will recreate a fresh instance of Open WebUI, allowing you to create a new account.</p> <pre><code>sudo docker run -d --network=host \\\n    -v ${HOME}/open-webui:/app/backend/data \\\n    -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \\\n    --name open-webui \\\n    --restart always \\\n    ghcr.io/open-webui/open-webui:main\n</code></pre>"},{"location":"tutorial_openwebui.html#how-to-shut-down-open-webui","title":"How to Shut Down Open WebUI?","text":"<p>To gracefully stop and remove the Open WebUI container, run the following commands:</p> <pre><code>sudo docker stop open-webui\nsudo docker rm open-webui\n</code></pre>"},{"location":"tutorial_openwebui.html#optional-setup-mlc-backend","title":"Optional Setup: \ud83e\udd16 MLC backend","text":"<p>Instead of running Open WebUI with Ollama backend, we can run models with more performant backend with MLC that are designed to run optimally on Jetson and can communicate to Open WebUI through OpenAI compatible API.</p> <p></p>"},{"location":"tutorial_openwebui.html#case-1-separate-docker-run-commands","title":"Case 1: Separate <code>docker run</code> commands","text":"<p>After running Open WebUI container with the following command, you can run the <code>docker run</code> command you find in the new Models page.</p> Open WebUI containerMLC container with LLama 3.1 8BMLC container with DeepSeek R1 (Llama 8B) <pre><code>sudo docker run -d --network=host \\\n    -v ${HOME}/open-webui:/app/backend/data \\\n    -e OLLAMA_BASE_URL=http://127.0.0.1:11434 \\\n    --name open-webui \\\n    ghcr.io/open-webui/open-webui:main\n</code></pre> <pre><code>docker run -it --rm --gpus all \\\n  -p 9000:9000 \\\n  --pull always \\\n  -e DOCKER_PULL=on \\\n  -e HF_HUB_CACHE=/root/.cache/huggingface \\\n  -v /mnt/nvme/cache:/root/.cache \\\n  dustynv/mlc:r36.4.0 \\\n    sudonim serve \\\n      --model dusty-nv/Llama-3.1-8B-Instruct-q4f16_ft-MLC \\\n      --quantization q4f16_ft \\\n      --max-batch-size 1 \\\n      --host 0.0.0.0 \\\n      --port 9000\n</code></pre> <pre><code>docker run -it --rm --gpus all \\\n  -p 9000:9000 \\\n  --pull always \\\n  -e DOCKER_PULL=on \\\n  -e HF_HUB_CACHE=/root/.cache/huggingface \\\n  -v /mnt/nvme/cache:/root/.cache \\\n  dustynv/mlc:r36.4.0 \\\n    sudonim serve \\\n      --model dusty-nv/DeepSeek-R1-Distill-Llama-8B-q4f16_ft-MLC \\\n      --quantization q4f16_ft \\\n      --max-batch-size 1 \\\n      --chat-template deepseek_r1_llama \\\n      --host 0.0.0.0 \\\n      --port 9000\n</code></pre>"},{"location":"tutorial_openwebui.html#case-2-docker-compose","title":"Case 2: Docker compose","text":"<p>You can save the following YML file (select one, and save as <code>compose.yml</code>) and issue <code>docker compose up</code>.</p> compose.yml for Llama 3.1 8Bcompose.yml for DeepSeek R1 (Llama-8B) <pre><code># Save as compose.yml and run 'docker compose up'\nservices:\n  llm_server:\n    stdin_open: true\n    tty: true\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities:\n                - gpu\n    ports:\n      - 9000:9000\n    pull_policy: always\n    environment:\n      - DOCKER_PULL=on\n      - HF_HUB_CACHE=/root/.cache/huggingface\n    volumes:\n      - /mnt/nvme/cache:/root/.cache\n    image: dustynv/mlc:r36.4.0\n    command: sudonim serve --model dusty-nv/Llama-3.1-8B-Instruct-q4f16_ft-MLC\n      --quantization q4f16_ft --max-batch-size 1 --host 0.0.0.0 --port 9000\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://0.0.0.0:9000/v1/models\"]\n      interval: 20s\n      timeout: 60s\n      retries: 45\n      start_period: 15s\n\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    restart: always\n    network_mode: host\n    environment:\n      - ENABLE_OPENAI_API=True\n      - OPENAI_API_BASE_URL=http://localhost:9000/v1\n      - OPENAI_API_KEY=foo\n    volumes:\n      - \"${HOME}/open-webui:/app/backend/data\"\n</code></pre> <pre><code># Save as compose.yml and run 'docker compose up'\nservices:\n  llm_server:\n    stdin_open: true\n    tty: true\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities:\n                - gpu\n    ports:\n      - 9000:9000\n    pull_policy: always\n    environment:\n      - DOCKER_PULL=on\n      - HF_HUB_CACHE=/root/.cache/huggingface\n    volumes:\n      - /mnt/nvme/cache:/root/.cache\n    image: dustynv/mlc:r36.4.0\n    command: sudonim serve --model dusty-nv/DeepSeek-R1-Distill-Llama-8B-q4f16_ft-MLC\n      --quantization q4f16_ft --max-batch-size 1 --chat-template deepseek_r1_llama --host 0.0.0.0 --port 9000\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://0.0.0.0:9000/v1/models\"]\n      interval: 20s\n      timeout: 60s\n      retries: 45\n      start_period: 15s\n\n  open-webui:\n    image: ghcr.io/open-webui/open-webui:main\n    container_name: open-webui\n    restart: always\n    network_mode: host\n    environment:\n      - ENABLE_OPENAI_API=True\n      - OPENAI_API_BASE_URL=http://localhost:9000/v1\n      - OPENAI_API_KEY=foo\n    volumes:\n      - \"${HOME}/open-webui:/app/backend/data\"\n</code></pre> <p>Once you save the above YAML file in a directory, issue the following.</p> <pre><code>docker compose up\n</code></pre> <p></p> <p>Note that it generates faster than Ollama (llama.cpp).</p> <p></p>"},{"location":"tutorial_slm.html","title":"Tutorial - Small Language Models (SLM)","text":"<p>Small Language Models (SLMs) represent a growing class of language models that have &lt;7B parameters - for example StableLM, Phi-2, and Gemma-2B.  Their smaller memory footprint and faster performance make them good candidates for deploying on Jetson Orin Nano.  Some are very capable with abilities at a similar level as the larger models, having been trained on high-quality curated datasets.</p> <p></p> <p>This tutorial shows how to run optimized SLMs with quantization using the <code>NanoLLM</code> library and MLC/TVM backend.  You can run these models through tools like <code>text-generation-webui</code> and llama.cpp as well, just not as fast - and since the focus of SLMs is reduced computational and memory requirements, here we'll use the most optimized path available.  Those shown below have been profiled:</p>"},{"location":"tutorial_slm.html#slm-benchmarks","title":"SLM Benchmarks","text":"<p>\u2022 \u00a0 The HuggingFace Open LLM Leaderboard is a collection of multitask benchmarks including reasoning &amp; comprehension, math, coding, history, geography, ect. \u2022 \u00a0 The model's memory footprint includes 4-bit weights and KV cache at full context length (factor in extra for process overhead, library code, ect) \u2022 \u00a0 The <code>Chat Model</code> is the instruction-tuned variant for chatting with in the commands below, as opposed to the base completion model. </p> <p>Based on user interactions, the recommended models to try are <code>stabilityai/stablelm-zephyr-3b</code> and <code>princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT</code>, for having output quality on par with Llama-2-7B and well-optimized neural architectures. These models have also been used as the base for various fine-tunes (for example <code>Nous-Capybara-3B-V1.9</code>) and mini VLMs. Others may not be particularly coherent.</p>"},{"location":"tutorial_slm.html#chatting-with-slms","title":"Chatting with SLMs","text":"<p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>22GB</code> for <code>nano_llm</code> container image</li> <li>Space for models (<code>&gt;5GB</code>)</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol> <p>The <code>nano_llm.chat</code> program will automatically download and quantize models from HuggingFace like those listed in the table above:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.chat --api=mlc \\\n    --model princeton-nlp/Sheared-LLaMA-2.7B-ShareGPT\n</code></pre> <p>\u2022 \u00a0 For models requiring authentication, use <code>--env HUGGINGFACE_TOKEN=&lt;YOUR-ACCESS-TOKEN&gt;</code> \u2022 \u00a0 Press Ctrl+C twice in succession to exit (once will interrupt bot output) </p> <p>This will enter into interactive mode where you chat back and forth using the keyboard (entering <code>reset</code> will clear the chat history)  </p> <p></p>"},{"location":"tutorial_slm.html#automated-prompts","title":"Automated Prompts","text":"<p>During testing, you can specify prompts on the command-line that will run sequentially:</p> <pre><code>jetson-containers run $(autotag nano_llm) \\\n  python3 -m nano_llm.chat --api=mlc \\\n    --model stabilityai/stablelm-zephyr-3b \\\n    --max-new-tokens 512 \\\n    --prompt 'hi, how are you?' \\\n    --prompt 'whats the square root of 900?' \\\n    --prompt 'can I get a recipie for french onion soup?'\n</code></pre> <p>You can also load JSON files containing prompt sequences, like with <code>--prompt /data/prompts/qa.json</code> (the output of which is below)</p>"},{"location":"tutorial_slm.html#results","title":"Results","text":"<p>\u2022 \u00a0 The model responses are with 4-bit quantization, and are truncated to 256 tokens for brevity. \u2022 \u00a0 These chat questions are from <code>/data/prompts/qa.json</code> (found in jetson-containers) </p>"},{"location":"tutorial_slm.html#nemotron-mini","title":"Nemotron Mini","text":"<p>Nemotron-Mini-4B-Instruct is a 4B SLM tuned for on-device deployment, RAG, and function calling and is based on Minitron-4B (pruned and distilled from Nemotron4 15B).  Inference on Jetson is available through HuggingFace Transformers and llama.cpp for quantization.  Here's how to run a local OpenAI-compatible server with llama.cpp and 4-bit quantized GGUF:</p> <pre><code>jetson-containers run $(autotag llama_cpp) \\\n  llama-server \\\n    --hf-repo Obenlia/Nemotron-Mini-4B-Instruct-Q4_K_M-GGUF \\\n    --hf-file nemotron-mini-4b-instruct-q4_k_m.gguf \\\n    --gpu-layers 34 \\\n    --seed 42 \\\n    --host 0.0.0.0 \\\n    --port 8080\n</code></pre> <p>For a quick test, you can navigate your browser to <code>http://JETSON_IP:8080</code>, connect other clients like Open WebUI, or have applications send requests to your server's OpenAI chat completion endpoints (i.e. from openai-python, REST, JavaScript, ect)</p> <p></p> <p>You can more easily see the performance with the <code>llama-cli</code> tool:</p> <pre><code>jetson-containers run $(autotag llama_cpp) \\\n  llama-cli \\\n    --hf-repo Obenlia/Nemotron-Mini-4B-Instruct-Q4_K_M-GGUF \\\n    --hf-file nemotron-mini-4b-instruct-q4_k_m.gguf \\\n    --gpu-layers 34 \\\n    --seed 42 \\\n    --ignore-eos \\\n    -n 128 \\\n    -p \"The meaning to life and the universe is\"\n</code></pre> <pre><code># Jetson AGX Orin\nllama_print_timings:        load time =    1408.27 ms\nllama_print_timings:      sample time =      70.05 ms /   128 runs   (    0.55 ms per token,  1827.32 tokens per second)\nllama_print_timings: prompt eval time =     120.08 ms /     9 tokens (   13.34 ms per token,    74.95 tokens per second)\nllama_print_timings:        eval time =    3303.93 ms /   127 runs   (   26.02 ms per token,    38.44 tokens per second)\nllama_print_timings:       total time =    3597.17 ms /   136 tokens\n</code></pre> <p>The model can also be previewed at build.nvidia.com (example client requests for OpenAI API are also there)</p>"},{"location":"tutorial_slm.html#llama-32","title":"Llama 3.2","text":"<p>Meta has released multilingual 1B and 3B SLMs in the latest additions to the Llama family with <code>Llama-3.2-1B</code> and <code>Llama-3.2-3B</code>.  These can be run with INT4 quantization using the latest MLC container for Jetson (<code>dustynv/mlc:0.1.2-r36.3.0</code>).  After having requested access to the models from Meta with your HuggingFace API key, you can download, quantize, and benchmark them with these commands:</p> <pre><code>HUGGINGFACE_KEY=YOUR_API_KEY \\\nMLC_VERSION=0.1.2 \\\n jetson-containers/packages/llm/mlc/benchmark.sh \\\n   meta-llama/Llama-3.2-1B\n</code></pre> <ul> <li><code>Llama-3.2-1B</code> \u00a0 Jetson Orin Nano 54.8 tokens/sec, Jetson AGX Orin 163.9 tokens/sec</li> <li><code>Llama-3.2-3B</code> \u00a0 Jetson Orin Nano 27.7 tokens/sec, Jetson AGX Orin 80.4 tokens/sec</li> </ul> <p>The Llama-3.2 SLMs use the same core Llama architecture as previous Llama releases (except <code>tie_word_embeddings=True</code>), so it is already supported with quantization and full performance on edge devices.  Thanks to Meta for continuing to advance open generative AI models with Llama.</p>"},{"location":"tutorial_stable-diffusion-xl.html","title":"Tutorial - Stable Diffusion XL","text":"<p>Stable Diffusion XL is a newer ensemble pipeline consisting of a base model and refiner that results in significantly enhanced and detailed image generation capabilities.  All told, SDXL 1.0 has 6.6 billion model parameters, in comparison to 0.98 billion for the original SD 1.5 model.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices (SDXL requires &gt;= ~13GB memory)</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.8GB</code> for container image</li> <li><code>12.4GB</code> for SDXL models</li> </ul> </li> <li> <p>Have followed the previous <code>stable-diffusion-webui</code> tutorial and have the webserver container running.</p> </li> </ol>"},{"location":"tutorial_stable-diffusion-xl.html#downloading-sdxl-models","title":"Downloading SDXL Models","text":"<p>Stable Diffusion XL is supported through AUTOMATIC1111's <code>stable-diffusion-webui</code> with some additional settings.  First you need to download the SDXL models to your <code>jetson-containers</code> data directory (which is automatically mounted into the container)</p> <pre><code># run these outside of container, and replace CONTAINERS_DIR with the path to the jetson-containers repo on your device\nCONTAINERS_DIR=/path/to/your/jetson-containers\nMODEL_DIR=$CONTAINERS_DIR/data/models/stable-diffusion/models/Stable-diffusion/\n\nsudo chown -R $USER $MODEL_DIR\n\nwget -P $MODEL_DIR https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/resolve/main/sd_xl_base_1.0.safetensors\nwget -P $MODEL_DIR https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/resolve/main/sd_xl_refiner_1.0.safetensors\n</code></pre> <p>It's assumed that you already have the <code>stable-diffusion-webui</code> container and webserver running from the previous tutorial.</p>"},{"location":"tutorial_stable-diffusion-xl.html#sdxl-settings","title":"SDXL Settings","text":"<p>After the models have finished downloading, click the \ud83d\udd04 button to refresh the model list in the web UI.  Select <code>sd_xl_base_1.0.safetensors</code> from the Stable Diffusion checkpoint drop-down:</p> <p></p> <p>Then under the Generation tab, expand the Refiner section, and select <code>sd_xl_refiner_1.0.safetensors</code> from the drop-down:</p> <p></p> <p>Guidance on other relevant settings:</p> <ul> <li>Change the width/height to 1024x1024.  SDXL works best at higher resolutions, and using 512x512 often results in more simplistic/cartoonish content.  Changing image resolutions impacts the actual scene contents of the image, not just the details.</li> <li>The refiner's <code>Switch at</code> setting dictates the step at which the refiner takes over from the base model.  At this point, additional subject content will no longer be added to the scene, but rather its details further refined in the image.</li> <li>Typical <code>Sampling steps</code> are between 20-40 and <code>Switch at</code> is between 0.7-0.9.  This takes experimentation to find the best combination for the characteristics of your desired output.</li> <li>Extensive negative prompts are not as necessary in the same way as SD 1.5 was (e.g. <code>low quality, jpeg artifacts, blurry, ect</code>)</li> <li><code>CFG Scale</code> controls how closely the model conforms to your prompt versus how creative it is.</li> </ul> <p>When you get a good image, remember to save your random seed and settings so you can re-generate it later!</p>"},{"location":"tutorial_stable-diffusion-xl.html#results","title":"Results","text":"<p> photograph of a friendly robot alongside a person climbing a mountain (seed 1576166644, steps 25, switch @ 0.8, cfg scale 15)</p> <p> a girl and a boy building a friendly robot in their basement workshop (seed 642273464, steps 25, switch @ 0.9, cfg scale 7)</p> <p> small friendly robots playing games with people, in a futuristic Tokyo central park gardens with cherry blossoms and water, coy fish swimming in the water, sunshine (seed 642273464, steps 40, switch @ 0.8, cfg scale 7)</p> <p> small friendly robots playing games with people in a futuristic New York City Central Park in autumn, water (seed 642273464, steps 25, switch @ 0.8, cfg scale 7)</p> <p>Want to explore using Python APIs to run diffusion models directly? See <code>jetson-containers/stable-diffusion</code>.</p>"},{"location":"tutorial_stable-diffusion.html","title":"Tutorial - Stable Diffusion","text":"<p>Let's run AUTOMATIC1111's <code>stable-diffusion-webui</code> on NVIDIA Jetson to generate images from our prompts!</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.8GB</code> for container image</li> <li><code>4.1GB</code> for SD 1.5 model</li> </ul> </li> </ol>"},{"location":"tutorial_stable-diffusion.html#setup-a-container-for-stable-diffusion-webui","title":"Setup a container for stable-diffusion-webui","text":"<p>The jetson-containers project provides pre-built Docker images for <code>stable-diffusion-webui</code>.  You can clone the repo to use its utilities that will automatically pull/start the correct container for you, or you can do it manually.</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> <p>Info</p> <p>JetsonHacks provides an informative walkthrough video on jetson-containers, showcasing the usage of both the <code>stable-diffusion-webui</code> and <code>text-generation-webui</code>.  You can find the complete article with detailed instructions here.</p> <p></p>"},{"location":"tutorial_stable-diffusion.html#how-to-start","title":"How to start","text":"<p>Use <code>jetson-containers run</code> and <code>autotag</code> tools to automatically pull or build a compatible container image:</p> <pre><code>jetson-containers run $(autotag stable-diffusion-webui)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the webserver like this:</p> <pre><code>cd /opt/stable-diffusion-webui &amp;&amp; python3 launch.py \\\n  --data=/data/models/stable-diffusion \\\n  --enable-insecure-extension-access \\\n  --xformers \\\n  --listen \\\n  --port=7860\n</code></pre> <p>You should see it downloading the model checkpoint on the first run.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code></p>"},{"location":"tutorial_stable-diffusion.html#results-output-examples","title":"Results / Output Examples","text":""},{"location":"tutorial_stable-diffusion.html#stable-diffusion-xl","title":"Stable Diffusion XL","text":"<p>To generate even higher-quality and detailed images, check out the next part of the tutorial that uses the latest Stable Diffusion XL models!</p> <p>Want to explore using Python APIs to run diffusion models directly? See <code>jetson-containers/stable-diffusion</code>.</p>"},{"location":"tutorial_text-generation.html","title":"Tutorial - text-generation-webui","text":"<p>Interact with a local AI assistant by running a LLM with oobabooga's <code>text-generaton-webui</code> on NVIDIA Jetson!</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)\u26a0\ufe0f<sup>1</sup></p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.2GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> </ol>"},{"location":"tutorial_text-generation.html#set-up-a-container-for-text-generation-webui","title":"Set up a container for text-generation-webui","text":"<p>The jetson-containers project provides pre-built Docker images for <code>text-generation-webui</code> along with all of the loader API's built with CUDA enabled (llama.cpp, ExLlama, AutoGPTQ, Transformers, ect).  You can clone the repo to use its utilities that will automatically pull/start the correct container for you, or you can do it manually.</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> <p>Info</p> <p>JetsonHacks provides an informative walkthrough video on jetson-containers, showcasing the usage of both the <code>stable-diffusion-webui</code> and <code>text-generation-webui</code>.  You can find the complete article with detailed instructions here.</p> <p></p>"},{"location":"tutorial_text-generation.html#how-to-start","title":"How to start","text":"<p>Use <code>jetson-containers run</code> and <code>autotag</code> tools to automatically pull or build a compatible container image:</p> <pre><code>jetson-containers run $(autotag text-generation-webui)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the webserver like this:</p> <pre><code>cd /opt/text-generation-webui &amp;&amp; python3 server.py \\\n  --model-dir=/data/models/text-generation-webui \\\n  --chat \\\n  --listen\n</code></pre> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code>.</p>"},{"location":"tutorial_text-generation.html#download-a-model-on-web-ui","title":"Download a model on web UI","text":"<p>See the oobabooga documentation for instructions for downloading models - either from within the web UI, or using <code>download-model.py</code></p> <pre><code>jetson-containers run --workdir=/opt/text-generation-webui $(./autotag text-generation-webui) /bin/bash -c \\\n  'python3 download-model.py --output=/data/models/text-generation-webui TheBloke/Llama-2-7b-Chat-GPTQ'\n</code></pre> <p>From within the web UI, select Model tab and navigate to \"Download model or LoRA\" section.  </p> <p>You can find text generation models on Hugging Face Hub, then enter the Hugging Face username/model path (which you can have copied to your clipboard from the Hub).  Then click the Download button.</p>"},{"location":"tutorial_text-generation.html#gguf-models","title":"GGUF models","text":"<p>The fastest oobabooga model loader to use is currently llama.cpp with 4-bit quantized GGUF models.</p> <p>You can download a single model file for a particular quantization, like <code>*.Q4_K_M.bin</code>. Input the file name and hit Download button.</p> Model Quantization Memory (MB) <code>TheBloke/Llama-2-7b-Chat-GGUF</code> <code>llama-2-7b-chat.Q4_K_M.gguf</code> 5,268 <code>TheBloke/Llama-2-13B-chat-GGUF</code> <code>llama-2-13b-chat.Q4_K_M.gguf</code> 8,609 <code>TheBloke/LLaMA-30b-GGUF</code> <code>llama-30b.Q4_K_S.gguf</code> 19,045 <code>TheBloke/Llama-2-70B-chat-GGUF</code> <code>llama-2-70b-chat.Q4_K_M.gguf</code> 37,655 <p></p> <p>Info</p>"},{"location":"tutorial_text-generation.html#model-selection-for-jetson-orin-nano","title":"Model selection for Jetson Orin Nano","text":"<p>Jetson Orin Nano Developer Kit has only 8GB RAM for both CPU (system) and GPU, so you need to pick a model that fits in the RAM size - see the Model Size section below.  The 7B models with 4-bit quantization are the ones to use on Jetson Orin Nano.  Make sure you go through the RAM optimization steps before attempting to load such model on Jetson Orin Nano.</p>"},{"location":"tutorial_text-generation.html#load-a-model","title":"Load a model","text":"<p>After you have downloaded a model, click the \ud83d\udd04 button to refresh your model list, and select the model you want to use.</p> <p>For a GGUF model, remember to</p> <ul> <li>Set <code>n-gpu-layers</code> to <code>128</code></li> <li>Set <code>n_gqa</code> to <code>8</code> if you using Llama-2-70B (on Jetson AGX Orin 64GB)</li> </ul> <p>Then click the Load button.</p>"},{"location":"tutorial_text-generation.html#chat-template","title":"Chat Template","text":"<p>If you're using a Llama model fine-tuned for chat, like the models listed above (except for <code>LLaMA-30b</code>), you need to use the oobabooga Instruct mode and set the template.  On the Parameters tab, go to the Instruction Template sub-tab, then select <code>Llama-v2</code> from the Instruction Template drop-down (or Vicuna, Guanaco, ect if you are using one of those models)  </p> <p>For the base text completion models (like <code>LLaMA-30b</code>), use the Default or Notebook tab.</p> <p>Selecting the right chat template will make sure the model is being prompted correctly - you can also change the system prompt in the Context box to alter the agent's personality and behavior.  There are a lot of other settings under the Generation tab, like the maximum length it should output per reply, and token sampling parameters like <code>temperature</code> and <code>top_p</code> for controlling randomness.</p> <p>Then change back to the Chat tab, and under the mode section, make sure Instruct is selected (confusingly, not chat mode).  Then you can start chatting with the LLM!</p>"},{"location":"tutorial_text-generation.html#results","title":"Results","text":""},{"location":"tutorial_text-generation.html#things-to-do-with-your-llm","title":"Things to do with your LLM","text":"<p>Here are some common test prompts for coding, math, history ect.  You can also ask it about geography, travel, nature, recipies, fixing things, general life advice, and practically everything else.  Also Llama-2 is quite playful and likes to play games to test its logic abilities!</p> <pre><code>&gt;&gt; What games do you like to play?\n\nI'm a large language model, so I can play text-based games and answer questions on a wide variety of topics. Here are some game ideas:\n\n- 20 Questions:  You or I can think of an object, and the other player tries to guess what it is by asking up to 20 yes-or-no questions.  The goal is to guess the object in as few questions as possible.\n- Hangman: I can think of a word or phrase and you try to guess it by suggesting letters. Each incorrect guess will bring the hangman one step closer to being complete!\n- Word association: I'll give you a word and you have to come up with a word that is related to it. We can keep playing back and forth as long as you'd like!\n- Trivia: I can provide questions on a wide range of topics, from history to science to pop culture. See how many questions you can answer correctly!\n- Storytelling: I can start telling a story and then stop at a cliffhanger. You can then try to guess what happens next or even take over the storytelling and continue it in your own\n- Jokes: I love to tell jokes and make people laugh with my \"Dad humor\"!  Knock knock!  *giggles*\n</code></pre>"},{"location":"tutorial_text-generation.html#model-size-tested","title":"Model size tested","text":"<p>With llama.cpp, GGUF model, 4-bit quantization:</p> Model size Jetson AGX Orin 64GB Jetson AGX Orin 32GB Jetson Orin Nano 8GB 70B model \u2705 30B model \u2705 \u2705 13B model \u2705 \u2705 7B model \u2705 \u2705 \u2705 <p>Want to explore using Python APIs to run LLMs directly?  See <code>jetson-containers</code> for its LLM related packages and containers.</p> <ol> <li> <p>Limited to 7B model (4-bit quantized).\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorial_ultralytics.html","title":"Tutorial - Ultralytics YOLOv8","text":"<p>Let's run Ultralytics YOLOv8 on Jetson with NVIDIA TensorRT.</p> <p>Ultralytics YOLOv8 is a cutting-edge, state-of-the-art (SOTA) model that builds upon the success of previous YOLO versions and introduces new features and improvements to further boost performance and flexibility. YOLOv8 is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and tracking, instance segmentation, image classification and pose estimation tasks.</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB) Jetson Nano (4GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 4 (L4T r32.x) JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> </ol>"},{"location":"tutorial_ultralytics.html#how-to-start","title":"How to start","text":"<p>Execute the below commands according to the JetPack version to pull the corresponding Docker container and run on Jetson.</p> JetPack 4JetPack 5JetPack 6 <pre><code>t=ultralytics/ultralytics:latest-jetson-jetpack4\nsudo docker pull $t &amp;&amp; sudo docker run -it --ipc=host --runtime=nvidia $t\n</code></pre> <pre><code>t=ultralytics/ultralytics:latest-jetson-jetpack5\nsudo docker pull $t &amp;&amp; sudo docker run -it --ipc=host --runtime=nvidia $t\n</code></pre> <pre><code>t=ultralytics/ultralytics:latest-jetson-jetpack6\nsudo docker pull $t &amp;&amp; sudo docker run -it --ipc=host --runtime=nvidia $t\n</code></pre>"},{"location":"tutorial_ultralytics.html#convert-model-to-tensorrt-and-run-inference","title":"Convert model to TensorRT and run inference","text":"<p>The YOLOv8n model in PyTorch format is converted to TensorRT to run inference with the exported model.</p> <p>Example</p> PythonCLI <pre><code>from ultralytics import YOLO\n\n# Load a YOLOv8n PyTorch model\nmodel = YOLO(\"yolov8n.pt\")\n\n# Export the model\nmodel.export(format=\"engine\")  # creates 'yolov8n.engine'\n\n# Load the exported TensorRT model\ntrt_model = YOLO(\"yolov8n.engine\")\n\n# Run inference\nresults = trt_model(\"https://ultralytics.com/images/bus.jpg\")\n</code></pre> <pre><code># Export a YOLOv8n PyTorch model to TensorRT format\nyolo export model=yolov8n.pt format=engine  # creates 'yolov8n.engine'\n\n# Run inference with the exported model\nyolo predict model=yolov8n.engine source='https://ultralytics.com/images/bus.jpg'\n</code></pre> Manufacturing Sports Wildlife Vehicle Spare Parts Detection Football Player Detection Tiger pose Detection <p>Note</p> <p>Visit the Export page to access additional arguments when exporting models to different model formats. Note that the default arguments require inference using fixed image dimensions when <code>dynamic=False</code>. To change the input source for inference, please refer to Model Prediction page. </p>"},{"location":"tutorial_ultralytics.html#benchmarks","title":"Benchmarks","text":"<p>Benchmarks of the YOLOv8 variants with TensorRT were run by Seeed Studio on their reComputer systems:</p> <p></p> Xavier NX 8GBOrin NX 16GBAGX Orin 32GB Model PyTorch FP32 FP16 INT8 YOLOv8n 32 63 120 167 YOLOv8s 25 26 69 112 YOLOv8m 11 11 33 56 YOLOv8l 6 6 20 38 Model PyTorch FP32 FP16 INT8 YOLOv8n 56 115 204 256 YOLOv8s 53 67 128 196 YOLOv8m 26 31 63 93 YOLOv8l 16 20 42 69 Model PyTorch FP32 FP16 INT8 YOLOv8n 77 192 323 385 YOLOv8s 67 119 213 303 YOLOv8m 40 56 105 145 YOLOv8l 27 38 73.5 114 <ul> <li>FP32/FP16/INT8 with TensorRT (frames per second)</li> <li>Original post with the benchmarks are found here</li> </ul>"},{"location":"tutorial_ultralytics.html#further-reading","title":"Further reading","text":"<p>To learn more, visit our comprehensive guide on running Ultralytics YOLOv8 on NVIDIA Jetson including benchmarks!</p> <p>Note</p> <p>Ultralytics YOLOv8 models are offered under <code>AGPL-3.0 License</code> which is an OSI-approved open-source license and is ideal for students and enthusiasts, promoting open collaboration and knowledge sharing. See the LICENSE file for more details.</p>"},{"location":"tutorial_ultralytics.html#one-click-run-ultralytics-yolo-on-jetson-orin-by-seeed-studio-jetson-examples","title":"One-Click Run Ultralytics YOLO on Jetson Orin - by Seeed Studio jetson-examples","text":""},{"location":"tutorial_ultralytics.html#quickstart","title":"Quickstart \u26a1","text":"<ol> <li> <p>Install the package:     <pre><code>pip install jetson-examples\n</code></pre></p> </li> <li> <p>Restart your reComputer:     <pre><code>sudo reboot\n</code></pre></p> </li> <li> <p>Run Ultralytics YOLO on Jetson with one command:     <pre><code>reComputer run ultralytics-yolo\n</code></pre></p> </li> <li>Enter <code>http://127.0.0.1:5001</code> or <code>http://device_ip:5001</code> in your browser to access the Web UI.     <p> </p></li> </ol> <p>For more details, please read: Jetson-Example: Run Ultralytics YOLO Platform Service on NVIDIA Jetson Orin .</p>"},{"location":"tutorial_ultralytics.html#follow-ultralytics-to-stay-updated","title":"Follow Ultralytics to stay updated!","text":""},{"location":"tutorial_voicecraft.html","title":"Tutorial - VoiceCraft","text":"<p>Let's run VoiceCraft, a Zero-Shot Speech Editing and Text-to-Speech in the Wild!</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) </p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p> JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>15.6 GB</code> for <code>voicecraft</code> container image</li> <li>Space for models</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_voicecraft.html#how-to-start","title":"How to start","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run $(autotag voicecraft)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the Gradio app.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:7860</code>.</p>"},{"location":"tutorial_voicecraft.html#gradio-app","title":"Gradio app","text":"<p>VoiceCraft repo comes with Gradio demo app.</p> <ol> <li>Select which models you want to use, I recommend using 330M_TTSEnhanced on 32GB AGX Orin</li> <li>Click load, if you run it for the first time, models are downloaded from huggingface, otherwise are loaded from <code>/data</code> folder, where are saved to from previous runs</li> <li>Upload audio file of your choice (MP3/wav)</li> <li>Click transcribe, it will use whisper to get transcription along with start/end time of each word spoken</li> <li>Now you can edit the sentence, or use TTS. Click Run to generate output.</li> </ol> <p></p> <p>Warning</p> <p>For TTS it's okay to use only first few seconds of audio as prompt, since it consumes a lot of memory. On AGX 32GB Orin the maximal TTS length of generated audio is around ~16 seconds in headless mode.</p>"},{"location":"tutorial_voicecraft.html#resources","title":"Resources","text":"<p>If you want to know how it works under the hood, you can read following papers:</p> <ol> <li>VOICECRAFT: Zero-Shot Speech Editing and Text-to-Speech in the Wild</li> <li>High Fidelity Neural Audio Compression</li> <li>Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers</li> </ol>"},{"location":"tutorial_whisper.html","title":"Tutorial - Whisper","text":"<p>Let's run OpenAI's Whisper, pre-trained model for automatic speech recognition on Jetson!</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.1 GB</code> for <code>whisper</code> container image</li> <li>Space for checkpoints</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"tutorial_whisper.html#how-to-start","title":"How to start","text":"<p>Use <code>run.sh</code> and <code>autotag</code> script to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run $(autotag whisper)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the Jupyter Lab server, with SSL enabled.</p> <p>Open your browser and access <code>https://&lt;IP_ADDRESS&gt;:8888</code>.</p> <p>Attention</p> <p>Note it is <code>https</code> (not <code>http</code>).</p> <p>HTTPS (SSL) connection is needed to allow <code>ipywebrtc</code> widget to have access to your microphone (for <code>record-and-transcribe.ipynb</code>).</p> <p>You will see a warning message like this.</p> <p></p> <p>Press \"Advanced\" button and then click on \"Proceed to  (unsafe)\" link to proceed to the Jupyter Lab web interface. <p></p> <p>The default password for Jupyter Lab is <code>nvidia</code>.</p>"},{"location":"tutorial_whisper.html#run-jupyter-notebooks","title":"Run Jupyter notebooks","text":"<p>Whisper repo comes with demo Jupyter notebooks, which you can find under <code>/notebooks/</code> directory.</p> <p><code>jetson-containers</code> also adds one convenient notebook (<code>record-and-transcribe.ipynb</code>) to record your audio sample on Jupyter notebook in order to run transcribe on your recorded audio. </p> <p></p>"},{"location":"tutorial_whisper.html#record-and-transcribeipynb","title":"<code>record-and-transcribe.ipynb</code>","text":"<p>This notebook is to let you record your own audio sample using your PC's microphone and apply Whisper's <code>medium</code> model to transcribe the audio sample.</p> <p>It uses Jupyter notebook/lab's <code>ipywebrtc</code> extension to record an audio sample on your web browser.</p> <p></p> <p>Attention</p> <p>When you click the \u23fa botton, your web browser may show a pop-up to ask you to allow it to use your microphone. Be sure to allow the access.</p> <p></p> Final check <p>Once done, if you click on the \"\u26a0 Not secure\" part in the URL bar, you should see something like this.</p> <p></p>"},{"location":"tutorial_whisper.html#result","title":"Result","text":"<p>Once you go through all the steps, you should see the transcribe result in text like this.</p> <p></p>"},{"location":"portal/index.html","title":"nanolab","text":"<p>Lightweight JavaScript and Python utilities for bootstrapping edge2cloud microservices and local AI/ML experiments</p> <ul> <li>Serverless property graph database with in-browser queries and search portal.</li> <li>Generate docker-run and docker-compose commands for automated model deployment.</li> <li>Easily update content packs with new models, tutorials, community projects, ect.</li> <li>Low-latency multimodal streaming with WebRTC, OpenAI endpoints, in-browser video.</li> <li>Interactive launcher, model builder, node-based editor, dockable grids, ect.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html","title":"Changelog","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v3101-2022-08-02","title":"v3.10.1 2022-08-02","text":"<ul> <li>Add sponsorship files.<ul> <li>If you appreciate the time spent maintaining JSZip then I would really appreciate your sponsorship.</li> </ul> </li> <li>Consolidate metadata types and expose OnUpdateCallback #851 and #852</li> <li>use <code>const</code> instead <code>var</code> in example from README.markdown #828</li> <li>Switch manual download link to HTTPS #839</li> </ul> <p>Internals:</p> <ul> <li>Replace jshint with eslint #842</li> <li>Add performance tests #834</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v3100-2022-05-20","title":"v3.10.0 2022-05-20","text":"<ul> <li>Change setimmediate dependency to more efficient one. Fixes https://github.com/Stuk/jszip/issues/617 (see #829)</li> <li>Update types of <code>currentFile</code> metadata to include <code>null</code> (see #826)</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v391-2022-04-06","title":"v3.9.1 2022-04-06","text":"<ul> <li>Fix recursive definition of <code>InputFileFormat</code> introduced in 3.9.0.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v390-2022-04-04","title":"v3.9.0 2022-04-04","text":"<ul> <li>Update types JSZip#loadAsync to accept a promise for data, and remove arguments from <code>new JSZip()</code> (see #752)</li> <li>Update types for <code>compressionOptions</code> to JSZipFileOptions and JSZipGeneratorOptions (see #722)</li> <li>Add types for <code>generateInternalStream</code> (see #774)</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v380-2022-03-30","title":"v3.8.0 2022-03-30","text":"<ul> <li>Santize filenames when files are loaded with <code>loadAsync</code>, to avoid \"zip slip\" attacks. The original filename is available on each zip entry as <code>unsafeOriginalName</code>. See the documentation. Many thanks to McCaulay Hudson for reporting.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v371-2021-08-05","title":"v3.7.1 2021-08-05","text":"<ul> <li>Fix build of <code>dist</code> files.<ul> <li>Note: this version ensures the changes from 3.7.0 are actually included in the <code>dist</code> files. Thanks to Evan W for reporting.</li> </ul> </li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v370-2021-07-23","title":"v3.7.0 2021-07-23","text":"<ul> <li>Fix: Use a null prototype object for this.files  (see #766)<ul> <li>This change might break existing code if it uses prototype methods on the <code>.files</code> property of a zip object, for example <code>zip.files.toString()</code>. This approach is taken to prevent files in the zip overriding object methods that would exist on a normal object.</li> </ul> </li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v360-2021-02-09","title":"v3.6.0 2021-02-09","text":"<ul> <li>Fix: redirect main to dist on browsers (see #742)</li> <li>Fix duplicate require DataLengthProbe, utils (see #734)</li> <li>Fix small error in read_zip.md (see #703)</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v350-2020-05-31","title":"v3.5.0 2020-05-31","text":"<ul> <li>Fix 'End of data reached' error when file extra field is invalid (see #544).</li> <li>Typescript definitions: Add null to return types of functions that may return null (see #669).</li> <li>Typescript definitions: Correct nodeStream's type (see #682)</li> <li>Typescript definitions: Add string output type (see #666)</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v340-2020-04-19","title":"v3.4.0 2020-04-19","text":"<ul> <li>Add Typescript type definitions (see #601).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v330-2020-04-1","title":"v3.3.0 2020-04-1","text":"<ul> <li>Change browser module resolution to support Angular packager (see #614).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v322-2019-07-04","title":"v3.2.2 2019-07-04","text":"<ul> <li>No public changes, but a number of testing dependencies have been updated.</li> <li>Tested browsers are now: Internet Explorer 11, Chrome (most recent) and Firefox (most recent). Other browsers (specifically Safari) are still supported however testing them on Saucelabs is broken and so they were removed from the test matrix.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v321-2019-03-22","title":"v3.2.1 2019-03-22","text":"<ul> <li>Corrected built dist files</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v320-2019-02-21","title":"v3.2.0 2019-02-21","text":"<ul> <li>Update dependencies to reduce bundle size (see #532).</li> <li>Fix deprecated Buffer constructor usage and add safeguards (see #506).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v315-2017-11-09","title":"v3.1.5 2017-11-09","text":"<ul> <li>Fix IE11 memory leak (see #429).</li> <li>Handle 2 nodejs deprecations (see #459).</li> <li>Improve the \"unsupported format\" error message (see #461).</li> <li>Improve webworker compatibility (see #468).</li> <li>Fix nodejs 0.10 compatibility (see #480).</li> <li>Improve the error without type in async() (see #481).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v314-2017-08-24","title":"v3.1.4 2017-08-24","text":"<ul> <li>consistently use our own utils object for inheritance (see #395).</li> <li>lower the memory consumption in <code>generate*</code> with a lot of files (see #449).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v313-2016-10-06","title":"v3.1.3 2016-10-06","text":"<ul> <li>instanceof failing in window / iframe contexts (see #350).</li> <li>remove a copy with blob output (see #357).</li> <li>fix crc32 check for empty entries (see #358).</li> <li>fix the base64 error message with data uri (see #359).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v312-2016-08-23","title":"v3.1.2 2016-08-23","text":"<ul> <li>fix support of nodejs <code>process.platform</code> in <code>generate*</code> methods (see #335).</li> <li>improve browserify/webpack support (see #333).</li> <li>partial support of a promise of text (see #337).</li> <li>fix streamed zip files containing folders (see #342).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v311-2016-08-08","title":"v3.1.1 2016-08-08","text":"<ul> <li>Use a hard-coded JSZip.version, fix an issue with webpack (see #328).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v310-2016-08-03","title":"v3.1.0 2016-08-03","text":"<ul> <li>utils.delay: use macro tasks instead of micro tasks (see #288).</li> <li>Harden base64 decode (see #316).</li> <li>Add JSZip.version and the version in the header (see #317).</li> <li>Support Promise(Blob) (see #318).</li> <li>Change JSZip.external.Promise implementation (see #321).</li> <li>Update pako to v1.0.2 to fix a DEFLATE bug (see #322).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v300-2016-04-13","title":"v3.0.0 2016-04-13","text":"<p>This release changes a lot of methods, please see the upgrade guide.</p> <ul> <li>replace sync getters and <code>generate()</code> with async methods (see #195).</li> <li>support nodejs streams (in <code>file()</code> and <code>generateAsync()</code>).</li> <li>support Blob and Promise in <code>file()</code> and <code>loadAsync()</code> (see #275).</li> <li>add <code>support.nodestream</code>.</li> <li>zip.filter: remove the defensive copy.</li> <li>remove the deprecated API (see #253).</li> <li><code>type</code> is now mandatory in <code>generateAsync()</code>.</li> <li>change the createFolders default value (now <code>true</code>).</li> <li>Dates: use UTC instead of the local timezone.</li> <li>Add <code>base64</code> and <code>array</code> as possible output type.</li> <li>Add a forEach method.</li> <li>Drop node 0.8 support (see #270).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v261-2016-07-28","title":"v2.6.1 2016-07-28","text":"<ul> <li>update pako to v1.0.2 to fix a DEFLATE bug (see #322).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v260-2016-03-23","title":"v2.6.0 2016-03-23","text":"<ul> <li>publish <code>dist/</code> files in the npm package (see #225).</li> <li>update pako to v1.0.0 (see #261).</li> <li>add support of Array in JSZip#load (see #252).</li> <li>improve file name / comment encoding support (see #211).</li> <li>handle prepended data (see #266).</li> <li>improve platform coverage in tests (see #233 and #269).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v250-2015-03-10","title":"v2.5.0 2015-03-10","text":"<ul> <li>add support for custom mime-types (see #199).</li> <li>add an option to set the DEFLATE level (see #201).</li> <li>improve the error message with corrupted zip (see #202).</li> <li>add support for UNIX / DOS permissions (see #200 and #205).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v240-2014-07-24","title":"v2.4.0 2014-07-24","text":"<ul> <li>update pako to 0.2.5 (see #156).</li> <li>make JSZip work in a Firefox addon context (see #151).</li> <li>add an option (<code>createFolders</code>) to control the subfolder generation (see #154).</li> <li>allow <code>Buffer</code> polyfill in the browser (see #139).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v230-2014-06-18","title":"v2.3.0 2014-06-18","text":"<ul> <li>don't generate subfolders (see #130).</li> <li>add comment support (see #134).</li> <li>on <code>ZipObject#options</code>, the attributes <code>date</code> and <code>dir</code> have been deprecated and are now on <code>ZipObject</code> (see the upgrade guide).</li> <li>on <code>ZipObject#options</code>, the attributes <code>base64</code> and <code>binary</code> have been deprecated (see the upgrade guide).</li> <li>deprecate internal functions exposed in the public API (see #123).</li> <li>improve UTF-8 support (see #142).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v222-2014-05-01","title":"v2.2.2, 2014-05-01","text":"<ul> <li>update pako to v0.2.1, fix an error when decompressing some files (see #126).</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v221-2014-04-23","title":"v2.2.1, 2014-04-23","text":"<ul> <li>fix unreadable generated file on Windows 8 (see #112).</li> <li>replace zlibjs with pako.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v220-2014-02-25","title":"v2.2.0, 2014-02-25","text":"<ul> <li>make the <code>new</code> operator optional before the <code>JSZip</code> constructor (see #93).</li> <li>update zlibjs to v0.2.0.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v211-2014-02-13","title":"v2.1.1, 2014-02-13","text":"<ul> <li>use the npm package for zlib.js instead of the github url.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v210-2014-02-06","title":"v2.1.0, 2014-02-06","text":"<ul> <li>split the files and use Browserify to generate the final file (see #74)</li> <li>packaging change : instead of 4 files (jszip.js, jszip-load.js, jszip-inflate.js, jszip-deflate.js) we now have 2 files : dist/jszip.js and dist/jszip.min.js</li> <li>add component/bower support</li> <li>rename variable: 'byte' is a reserved word (see #76)</li> <li>add support for the unicode path extra field (see #82)</li> <li>ensure that the generated files have a header with the licenses (see #80)</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v200-2013-10-20","title":"v2.0.0, 2013-10-20","text":"<ul> <li><code>JSZipBase64</code> has been renamed to <code>JSZip.base64</code>.</li> <li> <p>The <code>data</code> attribute on the object returned by <code>zip.file(name)</code> has been removed. Use <code>asText()</code>, <code>asBinary()</code>, <code>asUint8Array()</code>, <code>asArrayBuffer()</code> or <code>asNodeBuffer()</code>.</p> </li> <li> <p>Fix issue with Android browser</p> </li> <li> <p>The compression/decompression methods now give their input type with the <code>compressInputType</code> and <code>uncompressInputType</code> attributes.</p> </li> <li>Lazily decompress data when needed and improve performance in general</li> <li>Add support for <code>Buffer</code> in Node.js.</li> <li>Package for CommonJS/npm.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v101-2013-03-04","title":"v1.0.1, 2013-03-04","text":"<ul> <li>Fixed an issue when generating a compressed zip file with empty files or folders, see #33.</li> <li>With bad data (null or undefined), asText/asBinary/asUint8Array/asArrayBuffer methods now return an empty string, see #36.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/CHANGES.html#v100-2013-02-14","title":"v1.0.0, 2013-02-14","text":"<ul> <li>First release after a long period without version.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/LICENSE.html","title":"LICENSE","text":"<p>JSZip is dual licensed. At your choice you may use it under the MIT license or the GPLv3 license.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/LICENSE.html#the-mit-license","title":"The MIT License","text":"<p>Copyright (c) 2009-2016 Stuart Knightley, David Duponchel, Franz Buchinger, Ant\u00f3nio Afonso</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/LICENSE.html#gpl-version-3","title":"GPL version 3","text":"<pre><code>                GNU GENERAL PUBLIC LICENSE\n                   Version 3, 29 June 2007\n</code></pre> <p>Copyright (C) 2007 Free Software Foundation, Inc. http://fsf.org/  Everyone is permitted to copy and distribute verbatim copies  of this license document, but changing it is not allowed.</p> <pre><code>                        Preamble\n</code></pre> <p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works.  By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users.  We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors.  You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price.  Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights.  Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received.  You must make sure that they, too, receive or can get the source code.  And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software.  For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so.  This is fundamentally incompatible with the aim of protecting users' freedom to change the software.  The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable.  Therefore, we have designed this version of the GPL to prohibit the practice for those products.  If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary.  To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p> <pre><code>                   TERMS AND CONDITIONS\n</code></pre> <ol> <li>Definitions.</li> </ol> <p>\"This License\" refers to version 3 of the GNU General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License.  Each licensee is addressed as \"you\".  \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy.  The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy.  Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies.  Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License.  If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p> <ol> <li>Source Code.</li> </ol> <p>The \"source code\" for a work means the preferred form of the work for making modifications to it.  \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form.  A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities.  However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work.  For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p> <ol> <li>Basic Permissions.</li> </ol> <p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met.  This License explicitly affirms your unlimited permission to run the unmodified Program.  The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work.  This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force.  You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright.  Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below.  Sublicensing is not allowed; section 10 makes it unnecessary.</p> <ol> <li>Protecting Users' Legal Rights From Anti-Circumvention Law.</li> </ol> <p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p> <ol> <li>Conveying Verbatim Copies.</li> </ol> <p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p> <ol> <li>Conveying Modified Source Versions.</li> </ol> <p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <pre><code>a) The work must carry prominent notices stating that you modified\nit, and giving a relevant date.\n\nb) The work must carry prominent notices stating that it is\nreleased under this License and any conditions added under section\n7.  This requirement modifies the requirement in section 4 to\n\"keep intact all notices\".\n\nc) You must license the entire work, as a whole, under this\nLicense to anyone who comes into possession of a copy.  This\nLicense will therefore apply, along with any applicable section 7\nadditional terms, to the whole of the work, and all its parts,\nregardless of how they are packaged.  This License gives no\npermission to license the work in any other way, but it does not\ninvalidate such permission if you have separately received it.\n\nd) If the work has interactive user interfaces, each must display\nAppropriate Legal Notices; however, if the Program has interactive\ninterfaces that do not display Appropriate Legal Notices, your\nwork need not make them do so.\n</code></pre> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit.  Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p> <ol> <li>Conveying Non-Source Forms.</li> </ol> <p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <pre><code>a) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by the\nCorresponding Source fixed on a durable physical medium\ncustomarily used for software interchange.\n\nb) Convey the object code in, or embodied in, a physical product\n(including a physical distribution medium), accompanied by a\nwritten offer, valid for at least three years and valid for as\nlong as you offer spare parts or customer support for that product\nmodel, to give anyone who possesses the object code either (1) a\ncopy of the Corresponding Source for all the software in the\nproduct that is covered by this License, on a durable physical\nmedium customarily used for software interchange, for a price no\nmore than your reasonable cost of physically performing this\nconveying of source, or (2) access to copy the\nCorresponding Source from a network server at no charge.\n\nc) Convey individual copies of the object code with a copy of the\nwritten offer to provide the Corresponding Source.  This\nalternative is allowed only occasionally and noncommercially, and\nonly if you received the object code with such an offer, in accord\nwith subsection 6b.\n\nd) Convey the object code by offering access from a designated\nplace (gratis or for a charge), and offer equivalent access to the\nCorresponding Source in the same way through the same place at no\nfurther charge.  You need not require recipients to copy the\nCorresponding Source along with the object code.  If the place to\ncopy the object code is a network server, the Corresponding Source\nmay be on a different server (operated by you or a third party)\nthat supports equivalent copying facilities, provided you maintain\nclear directions next to the object code saying where to find the\nCorresponding Source.  Regardless of what server hosts the\nCorresponding Source, you remain obligated to ensure that it is\navailable for as long as needed to satisfy these requirements.\n\ne) Convey the object code using peer-to-peer transmission, provided\nyou inform other peers where the object code and Corresponding\nSource of the work are being offered to the general public at no\ncharge under subsection 6d.\n</code></pre> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling.  In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage.  For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product.  A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source.  The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information.  But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed.  Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p> <ol> <li>Additional Terms.</li> </ol> <p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law.  If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it.  (Additional permissions may be written to require their own removal in certain cases when you modify the work.)  You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <pre><code>a) Disclaiming warranty or limiting liability differently from the\nterms of sections 15 and 16 of this License; or\n\nb) Requiring preservation of specified reasonable legal notices or\nauthor attributions in that material or in the Appropriate Legal\nNotices displayed by works containing it; or\n\nc) Prohibiting misrepresentation of the origin of that material, or\nrequiring that modified versions of such material be marked in\nreasonable ways as different from the original version; or\n\nd) Limiting the use for publicity purposes of names of licensors or\nauthors of the material; or\n\ne) Declining to grant rights under trademark law for use of some\ntrade names, trademarks, or service marks; or\n\nf) Requiring indemnification of licensors and authors of that\nmaterial by anyone who conveys the material (or modified versions of\nit) with contractual assumptions of liability to the recipient, for\nany liability that these contractual assumptions directly impose on\nthose licensors and authors.\n</code></pre> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10.  If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term.  If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p> <ol> <li>Termination.</li> </ol> <p>You may not propagate or modify a covered work except as expressly provided under this License.  Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License.  If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p> <ol> <li>Acceptance Not Required for Having Copies.</li> </ol> <p>You are not required to accept this License in order to receive or run a copy of the Program.  Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance.  However, nothing other than this License grants you permission to propagate or modify any covered work.  These actions infringe copyright if you do not accept this License.  Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p> <ol> <li>Automatic Licensing of Downstream Recipients.</li> </ol> <p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License.  You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations.  If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License.  For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p> <ol> <li>Patents.</li> </ol> <p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based.  The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version.  For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement).  To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients.  \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License.  You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p> <ol> <li>No Surrender of Others' Freedom.</li> </ol> <p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License.  If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all.  For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p> <ol> <li>Use with the GNU Affero General Public License.</li> </ol> <p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work.  The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p> <ol> <li>Revised Versions of this License.</li> </ol> <p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time.  Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number.  If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation.  If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions.  However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p> <ol> <li>Disclaimer of Warranty.</li> </ol> <p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p> <ol> <li>Limitation of Liability.</li> </ol> <p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p> <ol> <li>Interpretation of Sections 15 and 16.</li> </ol> <p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <pre><code>                 END OF TERMS AND CONDITIONS\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/sponsors.html","title":"Sponsors","text":"<p>JSZip was created in 2009 by Stuart. Since then it has received well over 600 million downloads, is depended on by over 3000 packages on npm, and powers zipping and unzipping on sites large and small.</p> <p>This project only exists because of all the work dedicated to it by me and the other contributors.</p> <p>If you or your company has benefited from JSZip then please consider sponsoring on Github.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/sponsors.html#diamond","title":"\ud83d\udc8e Diamond","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/sponsors.html#gold","title":"\ud83e\udd47 Gold","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/sponsors.html#silver","title":"\ud83e\udd48 Silver","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/sponsors.html#bronze","title":"\ud83e\udd49 Bronze","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/sponsors.html#supporters","title":"Supporters","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip.html","title":"JSZip API","text":"<p>An instance of JSZip represents a set of files. You can add them, remove them, modify them. You can also import an existing zip file or generate one.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip.html#attributes","title":"Attributes","text":"attribute name type description <code>files</code> object the ZipObjects inside the zip with the name as key. See file(name). <code>comment</code> string the comment of the zip file."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper.html","title":"StreamHelper API","text":"<p>A <code>StreamHelper</code> can be viewed as a pausable stream with some helper methods. It is not a full featured stream like in nodejs (and can't directly used as one) but the exposed methods should be enough to write the glue code with other async libraries : <code>on('data', function)</code>, <code>on('end', function)</code> and <code>on('error', function)</code>.</p> <p>It starts paused, be sure to <code>resume()</code> it when ready.</p> <p>If you are looking for an asynchronous helper without writing glue code, take a look at <code>accumulate(function)</code>.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject.html","title":"ZipObject API","text":"<p>This represents an entry in the zip file. If the entry comes from an existing archive previously loaded, the content will be automatically decompressed/converted first.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject.html#attributes","title":"Attributes","text":"attribute name type description <code>name</code> string the absolute path of the file <code>dir</code> boolean true if this is a directory <code>date</code> date the last modification date <code>comment</code> string the comment for this file <code>unixPermissions</code> 16 bits number The UNIX permissions of the file, if any. <code>dosPermissions</code> 6 bits number The DOS permissions of the file, if any. <code>options</code> object the options of the file. The available options are : <code>options.compression</code> compression see file(name, data [,options]) <code>options.compressionOptions</code> object see file(name, data [,options]) <p>Example:</p> <pre><code>{ name: 'docs/',\n  dir: true,\n  date: 2016-12-25T08:09:27.153Z,\n  comment: null,\n  unixPermissions: 16877,\n  dosPermissions: null,\n  options: {\n    compression: 'STORE',\n    compressionOptions: null\n  }\n}\n</code></pre> <pre><code>{ name: 'docs/list.txt',\n  dir: false,\n  date: 2016-12-25T08:09:27.152Z,\n  comment: null,\n  unixPermissions: 33206,\n  dosPermissions: null,\n  options: {\n    compression: 'DEFLATE',\n    compressionOptions: null\n  }\n}\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/contributing.html","title":"Contributing","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/contributing.html#download-the-sources","title":"Download the sources","text":"<p>You should create a Github account and fork the repository (you will need one to create the pull request).</p> <p>If you just want the get the source code, you can use git and do <code>git clone https://github.com/Stuk/jszip.git</code> to get the sources. You can also download the latest sources here.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/contributing.html#building-the-project","title":"Building the project","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/contributing.html#code","title":"Code","text":"<p>The dependencies are handled by npm, the first step is to run <code>npm install</code> to get the dependencies. JSZip uses Grunt to handle the build, see here to install its CLI.</p> <p>Here are the interesting build commands :</p> <ul> <li><code>grunt</code> will generate the final js file in dist/ and the minified version.</li> <li><code>npm run test-node</code> will run the tests in nodejs.</li> <li><code>npm run test-browser</code> will the tests in some browsers using SauceLabs, see   below.</li> <li><code>npm run test</code> will run the tests in nodejs and in the browser.</li> <li><code>npm run lint</code> will use eslint the check the source code.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/contributing.html#documentation","title":"Documentation","text":"<p>The documentation uses jekyll on gh-pages. To render the documentation, you need to install jekyll and then run <code>jekyll serve --baseurl ''</code>.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/contributing.html#testing-the-project","title":"Testing the project","text":"<p>To test JSZip in nodejs, use <code>npm run test-node</code>.</p> <p>To test JSZip in a browser, you can open the file <code>test/index.html</code> in the browser you want to test. Don't forget to update the dist/ files with <code>grunt</code>.</p> <p>You can also test JSZip in a lot of browsers at once with SauceLabs. You will need a SauceLabs account and two variables into your environment. On linux, just use</p> <pre><code>export SAUCE_USERNAME=your-saucelabs-username\nexport SAUCE_ACCESS_KEY=your-saucelabs-access-key\n</code></pre> <p>before running the <code>npm run test-browser</code> command.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/contributing.html#merging-the-changes","title":"Merging the changes","text":"<p>If you have tested bug fixes or new features, you can open a pull request on Github.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/contributing.html#releasing-a-new-version","title":"Releasing a new version","text":"<ol> <li>In <code>package.json</code> temporarily remove <code>browser[\"./lib/index\"]</code>.</li> <li>Run <code>npm test</code><ul> <li>Locally open http://localhost:8080/test/</li> <li>Or use the SauceLabs configuration above</li> </ul> </li> <li>Update <code>JSZip.version</code> in <code>index.js</code>, <code>index.html</code> and in <code>package.json</code></li> <li>Run <code>grunt</code> to generate the new dist files<ul> <li>undo the package.json change, it was just needed to replace the <code>__VERSION__</code> in the header</li> </ul> </li> <li>Undo step 1.</li> <li>Change version back in <code>package.json</code> (it will get update by the npm command below)</li> <li>Update CHANGES.md</li> <li>Commit the appropriate changes</li> <li>Run <code>npm version ...</code> where <code>...</code> is <code>major</code>, <code>minor</code>, or <code>patch</code></li> <li>Run npm publish</li> </ol>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/examples.html","title":"How to use JSZip","text":"<p>An instance of JSZip represents a set of files. You can add them, remove them, modify them. You can also import an existing zip file or generate one.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/examples.html#getting-the-object","title":"Getting the object","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/examples.html#in-a-browser","title":"In a browser","text":"<p>For a browser, there are two interesting files : <code>dist/jszip.js</code> and <code>dist/jszip.min.js</code> (include just one).</p> <p>If you use an AMD loader (RequireJS for example) JSZip will register itself : you just have to put the js file at the right place, or configure the loader (see here for RequireJS).</p> <p>Without any loader, JSZip will declare in the global scope a variable named <code>JSZip</code>.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/examples.html#in-nodejs","title":"In nodejs","text":"<p>In nodejs, you can <code>require</code> it :</p> <pre><code>var JSZip = require(\"jszip\");\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/examples.html#basic-manipulations","title":"Basic manipulations","text":"<p>The first step is to create an instance of JSZip :</p> <pre><code>var zip = new JSZip();\n</code></pre> <p>On this instance, we can add (and update) files and folders with <code>.file(name, content)</code> and <code>.folder(name)</code>. They return the current JSZip instance so you can chain the calls.</p> <pre><code>// create a file\nzip.file(\"hello.txt\", \"Hello[p my)6cxsw2q\");\n// oops, cat on keyboard. Fixing !\nzip.file(\"hello.txt\", \"Hello World\\n\");\n\n// create a file and a folder\nzip.file(\"nested/hello.txt\", \"Hello World\\n\");\n// same as\nzip.folder(\"nested\").file(\"hello.txt\", \"Hello World\\n\");\n</code></pre> <p>With <code>.folder(name)</code>, the returned object has a different root : if you add files on this object, you will put them in the created subfolder. This is just a view, the added files will also be in the \"root\" object.</p> <pre><code>var photoZip = zip.folder(\"photos\");\n// this call will create photos/README\nphotoZip.file(\"README\", \"a folder with photos\");\n</code></pre> <p>You can access the file content with <code>.file(name)</code> and its getters :</p> <pre><code>zip.file(\"hello.txt\").async(\"string\").then(function (data) {\n  // data is \"Hello World\\n\"\n});\n\nif (JSZip.support.uint8array) {\n  zip.file(\"hello.txt\").async(\"uint8array\").then(function (data) {\n    // data is Uint8Array { 0=72, 1=101, 2=108, more...}\n  });\n}\n</code></pre> <p>You can also remove files or folders with <code>.remove(name)</code> :</p> <pre><code>zip.remove(\"photos/README\");\nzip.remove(\"photos\");\n// same as\nzip.remove(\"photos\"); // by removing the folder, you also remove its content.\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/examples.html#generate-a-zip-file","title":"Generate a zip file","text":"<p>With <code>.generateAsync(options)</code> or <code>.generateNodeStream(options)</code> you can generate a zip file (not a real file but its representation in memory). Check this page for more information on how to write / give the file to the user.</p> <pre><code>var promise = null;\nif (JSZip.support.uint8array) {\n  promise = zip.generateAsync({type : \"uint8array\"});\n} else {\n  promise = zip.generateAsync({type : \"string\"});\n}\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/examples.html#read-a-zip-file","title":"Read a zip file","text":"<p>With <code>.loadAsync(data)</code> you can load a zip file. Check this page to see how to do properly (it's more tricky that it seems).</p> <pre><code>var new_zip = new JSZip();\n// more files !\nnew_zip.loadAsync(content)\n.then(function(zip) {\n    // you now have every files contained in the loaded zip\n    zip.file(\"hello.txt\").async(\"string\"); // a promise of \"Hello World\\n\"\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/faq.html","title":"Frequently Asked Questions","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/faq.html#corrupted-zip-or-bug-unexpected-signature","title":"\"Corrupted zip or bug: unexpected signature\"","text":"<p>If you are sure that the zip file is correct, that error often comes from a corrupted content. An ajax request, if not prepared correctly, will try to decode the binary content as a text and corrupt it. See this page.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/faq.html#my-browser-crashes-becomes-unresponsive-never-finish-the-execution","title":"My browser crashes / becomes unresponsive / never finish the execution","text":"<p>That happens if you try to handle to much data with the synchronous API. If possible, try the asynchronous API, see this page for more information.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/faq.html#cant-read-the-data-of-is-it-in-a-supported-javascript-type","title":"Can't read the data of [...]. Is it in a supported JavaScript type ?","text":"<p>Or the old message:</p> <p>The data of [...] is in an unsupported format</p> <p>The method <code>file(name, data [,options])</code> accepts string and binary inputs for <code>data</code>.</p> <p>If you use an unsupported type, an object for example, you will get this error:</p> <pre><code>// WRONG\nvar data = {\n    content: new ArrayBuffer(...)\n};\nzip.file(\"my.data\", data); // won't work, data is an object\n\n// CORRECT\nvar data = new ArrayBuffer(...);\nzip.file(\"my.data\", data); // will work, JSZip accepts ArrayBuffer\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/faq.html#my-mac-generates-a-cpgz-file-when-i-try-to-extract-the-zip-file","title":"My mac generates a <code>.cpgz</code> file when I try to extract the zip file","text":"<p>MacOS Finder has a lot of bug related to zip files (the <code>unzip</code> command line tool is fine). When something goes wrong, Finder will generate this cpgz file instead of showing an error.</p> <p>To get a correct result, try to enable compression in <code>generateAsync</code>:</p> <pre><code>zip.generateAsync({\n    type:\"...\",\n    compression: \"DEFLATE\" // &lt;-- here\n});\n</code></pre> <p>Using <code>platform: \"UNIX\"</code> may help too.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/limitations.html","title":"Limitations of JSZip","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/limitations.html#not-supported-features","title":"Not supported features","text":"<p>Not all features of zip files are supported. Classic zip files will work but encrypted zip, multi-volume, etc are not supported and the loadAsync() method will return a failed promise.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/limitations.html#zip64-and-32bit-integers","title":"ZIP64 and 32bit integers","text":"<p>ZIP64 files can be loaded, but only if the zip file is not \"too big\". ZIP64 uses 64bits integers but JavaScript represents all numbers as 64-bit double precision IEEE 754 floating point numbers (see section 8.5). So, we have 53bits for integers and bitwise operations treat everything as 32bits. So if all the 64bits integers can fit into 32 bits integers, everything will be fine. If it's not the case, you will have other problems anyway (see next limitation).</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/limitations.html#performance-issues","title":"Performance issues","text":"<p>An other limitation comes from the browser (and the machine running the browser). A compressed zip file of 10MB is easily opened by firefox / chrome / opera / IE10+ but will crash older IE. Also keep in mind that strings in javascript are encoded in UTF-16 : a 10MB ascii text file will take 20MB of memory.</p> <p>The <code>async</code> method and the <code>generateAsync</code> method hold the full result in memory but doesn't freeze the browser. If the result is too big, and if you can't use the <code>nodeStream</code> method or the <code>generateNodeStream</code> method you need to use the underlying <code>StreamHelper</code> to handle the result chunk by chunk and <code>pause()</code>/<code>resume()</code> to handle the backpressure.</p> <p>If you're having performance issues, please consider the following :</p> <ul> <li>Don't use IE &lt;= 9. Everything is better with typed arrays.</li> <li>Use typed arrays (Uint8Array, ArrayBuffer, etc) if possible :</li> <li>If you generate a zip file, you should use <code>type:\"uint8array\"</code>     (or blob, arraybuffer, nodebuffer).</li> <li>If you load the file from an ajax call, ask your XHR an ArrayBuffer.     Loading a string is asking for troubles.</li> </ul> <p>Note about compression : When reading a file, JSZip will store the content without decompressing it. When generating a compressed file, JSZip will reuse if possible the compressed content :</p> <ul> <li>If you read a zip file compressed with DEFLATE and call <code>generate</code> with the   DEFLATE compression, JSZip won't call the compression algorithms (same with   STORE everywhere.)</li> <li>If you read a zip file compressed with DEFLATE and call <code>generate</code> with the   STORE compression, JSZip will have to decompress everything.</li> </ul> <p>On IE &lt;=9, typed arrays are not supported and the compression algorithm will fallback on arrays. In that case, JSZip needs to convert the binary string into an array, DEFLATE it and convert the result into a binary string. You don't want that to happen.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/limitations.html#the-output-zip-will-differ-from-the-input-zip","title":"The output zip will differ from the input zip","text":"<p>Reading and generating a zip file won't give you back the same file. Some data are discarded (file metadata) and other are added (subfolders).</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/limitations.html#encodings-support","title":"Encodings support","text":"<p>JSZip only supports UTF-8 natively. A zip file doesn't contain the name of the encoding used, you need to know it before doing anything.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/limitations.html#file-name","title":"File name","text":"<p>If the name of a file inside the zip is encoded with UTF-8 then JSZip can detect it (Language encoding flag, Unicode Path Extra Field). If not, JSZip can't detect the encoding used and will generate Mojibake. You can use the encodeFileName option and the decodeFileName option to encode/decode using a custom encoding.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/limitations.html#file-content","title":"File content","text":"<p>The <code>async(\"string\")</code> method uses UTF-8 to decode the content. If you have a text in a different encoding, you can get the bytes array with <code>async(\"uint8array\")</code> and decode it with a lib (iconv, iconv-lite, etc) on your side. To save a text using a non-UTF-8 encoding, do the same : encode it into a Uint8Array before adding it to JSZip.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/upgrade_guide.html","title":"Upgrade Guide","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/upgrade_guide.html#from-2x-to-300","title":"From 2.x to 3.0.0","text":"<ul> <li>Deprecated objects/methods has been removed:</li> <li><code>options.base64</code> in <code>generate()</code> (the base64 type is still valid)</li> <li><code>options.base64</code>, <code>options.binary</code>, <code>options.dir</code>, <code>options.date</code>     on <code>ZipObject</code> (see the 2.3 upgrade section)</li> <li><code>JSZip.utils</code></li> <li><code>JSZip.prototype.crc32</code>, <code>JSZip.prototype.utf8encode</code>, <code>JSZip.prototype.utf8decode</code></li> <li><code>JSZip.base64</code> (you can get the content of a file directly as a base64 string)</li> <li><code>JSZip.compressions</code> has been removed.</li> <li>On <code>ZipObject</code>, the synchronous getters has been replaced by <code>async()</code> and   <code>nodeStream()</code>.</li> <li>The <code>generate()</code> method has been replaced by <code>generateAsync()</code> and    <code>generateNodeStream()</code>.</li> <li>The <code>type</code> option in <code>generate()</code> is now mandatory.</li> <li>The \"text\" type has been replaced by the \"string\" type, a binary string is   named \"binarystring\".</li> <li>The <code>load()</code> method and the constructor with data (<code>new JSZip(data)</code>) have   been replaced by <code>loadAsync()</code>.</li> <li>When adding a file, the option <code>createFolders</code> now defaults to <code>true</code>. If   you don't want to create sub folders, set it to false.</li> <li><code>zip.generateAsync()</code> and <code>zip.generateNodeStream()</code> now depend on the   current folder level.</li> </ul> <pre><code>// 2.x\nzip.file(\"test.txt\").asText();\n// 3.x\nzip.file(\"test.txt\").async(\"string\")\n.then(function (content) {\n    // use content\n});\n\n\n// 2.x\nzip.generate();\n// 3.x\nzip.generateAsync({type:\"uint8array\"})\n.then(function (content) {\n    // use content\n});\n\n// 2.x\nnew JSZip(data);\nzip.load(data);\n// zip.file(...)\n// 3.x\nJSZip.loadAsync(data).then(zip) {...};\nzip.loadAsync(data).then(zip) {...};\n// here, zip won't have (yet) the updated content\n\n// 2.x\nvar data = zip.file(\"img.jpg\").asBinary();\nvar dataURI = \"data:image/jpeg;base64,\" + JSZip.base64.encode(data);\n// 3.x\nzip.file(\"img.jpg\").async(\"base64\")\n.then(function (data64) {\n    var dataURI = \"data:image/jpeg;base64,\" + data64;\n});\n</code></pre> <p><code>async</code> and <code>loadAsync</code> use (a polyfill of) promises, you can find the documentation here and a tutorial here.</p> <p>It is worth noting that:</p> <pre><code>/*\n * JSZip accepts these promise as input\n */\n\n// replace a content with JSZip v2\nvar content = zip.file(\"my_file\").asText();\ncontent = content.replace(/apples/, 'oranges');\nzip.file(\"my_file\", content);\n\n// replace a content with JSZip v3\nvar contentPromise = zip.file(\"my_file\").async(\"text\").then(function (content) {\n    return content.replace(/apples/, 'oranges');\n});\nzip.file(\"my_file\", contentPromise);\n\n\n/*\n * Promises are chainable\n */\n\n// read, update, generate a zip file with JSZip v2\nvar zip = new JSZip(content);\nzip.file(\"new_file\", \"new_content\");\nvar blob = zip.generate({type: \"blob\"});\nsaveAs(blob, \"result.zip\");\n\n// read, update, generate a zip file with JSZip v3\nJSZip.loadAsync(content)\n.then(function (zip) {\n    zip.file(\"new_file\", \"new_content\");\n    // if you return the zip object, it will be available in the next \"then\"\n    return zip;\n.then(function (zip) {\n    // if you return a promise of a blob, promises will \"merge\": the current\n    // promise will wait for the other and the next \"then\" will get the\n    // blob\n    return zip.generateAsync({type: \"blob\"});\n.then(function (blob) {\n    saveAs(blob, \"result.zip\");\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/upgrade_guide.html#from-222-to-230","title":"From 2.2.2 to 2.3.0","text":"<ul> <li>On <code>ZipObject#options</code>, the attributes <code>date</code> and <code>dir</code> have been   deprecated and are now on <code>ZipObject</code>.</li> <li>On <code>ZipObject#options</code>, the attributes <code>base64</code> and <code>binary</code> have been   deprecated.</li> <li><code>JSZip.base64</code>, <code>JSZip.prototype.crc32</code>, <code>JSZip.prototype.utf8decode</code>,   <code>JSZip.prototype.utf8encode</code> and <code>JSZip.utils</code> have been deprecated.</li> </ul> <pre><code>// deprecated\nzip.file(\"test.txt\").options.date\nzip.file(\"test.txt\").options.dir\n// new API\nzip.file(\"test.txt\").date\nzip.file(\"test.txt\").dir\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/upgrade_guide.html#from-200-to-210","title":"From 2.0.0 to 2.1.0","text":"<ul> <li>The packaging changed : instead of loading jszip.js, jszip-load.js,   jszip-inflate.js, jszip-deflate.js, just include dist/jszip.js or   dist/jszip.min.js.   For AMD loader users : JSZip now registers itself. You just have to put the   file at the right place or configure your loader.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/upgrade_guide.html#from-1x-to-2x","title":"From 1.x to 2.x","text":"<ul> <li><code>JSZipBase64</code> has been renamed to <code>JSZip.base64</code>.</li> <li>The <code>data</code> attribute doesn't exist anymore :   use the getters <code>asText()</code>, <code>asBinary()</code>, etc</li> <li>The compression/decompression methods now give their input type with the   <code>compressInputType</code> and <code>uncompressInputType</code> attributes.</li> </ul> <p>Example for the data attribute :</p> <pre><code>// before\nzip.file(\"test.txt\").data;\nzip.files[\"test.txt\"].data;\nzip.file(\"image.png\").data;\nzip.files[\"image.png\"].data;\n\n// after\nzip.file(\"test.txt\").asText();\nzip.files[\"test.txt\"].asText();\nzip.file(\"image.png\").asBinary();\nzip.files[\"image.png\"].asBinary();\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/constructor.html","title":"new JSZip() or JSZip()","text":"<p>Create a new JSZip instance.</p> <p>Returns : A new JSZip.</p> <p>Since: v1.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/constructor.html#example","title":"Example","text":"<pre><code>var zip = new JSZip();\n// same as\nvar zip = JSZip();\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/external.html","title":"JSZip.external","text":"<p>JSZip uses objects that may not exist on every platform, in which case it uses a shim. Accessing or replacing these objects can sometimes be useful. JSZip.external contains the following properties :</p> <ul> <li><code>Promise</code> : the Promise implementation used.</li> </ul> <p>The global object is preferred when available.</p> <p>Example</p> <pre><code>// use bluebird instead\nJSZip.external.Promise = Bluebird;\n\n// use the native Promise object:\nJSZip.external.Promise = Promise;\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html","title":"file(name, data [,options])","text":"<p>Add (or update) a file to the zip file. If something goes wrong (the data is not in a supported format for example), an exception will be propagated when accessing the data.</p> <p>Returns : The current JSZip object, for chaining.</p> <p>Since: v1.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#arguments","title":"Arguments","text":"name type description name string the name of the file. You can specify folders in the name : the folder separator is a forward slash (\"/\"). data String/ArrayBuffer/Uint8Array/Buffer/Blob/Promise/Nodejs stream the content of the file. options object the options. <p>Content of <code>options</code> :</p> name type default description base64 boolean <code>false</code> set to <code>true</code> if the data is base64 encoded. For example image data from a <code>&lt;canvas&gt;</code> element. Plain text and HTML do not need this option. More. binary boolean <code>false</code> set to <code>true</code> if the data should be treated as raw content, <code>false</code> if this is a text. If base64 is used, this defaults to <code>true</code>, if the data is not a string, this will be set to <code>true</code>. More. date date the current date the last modification date. More. compression string null If set, specifies compression method to use for this specific file. If not, the default file compression will be used, see generateAsync(options). More. compressionOptions object <code>null</code> the options to use when compressing the file, see generateAsync(options). More. comment string null The comment for this file. More. optimizedBinaryString boolean <code>false</code> Set to true if (and only if) the input is a \"binary string\" and has already been prepared with a 0xFF mask. createFolders boolean <code>true</code> Set to true if folders in the file path should be automatically created, otherwise there will only be virtual folders that represent the path to the file. More. unixPermissions 16 bits number null The UNIX permissions of the file, if any. More. dosPermissions 6 bits number null The DOS permissions of the file, if any. More. dir boolean false Set to true if this is a directory and content should be ignored. More."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#data-input","title":"data input","text":"<p>You shouldn't update the data given to this method: it is kept as it so any update will impact the stored data.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#about-promise-since-v300","title":"About Promise since v3.0.0","text":"<p>You can use a Promise of content directly to simplify async content handling. Let's use HTTP calls as examples:</p> <pre><code>/** with promises **/\n\n// instead of\n$.get(\"url/to.file.txt\") // jQuery v3 returns promises\n.then(function (content) {\n    zip.file(\"file.txt\", content);\n})\n\n// you can do\nvar promise = $.get(\"url/to.file.txt\");\nzip.file(\"file.txt\", promise);\n</code></pre> <pre><code>/** with callbacks **/\n\n// instead of\nrequest('url/to.file.txt', function (error, response, body) {\n    zip.file(\"file.txt\", body);\n});\n\n// you can do\nvar promise = new Promise(function (resolve, reject) {\n    request('url/to.file.txt', function (error, response, body) {\n        if (error) {\n            reject(error);\n        } else {\n          resolve(body);\n        }\n    });\n});\nzip.file(\"file.txt\", promise);\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#about-blob-since-v300","title":"About Blob since v3.0.0","text":"<p>You can use directly Blob as input, no need to use a <code>FileReader</code>. File objects are Blobs so you can use them directly:</p> <pre><code>// in a change callback of a &lt;input type=\"file\"&gt;\nvar files = evt.target.files;\nfor (var i = 0; i &lt; files.length; i++) {\n    var f = files[i];\n    zip.file(f.name, f);\n}\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#about-nodejs-stream-since-v300","title":"About nodejs stream since v3.0.0","text":"<p>A stream can't be restarted: if it is used once, it can't be used again ( by generateAsync() or by ZipObject methods). In that case, the promise/stream (depending on the method called) will get an error.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#base64-option","title":"<code>base64</code> option","text":"<pre><code>var zip = new JSZip();\nzip.file(\"hello.txt\", \"aGVsbG8gd29ybGQK\", {base64: true});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#binary-option","title":"<code>binary</code> option","text":"<pre><code>var zip = new JSZip();\n\n// here, we have a correct (unicode) string\nzip.file(\"hello.txt\", \"unicode \u2665\", {binary: false});\n\n// here, we have a binary string: it can contain binary content, one byte\n// per character.\nzip.file(\"hello.txt\", \"unicode \\xE2\\x99\\xA5\", {binary: true});\n</code></pre> <p>If you use a library that returns a binary string for example, you should use this option. Otherwise, you will get a corrupted result: JSZip will try to encode this string with UTF-8 when the content doesn't need to.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#date-option","title":"<code>date</code> option","text":"<pre><code>zip.file(\"Xmas.txt\", \"Ho ho ho !\", {\n    date: new Date(\"December 25, 2007 00:00:01\")\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#compression-and-compressionoptions-options","title":"<code>compression</code> and <code>compressionOptions</code> options","text":"<p>See also the same options on <code>JSZip#generateAsync()</code>.</p> <p>These options will be used when generating a zip file. They let you override entry per entry the compression / compression options to use.</p> <pre><code>zip.file(\"a.png\", contentOfA, {\n    compression: \"STORE\" // force a compression for this file\n});\nzip.file(\"b.txt\", contentOfA, {\n    compression: \"DEFLATE\",\n    compressionOptions: {\n        level: 9 // force a compression and a compression level for this file\n    }\n});\n\n// don't force anything, use the generateAsync options\nzip.file(\"c.txt\", contentOfB);\n\n// here:\n// - a.png will not be compressed (STORE)\n// - b.txt will be compressed at maximum level\n// - c.txt will be compressed with the default compression level\nzip.generateAsync({\n    type: \"blob\",\n    compression: \"DEFLATE\"\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#comment-option","title":"<code>comment</code> option","text":"<pre><code>zip.file(\"a.txt\", \"content\", {\n    comment: \"comment of a.txt\"\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#createfolders-option","title":"<code>createFolders</code> option","text":"<pre><code>zip.file(\"a/b/c/d.txt\", \"content\", {\n    createFolders: true // default value\n});\nconsole.log(zip.files);\n// will display:\n// - a/\n// - a/b/\n// - a/b/c/\n// - a/b/c/d.txt\n\n\nzip.file(\"a/b/c/d.txt\", \"content\", {\n    createFolders: false\n});\nconsole.log(zip.files);\n// will display:\n// - a/b/c/d.txt\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#unixpermissions-and-dospermissions-options","title":"<code>unixPermissions</code> and <code>dosPermissions</code> options","text":"<p>Each permission will be used for matching platform option of generateAsync(): on <code>DOS</code>, use <code>dosPermissions</code>, on <code>UNIX</code>, use <code>unixPermissions</code>.</p> <p>On nodejs you can use the <code>mode</code> attribute of nodejs' fs.Stats.</p> <p>When not set, a default value will be generated:</p> <ul> <li><code>0100664</code> or <code>040775</code> for <code>UNIX</code></li> <li>standard file or standard directory for <code>DOS</code></li> </ul> <p>The field <code>unixPermissions</code> also accepts a string representing the octal value: \"644\", \"755\", etc.</p> <pre><code>zip.file(\"script.sh\", \"#!/bin/bash\", {\n    unixPermissions: \"755\"\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#dir-option","title":"<code>dir</code> option","text":"<p>If <code>dir</code> is true or if a permission says it's a folder, this entry be flagged as a folder and the content will be ignored.</p> <p>See also folder(name).</p> <pre><code>zip.file(\"folder/\", null, {\n    dir: true\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_data.html#other-examples","title":"Other examples","text":"<pre><code>zip.file(\"Hello.txt\", \"Hello World\\n\");\n\n// base64\nzip.file(\"smile.gif\", \"R0lGODdhBQAFAIACAAAAAP/eACwAAAAABQAFAAACCIwPkWerClIBADs=\", {base64: true});\n// from an ajax call with xhr.responseType = 'arraybuffer'\nzip.file(\"smile.gif\", arraybufferFromXhr);\n// or on nodejs\nzip.file(\"smile.gif\", fs.readFileSync(\"smile.gif\"));\n\nzip.file(\"Xmas.txt\", \"Ho ho ho !\", {date : new Date(\"December 25, 2007 00:00:01\")});\nzip.file(\"folder/file.txt\", \"file in folder\");\n\nzip.file(\"animals.txt\", \"dog,platypus\\n\").file(\"people.txt\", \"james,sebastian\\n\");\n\n// result:\n// - Hello.txt\n// - smile.gif\n// - Xmas.txt\n// - animals.txt\n// - people.txt\n// - folder/\n// - folder/file.txt\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_name.html","title":"file(name)","text":"<p>Get a file with the specified name. You can specify folders in the name : the folder separator is a forward slash (\"/\").</p> <p>Returns : An instance of ZipObject representing the file if any, <code>null</code> otherwise.</p> <p>Since: v1.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_name.html#arguments","title":"Arguments","text":"name type description name string the name of the file. <p>Throws : Nothing.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_name.html#example","title":"Example","text":"<pre><code>var zip = new JSZip();\nzip.file(\"file.txt\", \"content\");\n\nzip.file(\"file.txt\").name // \"file.txt\"\nzip.file(\"file.txt\").async(\"string\") // a promise of \"content\"\nzip.file(\"file.txt\").dir // false\n\n// utf8 example\nvar zip = new JSZip();\nzip.file(\"amount.txt\", \"\u20ac15\");\nzip.file(\"amount.txt\").async(\"string\") // a promise of \"\u20ac15\"\nzip.file(\"amount.txt\").async(\"arraybuffer\") // a promise of an ArrayBuffer containing \u20ac15 encoded as utf8\nzip.file(\"amount.txt\").async(\"uint8array\") // a promise of an Uint8Array containing \u20ac15 encoded as utf8\n\n// with folders\nzip.folder(\"sub\").file(\"file.txt\", \"content\");\nzip.file(\"sub/file.txt\"); // the file\n// or\nzip.folder(\"sub\").file(\"file.txt\") // the file\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_regex.html","title":"file(regex)","text":"<p>Search a file in the current folder and subfolders with a regular expression. The regex is tested against the relative filename.</p> <p>Returns : An array of matching files (an empty array if none matched). Each matching file is an instance of ZipObject.</p> <p>Since: v1.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_regex.html#arguments","title":"Arguments","text":"name type description regex RegExp the regex to use."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/file_regex.html#example","title":"Example","text":"<pre><code>var zip = new JSZip();\nzip.file(\"file1.txt\", \"content\");\nzip.file(\"file2.txt\", \"content\");\n\nzip.file(/file/); // array of size 2\n\n// example with a relative path :\nvar folder = zip.folder(\"sub\");\nfolder\n  .file(\"file3.txt\", \"content\")  // relative path from folder : file3.txt\n  .file(\"file4.txt\", \"content\"); // relative path from folder : file4.txt\n\nfolder.file(/file/);  // array of size 2\nfolder.file(/^file/); // array of size 2, the relative paths start with file\n\n// arrays contain objects in the form:\n// {name: \"file2.txt\", dir: false, async : function () {...}, ...}\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/filter.html","title":"filter(predicate)","text":"<p>Filter nested files/folders with the specified function.</p> <p>Returns : An array of matching ZipObject.</p> <p>Since: v1.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/filter.html#arguments","title":"Arguments","text":"name type description predicate function the predicate to use. <p>The predicate has the following signature : <code>function (relativePath, file) {...}</code> :</p> name type description relativePath string the filename and its path, relative to the current folder. file ZipObject the file being tested. See ZipObject. <p>The predicate must return true if the file should be included, false otherwise.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/filter.html#examples","title":"Examples","text":"<pre><code>var zip = new JSZip().folder(\"dir\");\nzip.file(\"readme.txt\", \"content\");\nzip.filter(function (relativePath, file){\n  // relativePath == \"readme.txt\"\n  // file = {name:\"dir/readme.txt\",options:{...},async:function}\n  return true/false;\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/folder_name.html","title":"folder(name)","text":"<p>Create a directory if it doesn't exist, return a new JSZip object with the new folder as root.</p> <p>See also the <code>dir</code> option of file().</p> <p>Returns : A new JSZip (for chaining), with the new folder as root.</p> <p>Since: v1.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/folder_name.html#arguments","title":"Arguments","text":"name type description name string the name of the directory."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/folder_name.html#examples","title":"Examples","text":"<pre><code>zip.folder(\"images\");\nzip.folder(\"css\").file(\"style.css\", \"body {background: #FF0000}\");\n// or specify an absolute path (using forward slashes)\nzip.file(\"css/font.css\", \"body {font-family: sans-serif}\")\n\n// result : images/, css/, css/style.css, css/font.css\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/folder_regex.html","title":"folder(regex)","text":"<p>Search a subdirectory in the current directory with a regular expression. The regex is tested against the relative path.</p> <p>Returns : An array of matching folders (an empty array if none matched). Each matching folder is an instance of ZipObject.</p> <p>Since: v1.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/folder_regex.html#arguments","title":"Arguments","text":"name type description regex RegExp the regex to use."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/folder_regex.html#examples","title":"Examples","text":"<pre><code>var zip = new JSZip();\nzip.folder(\"home/Pierre/videos\");\nzip.folder(\"home/Pierre/photos\");\nzip.folder(\"home/Jean/videos\");\nzip.folder(\"home/Jean/photos\");\n\nzip.folder(/videos/); // array of size 2\n\nzip.folder(\"home/Jean\").folder(/^vid/); // array of 1\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/for_each.html","title":"forEach(callback)","text":"<p>Call a callback function for each entry at this folder level.</p> <p>Returns : Nothing.</p> <p>Since: v3.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/for_each.html#arguments","title":"Arguments","text":"name type description callback function the callback to use. <p>The callback has the following signature : <code>function (relativePath, file) {...}</code> :</p> name type description relativePath string the filename and its path, relative to the current folder. file ZipObject the current file. See ZipObject."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/for_each.html#examples","title":"Examples","text":"<pre><code>var zip = new JSZip();\nzip.file(\"package.json\", \"...\");\nzip.file(\"lib/index.js\", \"...\");\nzip.file(\"test/index.html\", \"...\");\nzip.file(\"test/asserts/file.js\", \"...\");\nzip.file(\"test/asserts/generate.js\", \"...\");\n\nzip.folder(\"test\").forEach(function (relativePath, file){\n    console.log(\"iterating over\", relativePath);\n});\n\n// will display:\n// iterating over index.html\n// iterating over asserts/\n// iterating over asserts/file.js\n// iterating over asserts/generate.js\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html","title":"generateAsync(options[, onUpdate])","text":"<p>Generates the complete zip file at the current folder level.</p> <p>Returns : A Promise of the generated zip file.</p> <p>An error will be propagated if the asked <code>type</code> is not available in the browser, see JSZip.support.</p> <p>Since: v3.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#arguments","title":"Arguments","text":"name type default description options object the options to generate the zip file : options.type string The type of zip to return, see below for the other types. Required. More. options.compression string <code>STORE</code> (no compression) the default file compression method to use. More. options.compressionOptions object <code>null</code> the options to use when compressing the file. More. options.comment string The comment to use for the zip file. More. options.mimeType string <code>application/zip</code> mime-type for the generated file. More. options.platform string <code>DOS</code> The platform to use when generating the zip file. More. options.encodeFileName function encode with UTF-8 the function to encode the file name / comment. More. options.streamFiles boolean false Stream the files and create file descriptors, see below. More. onUpdate function The optional function called on each internal update with the metadata. More."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#type-option","title":"<code>type</code> option","text":"<p>Possible values for <code>type</code> :</p> <ul> <li><code>base64</code>: the result will be a string, the binary in a base64 form.</li> <li><code>binarystring</code> (or <code>string</code>, deprecated): the result will be a string in \"binary\" form, using 1 byte per char (2 bytes).</li> <li><code>array</code>: the result will be an Array of bytes (numbers between 0 and 255) containing the zip.</li> <li><code>uint8array</code>: the result will be a Uint8Array containing the zip. This requires a compatible browser.</li> <li><code>arraybuffer</code>: the result will be a ArrayBuffer containing the zip. This requires a compatible browser.</li> <li><code>blob</code>: the result will be a Blob containing the zip. This requires a compatible browser.</li> <li><code>nodebuffer</code>: the result will be a nodejs Buffer containing the zip. This requires nodejs.</li> </ul> <p>Note : when using type = \"uint8array\", \"arraybuffer\" or \"blob\", be sure to check if the browser supports it (you can use <code>JSZip.support</code>).</p> <pre><code>zip.generateAsync({type: \"uint8array\"}).then(function (u8) {\n    // ...\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#compression-and-compressionoptions-options","title":"<code>compression</code> and <code>compressionOptions</code> options","text":"<p>Available <code>compression</code> methods are <code>STORE</code> (no compression) and <code>DEFLATE</code>.</p> <p>The <code>compressionOptions</code> parameter depends on the compression type. With <code>STORE</code> (no compression), this parameter is ignored. With <code>DEFLATE</code>, you can give the compression level with <code>compressionOptions : {level:6}</code> (or any level between 1 (best speed) and 9 (best compression)).</p> <p>Note : if the entry is already compressed (coming from a compressed zip file), calling <code>generateAsync()</code> with a different compression level won't update the entry. The reason is simple : JSZip doesn't know how compressed the content was and how to match the compression level with the implementation we use.</p> <pre><code>zip.generateAsync({\n    type: \"blob\",\n    compression: \"DEFLATE\",\n    compressionOptions: {\n        level: 9\n    }\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#comment-option","title":"<code>comment</code> option","text":"<p>The zip format has no flag or field to give the encoding of this field and JSZip will use UTF-8. With non ASCII characters you might get encoding issues if the file archiver doesn't use UTF-8 (or the given encoding) to decode the comment.</p> <pre><code>zip.generateAsync({\n    type: \"blob\",\n    comment: \"The comment text for this zip file\"\n})\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#mimetype-option","title":"<code>mimeType</code> option","text":"<p>This field is used when you generate a Blob and need to change the mime type. Useful when you need to generate a file with a different extension, ie: \".ods\".</p> <p>Note, this won't change the content of the file, only the other programs may see it.</p> <pre><code>//This example will Generate a Open Document Spreadsheet, with the correct mime type\nvar zip = new JSZip();\nzip.file(\"mimetype\", \"application/vnd.oasis.opendocument.spreadsheet\");\nvar metaInf = zip.folder(\"META-INF\");\nmetaInf.file(\"manifest.xml\", \"&lt;...\");\n// ...\n\n//Generate the file\nzip.generateAsync({\n    type: \"blob\",\n    mimeType: \"application/ods\",\n    compression: \"DEFLATE\"\n}).then(function (odsFile) {\n    // odsFile.type == \"application/ods\"\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#platform-option","title":"<code>platform</code> option","text":"<p>Possible values for <code>platform</code> : <code>DOS</code> and <code>UNIX</code>. It also accepts nodejs <code>process.platform</code> values. When using <code>DOS</code>, the attribute <code>dosPermissions</code> of each file is used. When using <code>UNIX</code>, the attribute <code>unixPermissions</code> of each file is used.</p> <p>If you set the platform value on nodejs, be sure to use <code>process.platform</code>. <code>fs.stats</code> returns a non executable mode for folders on windows, if you force the platform to <code>UNIX</code> the generated zip file will have a strange behavior on UNIX platforms.</p> <pre><code>// on nodejs\nzip.file(pathname, content, {\n    date: stat.mtime,\n    unixPermissions: stat.mode\n});\n\n// ...\n\nzip.generateAsync({\n    type: 'nodebuffer',\n    platform: process.platform\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#encodefilename-option","title":"<code>encodeFileName</code> option","text":"<p>By default, JSZip uses UTF-8 to encode the file names / comments. You can use this method to force an other encoding. Note: the encoding used is not stored in a zip file, not using UTF-8 may lead to encoding issues. The function takes a string and returns a bytes array (Uint8Array or Array).</p> <p>See also <code>decodeFileName</code> on <code>JSZip#loadAsync()</code>.</p> <pre><code>// using iconv-lite for example\nvar iconv = require('iconv-lite');\n\nzip.generateAsync({\n    type: 'uint8array',\n    encodeFileName: function (string) {\n        return iconv.encode(string, 'your-encoding');\n    }\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#streamfiles-option","title":"<code>streamFiles</code> option","text":"<p>In a zip file, the size and the crc32 of the content are placed before the actual content: to write it we must process the whole file. When this option is <code>false</code> (the default) the processed file is held in memory. It takes more memory but generates a zip file which should be read by every program. When this options is <code>true</code>, we stream the file and use data descriptors at the end of the entry. This option uses less memory but some program might not support data descriptors (and won't accept the generated zip file).</p> <pre><code>zip.generateAsync({\n    type: 'uint8array',\n    streamFiles: true\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#onupdate-callback","title":"<code>onUpdate</code> callback","text":"<p>If specified, this function will be called each time a chunk is pushed to the output stream (or internally accumulated).</p> <p>The function takes a <code>metadata</code> object which contains information about the ongoing process.</p> <p>Metadata : the metadata are:</p> name type description percent number the percent of completion (a double between 0 and 100) currentFile string the name of the current file being processed, if any. <pre><code>zip.generateAsync({type:\"blob\"}, function updateCallback(metadata) {\n    console.log(\"progression: \" + metadata.percent.toFixed(2) + \" %\");\n    if(metadata.currentFile) {\n        console.log(\"current file = \" + metadata.currentFile);\n    }\n})\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_async.html#other-examples","title":"Other examples","text":"<pre><code>zip.generateAsync({type:\"blob\"})\n.then(function (content) {\n    // see FileSaver.js\n    saveAs(content, \"hello.zip\");\n});\n</code></pre> <pre><code>zip.generateAsync({type:\"base64\"})\n.then(function (content) {\n    location.href=\"data:application/zip;base64,\"+content;\n});\n</code></pre> <pre><code>zip.folder(\"folder_1\").folder(\"folder_2\").file(\"hello.txt\", \"hello\");\n// zip now contains:\n// folder_1/\n// folder_1/folder_2/\n// folder_1/folder_2/hello.txt\n\nzip.folder(\"folder_1\").generateAsync({type:\"nodebuffer\"})\n.then(function (content) {\n    // relative to folder_1/, this file only contains:\n    // folder_2/\n    // folder_2/hello.txt\n    require(\"fs\").writeFile(\"hello.zip\", content, function(err){/*...*/});\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_internal_stream.html","title":"generateInternalStream(options)","text":"<p>Generates the complete zip file with the internal stream implementation.</p> <p>Returns : a StreamHelper.</p> <p>Since: v3.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_internal_stream.html#arguments","title":"Arguments","text":"name type default description options object the options to generate the zip file, see the options of <code>generateAsync()</code> <p>Metadata : see the metadata of <code>generateAsync()</code>.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_internal_stream.html#examples","title":"Examples","text":"<pre><code>zip\n.generateInternalStream({type:\"uint8array\"})\n.accumulate()\n.then(function (data) {\n    // data contains here the complete zip file as a uint8array (the type asked in generateInternalStream)\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_node_stream.html","title":"generateNodeStream(options[, onUpdate])","text":"<p>Generates the complete zip file as a nodejs stream.</p> <p>Returns : a nodejs Streams3.</p> <p>Since: v3.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_node_stream.html#arguments","title":"Arguments","text":"name type default description options object the options to generate the zip file, see the options of <code>generateAsync()</code> onUpdate function The optional function called on each internal update with the metadata. <p>The <code>type</code> parameter has here the default value of <code>nodebuffer</code>. Only <code>nodebuffer</code> is currently supported.</p> <p>Metadata : see the metadata of <code>generateAsync()</code>.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/generate_node_stream.html#examples","title":"Examples","text":"<pre><code>zip\n.generateNodeStream({streamFiles:true})\n.pipe(fs.createWriteStream('out.zip'))\n.on('finish', function () {\n    // JSZip generates a readable stream with a \"end\" event,\n    // but is piped here in a writable stream which emits a \"finish\" event.\n    console.log(\"out.zip written.\");\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/load_async.html","title":"loadAsync(data [, options])","text":"<p>Read an existing zip and merge the data in the current JSZip object at the current folder level. This technique has some limitations, see here. If the JSZip object already contains entries, new entries will be merged. If two have the same name, the loaded one will replace the other.</p> <p>Since v3.8.0 this method will santize relative path components (i.e. <code>..</code>) in loaded filenames to avoid \"zip slip\" attacks. For example: <code>../../../example.txt</code> \u2192 <code>example.txt</code>, <code>src/images/../example.txt</code> \u2192 <code>src/example.txt</code>. The original filename is available on each zip entry as <code>unsafeOriginalName</code>.</p> <p>Returns : A Promise with the updated zip object. The promise can fail if the loaded data is not valid zip data or if it uses unsupported features (multi volume, password protected, etc).</p> <p>Since: v3.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/load_async.html#arguments","title":"Arguments","text":"name type description data String/Array of bytes/ArrayBuffer/Uint8Array/Buffer/Blob/Promise the zip file options object the options to load the zip file <p>Content of <code>options</code> :</p> name type default description options.base64 boolean false set to <code>true</code> if the data is base64 encoded, <code>false</code> for binary. More. options.checkCRC32 boolean false set to <code>true</code> if the read data should be checked against its CRC32. More. options.optimizedBinaryString boolean false set to <code>true</code> if (and only if) the input is a string and has already been prepared with a 0xFF mask. options.createFolders boolean false set to <code>true</code> to create folders in the file path automatically. Leaving it false will result in only virtual folders (i.e. folders that merely represent part of the file path) being created. More. options.decodeFileName function decode from UTF-8 the function to decode the file name / comment. More. <p>You shouldn't update the data given to this method : it is kept as it so any update will impact the stored data.</p> <p>Zip features supported by this method :</p> <ul> <li>Compression (<code>DEFLATE</code> supported)</li> <li>zip with data descriptor</li> <li>ZIP64</li> <li>UTF8 in file name, UTF8 in file content</li> </ul> <p>Zip features not (yet) supported :</p> <ul> <li>password protected zip</li> <li>multi-volume zip</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/load_async.html#base64-option","title":"<code>base64</code> option","text":"<pre><code>var zip = new JSZip();\nzip.loadAsync(\"UEsDBAoDAAAAAJxs8T...AAAAAA==\", {base64: true});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/load_async.html#checkcrc32-option","title":"<code>checkCRC32</code> option","text":"<p>The <code>checkCRC32</code> option will load every files, compute the CRC32 value and compare it against the saved value. With larger zip files, this option can have a significant performance cost.</p> <pre><code>// here, \"bin\" is a corrupted zip file\n\nzip.loadAsync(bin)\n.then(function (zip) {\n    // will be called, even if content is corrupted\n}, function (e) {\n    // won't be called\n});\n\nzip.loadAsync(bin, {\n    checkCRC32: true\n})\n.then(function (zip) {\n    // won't be called\n}, function (e) {\n    // Error: Corrupted zip : CRC32 mismatch\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/load_async.html#createfolders-option","title":"<code>createFolders</code> option","text":"<pre><code>// here, \"bin\" is zip file containing:\n// folder1/folder2/folder3/file1.txt\n\nzip.loadAsync(bin)\n.then(function (zip) {\n    console.log(zip.files);\n    // folder1/folder2/folder3/file1.txt\n});\n\n// with createFolders: true, all folders will be created\nzip.loadAsync(bin, {createFolders: true})\n.then(function (zip) {\n    console.log(zip.files);\n    // folder1/\n    // folder1/folder2/\n    // folder1/folder2/folder3/\n    // folder1/folder2/folder3/file1.txt\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/load_async.html#decodefilename-option","title":"<code>decodeFileName</code> option","text":"<p>A zip file has a flag to say if the filename and comment are encoded with UTF-8. If it's not set, JSZip has no way to know the encoding used. It usually is the default encoding of the operating system. Some extra fields can give the unicode version of the filename/comment too (in that case, we use it).</p> <p>If we can't find an UTF-8 encoded filename/comment, we use the <code>decodeFileName</code> function (which is by default an UTF-8 decode).</p> <p>The function takes the bytes array (Uint8Array or Array) and returns the decoded string.</p> <pre><code>// here, \"bin\" is a russian zip file, using the cp866 encoding for file names\n// by default, using UTF-8 leads to wrong file names:\nzip.loadAsync(bin)\n.then(function (zip) {\n    console.log(zip.files);\n    // '\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd/': ...\n    // '\ufffd\ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd\ufffd\ufffd/\ufffd\ufffd\ufffd\ufffd\ufffd \u296a\ufffd\u2ba2\ufffd\ufffd \ufffd\ufffd\ufffd\u3b25\ufffd\ufffd.txt': ...\n});\n\n// using the correct encoding solve the issue:\nvar iconv = require('iconv-lite');\nzip.loadAsync(bin, {\n  decodeFileName: function (bytes) {\n    return iconv.decode(bytes, 'cp866');\n  }\n})\n.then(function (zip) {\n    console.log(zip.files);\n    // '\u041d\u043e\u0432\u0430\u044f \u043f\u0430\u043f\u043a\u0430/': ...\n    // '\u041d\u043e\u0432\u0430\u044f \u043f\u0430\u043f\u043a\u0430/\u041d\u043e\u0432\u044b\u0439 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0439 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442.txt': ...\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/load_async.html#other-examples","title":"Other examples","text":"<pre><code>var zip = new JSZip();\nzip.loadAsync(zipDataFromXHR);\n</code></pre> <pre><code>require(\"fs\").readFile(\"hello.zip\", function (err, data) {\n  if (err) throw err;\n  var zip = new JSZip();\n  zip.loadAsync(data);\n}\n</code></pre> <p>Using sub folders :</p> <pre><code>// here, \"bin\" is zip file containing:\n// file1.txt\n// folder1/file2.txt\n\nvar zip = new JSZip();\nzip.folder(\"subfolder\").loadAsync(bin)\n.then(function (zip) {\n    // \"zip\" is still in the \"subfolder\" folder\n    console.log(zip.files);\n    // subfolder/file1.txt\n    // subfolder/folder1/file2.txt\n});\n</code></pre> <p>Using <code>loadAsync</code> multiple times:</p> <pre><code>// here, \"bin1\" is zip file containing:\n// file1.txt\n// file2.txt\n// and \"bin2\" is zip file containing:\n// file2.txt\n// file3.txt\n\nvar zip = new JSZip();\nzip.loadAsync(bin1)\n.then(function (zip) {\n    return zip.loadAsync(bin2);\n}).then(function (zip) {\n    console.log(zip.files);\n    // file1.txt, from bin1\n    // file2.txt, from bin2\n    // file3.txt, from bin2\n});\n</code></pre> <p>Reading a zip file with relative filenames:</p> <pre><code>// here, \"unsafe.zip\" is zip file containing:\n// src/images/../file.txt\n// ../../example.txt\n\nrequire(\"fs\").readFile(\"unsafe.zip\", function (err, data) {\n    if (err) throw err;\n    var zip = new JSZip();\n    zip.loadAsync(data)\n    .then(function (zip) {\n        console.log(zip.files);\n        // src/file.txt\n        // example.txt\n        console.log(zip.files[\"example.txt\"].unsafeOriginalName);\n        // \"../../example.txt\"\n    });\n}\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/load_async_object.html","title":"JSZip.loadAsync(data [, options])","text":"<p>This is a shortcut for</p> <pre><code>var zip = new JSZip();\nzip.loadAsync(data, options);\n</code></pre> <p>Please see the documentation of loadAsync.</p> <p>Examples</p> <pre><code>dataAsPromise\n.then(JSZip.loadAsync)\n.then(function(zip) {...})\n</code></pre> <p>same as:</p> <pre><code>JSZip.loadAsync(dataAsPromise)\n.then(function(zip) {...})\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/remove.html","title":"remove(name)","text":"<p>Delete a file or folder (recursively).</p> <p>Returns : The current JSZip object.</p> <p>Since: v1.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/remove.html#arguments","title":"Arguments","text":"name type description name string the name of the file/folder to delete."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/remove.html#examples","title":"Examples","text":"<pre><code>var zip = new JSZip();\nzip.file(\"Hello.txt\", \"Hello World\\n\");\nzip.file(\"temp.txt\", \"nothing\").remove(\"temp.txt\");\n// result : Hello.txt\n\nzip.folder(\"css\").file(\"style.css\", \"body {background: #FF0000}\");\nzip.remove(\"css\");\n//result : empty zip\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/support.html","title":"JSZip.support","text":"<p>If the browser supports them, JSZip can take advantage of some \"new\" features : ArrayBuffer, Blob, Uint8Array. To know if JSZip can use them, you can check the JSZip.support object. It contains the following boolean properties :</p> <ul> <li><code>arraybuffer</code> : true if JSZip can read and generate ArrayBuffer, false otherwise.</li> <li><code>uint8array</code> : true if JSZip can read and generate Uint8Array, false otherwise.</li> <li><code>blob</code> : true if JSZip can generate Blob, false otherwise.</li> <li><code>nodebuffer</code> : true if JSZip can read and generate nodejs Buffer, false otherwise.</li> <li><code>nodestream</code> : true if JSZip can read and generate nodejs stream, false otherwise.</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/version.html","title":"JSZip.version","text":"<p>The version of JSZip as a string.</p> <p>Since: v3.1.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_jszip/version.html#example","title":"Example","text":"<pre><code>JSZip.version == \"3.1.0\";\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/accumulate.html","title":"accumulate([updateCallback])","text":"<p>Read the whole stream and call a callback with the complete content.</p> <p>Returns : A Promise of the full content.</p> <p>Since: v3.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/accumulate.html#arguments","title":"Arguments","text":"name type description updateCallback function the function called every time the stream updates. This function is optional. <p>The update callback function takes 1 parameter: the metadata (see the <code>on</code> method).</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/accumulate.html#example","title":"Example","text":"<pre><code>zip\n.generateInternalStream({type:\"uint8array\"})\n.accumulate(function updateCallback(metadata) {\n    // metadata contains for example currentFile and percent, see the generateInternalStream doc.\n}).then(function (data) {\n    // data contains here the complete zip file as a uint8array (the type asked in generateInternalStream)\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/on.html","title":"on(event, callback)","text":"<p>Register a listener on an event.</p> <p>Returns : The current StreamHelper object, for chaining.</p> <p>Throws : An exception if the event is unknown.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/on.html#arguments","title":"Arguments","text":"name type description event string the name of the event. Only 3 events are supported : <code>data</code>, <code>end</code> and <code>error</code>. callback function the function called when the event occurs. See below for the arguments. <p>The callbacks are executed in with the current <code>StreamHelper</code> as <code>this</code>.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/on.html#data-callback","title":"<code>data</code> callback","text":"<p>It takes 2 parameters:</p> <ul> <li>the current chunk of data (in a format specified by the method which   generated this StreamHelper)</li> <li>the metadata (see each method to know what's inside)</li> </ul>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/on.html#end-callback","title":"<code>end</code> callback","text":"<p>It does not take any parameter.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/on.html#error-callback","title":"<code>error</code> callback","text":"<p>It takes an <code>Error</code> as parameter.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/on.html#example","title":"Example","text":"<pre><code>zip\n.generateInternalStream({type:\"uint8array\"})\n.on('data', function (data, metadata) {\n    // data is a Uint8Array because that's the type asked in generateInternalStream\n    // metadata contains for example currentFile and percent, see the generateInternalStream doc.\n})\n.on('error', function (e) {\n    // e is the error\n})\n.on('end', function () {\n    // no parameter\n})\n.resume();\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/pause.html","title":"pause()","text":"<p>Pause the stream if the stream is running. Once paused, the stream stops sending <code>data</code> events.</p> <p>Returns : The current StreamHelper object, for chaining.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/pause.html#example","title":"Example","text":"<pre><code>zip\n.generateInternalStream({type:\"uint8array\"})\n.on('data', function(chunk) {\n\n    // if we push the chunk to an other service which is overloaded, we can\n    // pause the stream as backpressure.\n    this.pause();\n\n}).resume(); // start the stream the first time\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/resume.html","title":"resume()","text":"<p>Resume the stream if the stream is paused. Once resumed, the stream starts sending <code>data</code> events again.</p> <p>Returns : The current StreamHelper object, for chaining.</p> <p>Since: v3.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_streamhelper/resume.html#example","title":"Example","text":"<pre><code>zip\n.generateInternalStream({type:\"uint8array\"})\n.on('data', function() {...})\n.resume();\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/async.html","title":"async(type[, onUpdate])","text":"<p>Return a Promise of the content in the asked type.</p> <p>Returns : A Promise of the content.</p> <p>Since: v3.0.0</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/async.html#arguments","title":"Arguments","text":"name type description type String the type of the result. More. onUpdate Function an optional function called on each internal update with the metadata. More."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/async.html#type-option","title":"<code>type</code> option","text":"<p>Possible values for <code>type</code> :</p> <ul> <li><code>base64</code> : the result will be a string, the binary in a base64 form.</li> <li><code>text</code> (or <code>string</code>): the result will be an unicode string.</li> <li><code>binarystring</code>: the result will be a string in \"binary\" form, using 1 byte per char (2 bytes).</li> <li><code>array</code>: the result will be an Array of bytes (numbers between 0 and 255).</li> <li><code>uint8array</code> : the result will be a Uint8Array. This requires a compatible browser.</li> <li><code>arraybuffer</code> : the result will be a ArrayBuffer. This requires a compatible browser.</li> <li><code>blob</code> : the result will be a Blob. This requires a compatible browser.</li> <li><code>nodebuffer</code> : the result will be a nodejs Buffer. This requires nodejs.</li> </ul> <p>Note : when using type = \"uint8array\", \"arraybuffer\" or \"blob\", be sure to check if the browser supports it (you can use <code>JSZip.support</code>).</p> <pre><code>zip.file(\"image.png\").async(\"uint8array\").then(function (u8) {\n    // ...\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/async.html#onupdate-callback","title":"<code>onUpdate</code> callback","text":"<p>If specified, this function will be called each time a chunk is pushed to the output stream (or internally accumulated).</p> <p>The function takes a <code>metadata</code> object which contains information about the ongoing process.</p> <p>Metadata : the metadata are :</p> name type description percent number the percent of completion (a double between 0 and 100) <pre><code>zip.file(\"image.png\").async(\"uint8array\", function updateCallback(metadata) {\n    console.log(\"progression: \" + metadata.percent.toFixed(2) + \" %\");\n}).then(function (u8) {\n    // ...\n})\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/async.html#other-examples","title":"Other examples","text":"<pre><code>zip\n.file(\"my_text.txt\")\n.async(\"string\")\n.then(function success(content) {\n    // use the content\n}, function error(e) {\n    // handle the error\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/internal_stream.html","title":"internalStream(type)","text":"<p>Return a StreamHelper of the content in the asked type.</p> <p>Returns : a StreamHelper of the content in the asked type.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/internal_stream.html#arguments","title":"Arguments","text":"name type description type String the type of the result: <code>string</code>, <code>binarystring</code>, <code>uint8array</code>, <code>arraybuffer</code>, <code>nodebuffer</code>."},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/internal_stream.html#example","title":"Example","text":"<pre><code>zip\n.file(\"my_text.txt\")\n.internalStream(\"string\")\n.on(\"data\", function (data) {...})\n.on(\"error\", function (e) {...})\n.on(\"end\", function () {...});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/node_stream.html","title":"nodeStream(type[, onUpdate])","text":"<p>Return a nodejs Streams3 of the content in the asked type.</p> <p>Returns : a nodejs Streams3.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/node_stream.html#arguments","title":"Arguments","text":"name type default description type String <code>nodebuffer</code> only <code>nodebuffer</code> is currently supported. onUpdate Function an optional function called on each internal update with the metadata. <p>Metadata : see the metadata of <code>async()</code>.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/api_zipobject/node_stream.html#example","title":"Example","text":"<pre><code>zip\n.file(\"my_text.txt\")\n.nodeStream()\n.pipe(fs.createWriteStream('/tmp/my_text.txt'))\n.on('finish', function () {\n    // JSZip generates a readable stream with a \"end\" event,\n    // but is piped here in a writable stream which emits a \"finish\" event.\n    console.log(\"text file written.\");\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html","title":"How to read a file","text":"<p>This page explains how to read an existing zip file or add a existing file into the zip file.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html#in-the-browser","title":"In the browser","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html#ajax-request","title":"AJAX request","text":"<p>Getting binary data with an ajax request is hard (mainly because of IE &lt;= 9). The easy way is to use JSZipUtils.getBinaryContent. With JSZipUtils.getBinaryContent, you can do the following (see the documentation for more examples) :</p> <pre><code>JSZipUtils.getBinaryContent('path/to/content.zip', function(err, data) {\n    if(err) {\n        throw err; // or handle err\n    }\n\n    JSZip.loadAsync(data).then(function () {\n        // ...\n    });\n});\n\n// or, with promises:\n\nnew JSZip.external.Promise(function (resolve, reject) {\n    JSZipUtils.getBinaryContent('path/to/content.zip', function(err, data) {\n        if (err) {\n            reject(err);\n        } else {\n            resolve(data);\n        }\n    });\n}).then(function (data) {\n    return JSZip.loadAsync(data);\n})\n.then(...)\n</code></pre> <p></p> <p>If you need to adapt an existing solution to what getBinaryContent does, here are the details. When doing a XHR request (level 1, without setting the <code>responseType</code>) the browser will try to interpret the response as a string and decode it from its charset. To avoid this on Firefox/Chrome/Opera, you need to set mime type : <code>xhr.overrideMimeType(\"text/plain; charset=x-user-defined\");</code>. On IE &lt;= 9, this is harder. The overrideMimeType trick doesn't work so we need to use vbscript and non standard attributes. On IE &gt; 9, overrideMimeType doesn't work but xhr2 does.</p> <p>With xhr 2, you can just set the responseType attribute : <code>xhr.responseType = \"arraybuffer\";</code>. With this, the browser will return an ArrayBuffer.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html#local-files","title":"Local files","text":"<p>If the browser supports the FileReader API, you can use it to read a zip file. JSZip can read ArrayBuffer, so you can use <code>FileReader.readAsArrayBuffer(Blob)</code>, see this example.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html#in-nodejs","title":"In nodejs","text":"<p>JSZip can read Buffers so you can do the following :</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html#local-file","title":"Local file","text":"<pre><code>\"use strict\";\n\nvar fs = require(\"fs\");\nvar JSZip = require(\"jszip\");\n\n// read a zip file\nfs.readFile(\"test.zip\", function(err, data) {\n    if (err) throw err;\n    JSZip.loadAsync(data).then(function (zip) {\n        // ...\n    });\n});\n// or\nnew JSZip.external.Promise(function (resolve, reject) {\n    fs.readFile(\"test.zip\", function(err, data) {\n        if (err) {\n            reject(e);\n        } else {\n            resolve(data);\n        }\n    });\n}).then(function (data) {\n    return JSZip.loadAsync(data);\n})\n.then(...)\n\n\n// read a file and add it to a zip\nfs.readFile(\"picture.png\", function(err, data) {\n    if (err) throw err;\n    var zip = new JSZip();\n    zip.file(\"picture.png\", data);\n});\n// or\nvar contentPromise = new JSZip.external.Promise(function (resolve, reject) {\n    fs.readFile(\"picture.png\", function(err, data) {\n        if (err) {\n            reject(e);\n        } else {\n            resolve(data);\n        }\n    });\n});\nzip.file(\"picture.png\", contentPromise);\n\n\n// read a file as a stream and add it to a zip\nvar stream = fs.createReadStream(\"picture.png\");\nzip.file(\"picture.png\", stream);\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html#remote-file","title":"Remote file","text":"<p>There are a lot of nodejs libraries doing http requests, from the built-in http to the npm packages. Here are two examples, one with the default http API, the other with request (but you're free to use your favorite library !). If possible, download the file as a Buffer (you will get better performances). If it's not possible, you can fallback to a binary string (the option is likely to be <code>encoding : \"binary\"</code>).</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html#with-http","title":"With http :","text":"<pre><code>\"use strict\";\n\nvar http = require(\"http\");\nvar url = require(\"url\");\nvar JSZip = require(\"jszip\");\n\nvar req = http.get(url.parse(\"http://localhost/.../file.zip\"), function (res) {\n  if (res.statusCode !== 200) {\n    console.log(res.statusCode);\n    // handle error\n    return;\n  }\n  var data = [], dataLen = 0;\n\n  // don't set the encoding, it will break everything !\n  // or, if you must, set it to null. In that case the chunk will be a string.\n\n  res.on(\"data\", function (chunk) {\n    data.push(chunk);\n    dataLen += chunk.length;\n  });\n\n  res.on(\"end\", function () {\n    var buf = Buffer.concat(data);\n\n    // here we go !\n    JSZip.loadAsync(buf).then(function (zip) {\n      return zip.file(\"content.txt\").async(\"string\");\n    }).then(function (text) {\n      console.log(text);\n    });\n  });\n});\n\nreq.on(\"error\", function(err){\n  // handle error\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/read_zip.html#with-request","title":"With request :","text":"<pre><code>\"use strict\";\n\nvar request = require('request');\nvar JSZip = require(\"jszip\");\n\nrequest({\n  method : \"GET\",\n  url : \"http://localhost/.../file.zip\",\n  encoding: null // &lt;- this one is important !\n}, function (error, response, body) {\n  if(error ||  response.statusCode !== 200) {\n    // handle error\n    return;\n  }\n  JSZip.loadAsync(body).then(function (zip) {\n    return zip.file(\"content.txt\").async(\"string\");\n  }).then(function (text) {\n    console.log(text);\n  });\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/write_zip.html","title":"How to write a file / give it to the user","text":""},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/write_zip.html#in-the-browser","title":"In the browser","text":"<p>With only javascript, this part won't work in old browsers, including IE &lt; 10. For those browsers, you can use a flash polyfill, see below.</p> <p>You can also see this example.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/write_zip.html#blob-url-filesaver","title":"Blob URL / FileSaver","text":"<p>With recent browsers, the easiest way is to use <code>saveAs</code> or a polyfill, see FileSaver.js :</p> <pre><code>zip.generateAsync({type:\"blob\"})\n.then(function (blob) {\n    saveAs(blob, \"hello.zip\");\n});\n</code></pre> <p>Under the hood, the polyfill uses the native <code>saveAs</code> from the FileSaver API (on Chrome and IE10+) or use a Blob URL (on Firefox).</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/write_zip.html#data-uri","title":"Data URI","text":"<p>For older browsers that support data URI, you can also do the following :</p> <pre><code>zip.generateAsync({type:\"base64\"}).then(function (base64) {\n    location.href=\"data:application/zip;base64,\" + base64;\n});\n</code></pre> <p>The biggest issue here is that the filenames are very awkward, Firefox generates filenames such as <code>a5sZQRsx.zip.part</code> (see bugs 367231 and 532230, and Safari isn't much better with just <code>Unknown</code>.</p> <p>Browser support and resulting filename :</p> Opera Firefox Safari Chrome Internet Explorer \"default.zip\" random alphanumeric with \".part\" extension \"Unknown\" (no extension) \"download.zip\" on OSX and Linux, just \"download\" on Windows No"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/write_zip.html#downloadify","title":"Downloadify","text":"<p>Downloadify uses a small Flash SWF to download files to a user's computer with a filename that you can choose. Doug Neiner has added the <code>dataType</code> option to allow you to pass a zip for downloading. Follow the Downloadify demo with the following changes:</p> <pre><code>zip = new JSZip();\nzip.file(\"Hello.\", \"hello.txt\");\n\nzip.generateAsync({type:\"base64\"}).then(function (base64) {\n    Downloadify.create('downloadify',{\n    ...\n    data: function(){\n        return base64;\n    },\n    ...\n    dataType: 'base64'\n    });\n});\n</code></pre>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/write_zip.html#deprecated-google-gears","title":"Deprecated google gears","text":"<p>Franz Buchinger has written a brilliant tutorial on using JSZip with Google Gears (part 2). If you want to let your Gears users download several files at once I really recommend having a look at some of his examples.</p>"},{"location":"portal/dist/jszip/jszip-3.10.1/documentation/howto/write_zip.html#in-nodejs","title":"In nodejs","text":"<p>JSZip can generate Buffers so you can do the following :</p> <pre><code>var fs = require(\"fs\");\nvar JSZip = require(\"jszip\");\n\nvar zip = new JSZip();\n// zip.file(\"file\", content);\n// ... and other manipulations\n\nzip\n.generateNodeStream({type:'nodebuffer',streamFiles:true})\n.pipe(fs.createWriteStream('out.zip'))\n.on('finish', function () {\n    // JSZip generates a readable stream with a \"end\" event,\n    // but is piped here in a writable stream which emits a \"finish\" event.\n    console.log(\"out.zip written.\");\n});\n</code></pre>"},{"location":"tutorials/microservices_intro.html","title":"Microservices Intro","text":""},{"location":"tutorials/microservices_intro.html#ai-microservices-introduction","title":"AI Microservices - Introduction","text":"<p>Building AI-powered applications at the edge has never been easier!</p> <p>Jetson AI Lab now offers a collection of pre-built containers, each functioning as a local AI microservice, designed to bring flexibility, efficiency, and scalability to your projects.</p> <p>A microservice is a small, independent, and loosely coupled software component that performs a specific function. In the Models section of Jetson AI Lab, you'll find AI inference services accessible through a standardized REST API.</p> <p>These local AI microservices are powerful building blocks that enable you to create cutting-edge edge AI applications with ease. Whether you're working on robotics, vision, or intelligent automation, you now have the tools to accelerate innovation.</p> <p>Let\u2019s build something amazing together! \ud83d\udca1\u2728</p>"},{"location":"tutorials/microservices_intro.html#catalog","title":"Catalog","text":"<p>Info</p> <p>Currently, following models and Web UIs are listed.</p> <p></p>"},{"location":"tutorials/microservices_intro.html#launch-the-microservice-server","title":"Launch the Microservice Server","text":""},{"location":"tutorials/microservices_intro.html#walk-through-video","title":"Walk-through video","text":"Your browser does not support the video tag."},{"location":"tutorials/microservices_intro.html#steps","title":"Steps","text":"<ol> <li>Go to Models section of Jetson AI Lab</li> <li>Click the model of your interest (specifically, the small green box representing different Orin modules) to open the model card</li> <li>Check the parameter, change as needed, and click on the  (\"Copy to clipboard\") icon in the code snippet under the \"Docker Run\" section</li> <li>Paste the <code>docker run</code> command in Jetson terminal and execute</li> <li> <p>Once you see a line like the following (for the case of MLC based service), the server is up and ready</p> <pre><code>INFO:     Uvicorn running on http://0.0.0.0:9000 (Press CTRL+C to quit)\n</code></pre> </li> </ol>"},{"location":"tutorials/microservices_intro.html#api-endpoints","title":"API Endpoints","text":"Method Endpoint Description <code>GET</code> <code>/v1/models</code> Get a list of models available <code>POST</code> <code>/v1/chat/completions</code> Get a response from the model using a prompt"},{"location":"tutorials/microservices_intro.html#example-curl-commands","title":"Example <code>curl</code> commands","text":""},{"location":"tutorials/microservices_intro.html#v1models","title":"<code>/v1/models</code>","text":"Step-by-step Instruction Walk-through video <ol> <li> <p>Execute the following on a Jetson terminal</p> <pre><code>curl http://0.0.0.0:9000/v1/models\n</code></pre> </li> <li> <p>Check the output. It should show something like the following.</p> <pre><code>{\n\"object\": \"list\",\n    \"data\": [\n        {\n        \"id\": \"DeepSeek-R1-Distill-Qwen-1.5B-q4f16_ft-MLC\",\n        \"created\": 1741991907,\n        \"object\": \"model\",\n        \"owned_by\": \"MLC-LLM\"\n        }\n    ]\n}\n</code></pre> </li> </ol> <p>Note</p> <p>For the <code>/v1/models</code> endpoint usage, you can reference the OpenAI doc page like this.</p> <p><code>get https://api.openai.com/v1/models</code></p> <p>Note that you need to substitute the base URL (<code>https://api.openai.com/</code> with <code>http://0.0.0.0:9000</code>), and you don't need to provide the authorization field.</p> <p>  Your browser does not support the video tag. </p>"},{"location":"tutorials/microservices_intro.html#v1chatcompletions","title":"<code>/v1/chat/completions</code>","text":"Step-by-step Instruction Walk-through video <ol> <li> <p>Execute the following on a Jetson terminal</p> <pre><code>curl http://0.0.0.0:9000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"messages\": [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n    }\n    ]\n}'\n</code></pre> Tip <p>You can find the model <code>curl</code> command in a couple of ways.</p> OpenAI DocsJetson AI Lab Model Card <p>OpenAI API Reference Docs provides examples for combinations of their API requests .</p> <p>You can use their <code>curl</code> command, replace the base URL and remove the authorization header and <code>model</code> field, but you can see it's essentially compatible.</p> <p>This is something demonstrated in the walk-through video.</p> <p>Each model card on Jetson AI Lab provides the \"Curl Request\" example, along with \"Performance Benchmark\" and \"Curl Request\" <code>docker run</code> commands.</p> <p></p> </li> <li> <p>Check the output. It should show something like the following.</p> <pre><code>{\n\"id\": \"chatcmpl-9439e77a205a4ef3bc2d050a73a6e30b\",\n\"choices\": [\n    {\n    \"finish_reason\": \"stop\",\n    \"index\": 0,\n    \"message\": {\n        \"content\": \"&lt;think&gt;\\nAlright, the user greeted me with \\\"Hello!\\\" and then added \\\"hi\\\". I should respond politely and clearly. I want to make sure they feel comfortable and open to any further conversation.\\n\\nI'll start with a friendly greeting, maybe \\\"Hello!\\\" or \\\"Hi there?\\\" to keep it consistent. Then, I'll ask how I can assist them, which is important to build trust. I should mention that I'm here to help with any questions, comments, or suggestions they might have.\\n\\nI also want to invite them to ask anything, so I'll make sure to keep the door open for future interaction. I'll keep the tone friendly and supportive, avoiding any abrupt requests.\\n\\nSo, putting it all together, I'll have a clear and concise response that's helpful and inviting.\\n&lt;/think&gt;\\n\\nHello! I'm here to help with any questions, comments, or suggestions you have. Keep asking anything you like, and I'll do my best to assist!\",\n        \"role\": \"assistant\",\n        \"name\": null,\n        \"tool_calls\": null,\n        \"tool_call_id\": null\n    },\n    \"logprobs\": null\n    }\n],\n\"created\": 1741993253,\n\"model\": null,\n\"system_fingerprint\": \"\",\n\"object\": \"chat.completion\",\n\"usage\": {\n    \"prompt_tokens\": 11,\n    \"completion_tokens\": 196,\n    \"total_tokens\": 207,\n    \"extra\": null\n}\n}\n</code></pre> </li> </ol> <p>  Your browser does not support the video tag. </p>"},{"location":"tutorials/microservices_intro.html#example-python-scripts","title":"Example Python scripts","text":""},{"location":"tutorials/microservices_intro.html#example-with-open-webui","title":"Example with Open WebUI","text":"Step-by-step Instruction Walk-through video <ol> <li>Go to Models section of Jetson AI Lab</li> <li>Go to Web UI section, and click \"Open WebUI\" card</li> <li>Check the parameter, change as needed, and click on the  (\"Copy to clipboard\") icon in the code snippet under the \"Docker Run\" section<ul> <li>Note the \"Server IP / Port\" section. The default is <code>0.0.0.0:8080</code>.</li> </ul> </li> <li> <p>Paste the <code>docker run</code> command in Jetson terminal and execute</p> <pre><code>docker run -it --rm \\\n    --name open-webui \\\n    --network=host \\\n    -e PORT=8080 \\\n    -e ENABLE_OPENAI_API=True \\\n    -e ENABLE_OLLAMA_API=False \\\n    -e OPENAI_API_BASE_URL=http://0.0.0.0:9000/v1 \\\n    -e OPENAI_API_KEY=foo \\\n    -e AUDIO_STT_ENGINE=openai \\\n    -e AUDIO_TTS_ENGINE=openai \\\n    -e AUDIO_STT_OPENAI_API_BASE_URL=http://0.0.0.0:8990/v1 \\\n    -e AUDIO_TTS_OPENAI_API_BASE_URL=http://0.0.0.0:8995/v1 \\\n    -v /mnt/nvme/cache/open-webui:/app/backend/data \\\n    -e DOCKER_PULL=always --pull always \\\n    -e HF_HUB_CACHE=/root/.cache/huggingface \\\n    -v /mnt/nvme/cache:/root/.cache \\\n    ghcr.io/open-webui/open-webui:main\n</code></pre> </li> <li> <p>Once you see a line like the following, the Open WebUI server should be ready</p> <pre><code>INFO:     Started server process [1]\nINFO:     Waiting for application startup.\n</code></pre> </li> <li> <p>On a web browser on a PC (that is on the same network as Jetson), access  <code>http://&lt;JETSON_IP&gt;:8080/</code></p> </li> <li>Sign in (if you have not, create an account first).<ul> <li>For the detail, read this note from our Open WebUI tutorial.</li> </ul> </li> <li>Check the selected model</li> <li>Type your query in the chat box and check the response.</li> </ol> <p>Tip</p> <p>You can check out the walk-through video (in the next \"Walk-through video\" tab) for details.</p> <p>  Your browser does not support the video tag. </p>"},{"location":"tutorials/microservices_vlm.html","title":"Microservices for VLM","text":""},{"location":"tutorials/microservices_vlm.html#local-vlm-microservices","title":"Local VLM Microservices","text":"<p>Local VLM (Vision Language Model) microservices can unlock a new class of applications for VLMs at the edge, as it can enable capabilities and performance that cloud-hosted VLMs cannot achieve, especially when integrated with local live video sources.</p> <p>In this module, we</p> <ol> <li>first look at the example of local VML microservices and its applications, and then</li> <li>experiment with the \"Augmented\" VLM Microservice that enables the efficient local video source ingestion and explore the applications.</li> </ol>"},{"location":"tutorials/microservices_vlm.html#1-local-vlm-microservices","title":"1. Local VLM Microservices","text":""},{"location":"tutorials/microservices_vlm.html#gemma-3-4b-on-ollama","title":"Gemma-3-4b on Ollama","text":"<p>Google launched Gemma 3 in March 2025, introducing multimodal support for models with over 4 billion parameters, enabling vision input alongside text processing.</p> <p>Let's run the 4B model using Ollama.</p> Ollama Docker ContainerOllama natively installed <pre><code>docker run -it --rm \\\n--gpus all --network=host \\\n-e OLLAMA_HOST=\"0.0.0.0:9000\" \\\n-e OLLAMA_MODEL=gemma3:4b \\\n-e OLLAMA_CONTEXT_LENGTH=4096 \\\n-e OLLAMA_LOGS=/root/.ollama/ollama.log \\\n-v /mnt/nvme/cache:/root/.cache \\\n-v /mnt/nvme/cache/ollama:/root/.ollama \\\ndustynv/ollama:main-r36.4.0\n</code></pre> <p>Note</p> <p>This assumes you have installed <code>ollama</code> natively on your Jetson, as shown in this tutorial section.</p> <pre><code>ollama run gemma3:4b\n</code></pre>"},{"location":"tutorials/microservices_vlm.html#llm-text-only","title":"LLM - Text-only","text":""},{"location":"tutorials/microservices_vlm.html#curl-request-with-text-only","title":"Curl Request with Text-only","text":"<p>You can reuse the curl command we used in the earlier tutorial.</p> <pre><code>curl http://0.0.0.0:9000/v1/chat/completions \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"messages\": [\n    {\n        \"role\": \"system\",\n        \"content\": \"You are a helpful assistant.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"Hello!\"\n    }\n    ]\n}'\n</code></pre> <p>You will see a response something like this.</p> <pre><code>{\n\"id\": \"chatcmpl-44f2b0d94e0e444e9fcdacfbe38728fc\",\n\"object\": \"chat.completion\",\n\"created\": 1742065798,\n\"model\": \"google/gemma-3-4b-it\",\n\"choices\": [\n    {\n    \"index\": 0,\n    \"message\": {\n        \"role\": \"assistant\",\n        \"reasoning_content\": null,\n        \"content\": \"Hello there! How can I help you today? \ud83d\ude0a \\n\\nDo you need me to:\\n\\n*   Answer a question?\\n*   Brainstorm ideas?\\n*   Write something?\\n*   Just chat?\\n\\nLet me know what's on your mind!\",\n        \"tool_calls\": []\n    },\n    \"logprobs\": null,\n    \"finish_reason\": \"stop\",\n    \"stop_reason\": 106\n    }\n],\n\"usage\": {\n    \"prompt_tokens\": 18,\n    \"total_tokens\": 75,\n    \"completion_tokens\": 57,\n    \"prompt_tokens_details\": null\n},\n\"prompt_logprobs\": null\n}\n</code></pre>"},{"location":"tutorials/microservices_vlm.html#vlm-with-image","title":"VLM - With Image","text":""},{"location":"tutorials/microservices_vlm.html#the-vision-api","title":"The \"Vision API\"","text":"<p>OpenAI API Doc shows how to use the vision capabilities through their API. This works with Ollama as well.</p> <p>You make the request to the same <code>/v1/chat/completions</code> endpoint, and provide extra image information, either by putting a URL for an image file or by providing an image as a Base64-encoded data.</p> <p></p>"},{"location":"tutorials/microservices_vlm.html#curl-request-with-image","title":"Curl Request with Image","text":"<p>You can test the <code>curl</code> commands in both styles against Ollama server.</p> <p>You will see the response that shows that it processed the passed or encoded image.</p> Passing a URLPassing a Base64 encoded image <pre><code>curl --location --request POST http://0.0.0.0:9000/v1/chat/completions \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"messages\": [\n    {\n        \"role\": \"user\",\n        \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"What is in this image?\"\n        },\n        {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n            }\n        }\n        ]\n    }\n    ],\n    \"max_tokens\": 300\n}'\n</code></pre> Example response <pre><code>{\n\"id\": \"chatcmpl-7055ee78ebd64a7895bdf9a5e22e495e\",\n\"object\": \"chat.completion\",\n\"created\": 1742067887,\n\"model\": \"google/gemma-3-4b-it\",\n\"choices\": [\n    {\n    \"index\": 0,\n    \"message\": {\n        \"role\": \"assistant\",\n        \"reasoning_content\": null,\n        \"content\": \"Here's a breakdown of what's in the image:\\n\\n*   **Wooden boardwalk:** A long, wooden walkway stretches into the image, leading into the grassy field.\\n*   **Tall grass and reeds:** The foreground is dominated by very tall, green grass and reeds, creating a lush, natural environment.\\n*   **Field/Wetland:** The boardwalk leads into a wide field, which appears to be a wetland or marsh area.\\n*   **Trees and shrubs:** A line of trees and shrubs forms the distant horizon.\\n*   **Sky:** A vibrant blue sky filled with fluffy white clouds.\\n\\nOverall, it\u2019s a beautiful landscape shot of a serene wetland area, likely a nature preserve or park, with a boardwalk providing access to the area.\",\n        \"tool_calls\": []\n    },\n    \"logprobs\": null,\n    \"finish_reason\": \"stop\",\n    \"stop_reason\": 106\n    }\n],\n\"usage\": {\n    \"prompt_tokens\": 275,\n    \"total_tokens\": 435,\n    \"completion_tokens\": 160,\n    \"prompt_tokens_details\": null\n},\n\"prompt_logprobs\": null\n}\n</code></pre> <pre><code>curl --location --request POST 'http://0.0.0.0:9000/v1/chat/completions' \\\n--header 'Content-Type: application/json' \\\n--data '{\n    \"messages\": [\n    {\n        \"role\": \"user\",\n        \"content\": [\n        {\n            \"type\": \"text\",\n            \"text\": \"Describe this icon.\"\n        },\n        {\n            \"type\": \"image_url\",\n            \"image_url\": {\n                \"url\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAAXNSR0IArs4c6QAAAYNJREFUOE9j3CEjU/efkbGakYGBjYEE8J+B4Rfj//+tjNtlZX+SqhlmD9iQHbKy/2EC3CoqDNxKSnB3fL5xg4GZi4uBS04OLvbxwgWGn69ewfkoBvBqazMYzJjBwCUvD1ZwKjQUrFmntxfM/3DuHMPF7GyGH0+fYjcAJGq+YQODgJERw59Pnxj2GRgwsAkKMtifOcPAyMjI8Gz9eobL+fkoIYXiAmQDXm7bxnAhIwOs2HLrVgY+XV3SDLhSWsrwdOVKsAEqJSUMynl5pBnw9vhxsDdAgF1cnEHAwIB4A5jY2BiOe3khAoqZmcHx3DmG1wcOEBcG744eZbjd3Y0SWHpTpjD8//uXOANutrQwfDhzBsUAqeBgBmE7O8IGGC1cyHA+KQlsGzJgExFhUK+pYbhcUIA7GkWdnBikwsIYHi1YwPD+xAkUhYLm5gzyiYkMT1evZni9dy/2hCRsY8PAwsfH8PPlS4YPZ8+iGMBvZMTAISHB8OfzZ4a3hw8jDKA8M1GYnQE8m7INTv0HFQAAAABJRU5ErkJggg==\"\n            }\n        }\n        ]\n    }\n    ],\n    \"max_tokens\": 128\n}'\n</code></pre> Example response <pre><code>{\n\"id\": \"chatcmpl-76126099d5ea4560b32ddc74a307a210\",\n\"object\": \"chat.completion\",\n\"created\": 1742068087,\n\"model\": \"google/gemma-3-4b-it\",\n\"choices\": [\n    {\n    \"index\": 0,\n    \"message\": {\n        \"role\": \"assistant\",\n        \"reasoning_content\": null,\n        \"content\": \"Okay, here\u2019s a description of the icon:\\n\\nIt\u2019s a stylized white \\\"H\\\" on a solid red background. The \\\"H\\\" appears to have a slight glow or halo effect around it, and it\u2019s somewhat blurred or hazy, giving it a soft, diffused look. \\n\\nBased on this, it\u2019s most likely the icon for **HBO** (Home Box Office), the television network.\",\n        \"tool_calls\": []\n    },\n    \"logprobs\": null,\n    \"finish_reason\": \"stop\",\n    \"stop_reason\": 106\n    }\n],\n\"usage\": {\n    \"prompt_tokens\": 281,\n    \"total_tokens\": 368,\n    \"completion_tokens\": 87,\n    \"prompt_tokens_details\": null\n},\n\"prompt_logprobs\": null\n}\n</code></pre>"},{"location":"tutorials/microservices_vlm.html#python-examples","title":"Python Examples","text":"<p>As demonstrated on the OpenAI Doc page, you can do the same with Python scripts.</p> <p>Note</p> <p>Make sure you have installed <code>openai</code> Python library.</p> <pre><code>pip install openai\n</code></pre> Passing a URLPassing a Base64 encoded image <pre><code>from openai import OpenAI\nclient = OpenAI(\n    base_url = os.environ.get('OPENAI_BASE_URL', 'http://0.0.0.0:9000/v1'),\n    api_key = 'foo',\n)\n\nresponse = client.chat.completions.create(\n    model=\"google/gemma-3-4b-it\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n                },\n            },\n        ],\n    }],\n)\n\nprint(response.choices[0].message.content)\n</code></pre> <pre><code>import os, requests, base64\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url = os.environ.get('OPENAI_BASE_URL', 'http://0.0.0.0:9000/v1'),\n    api_key = 'foo',\n)\n\n# Function to encode the image\ndef encode_image(url, toBase64=True):\n    \"\"\" Download + convert to base64, or pass through as image_url \"\"\"\n    if not toBase64:\n        return url\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise ValueError(f\"Failed to fetch image: {url} (HTTP {response.status_code})\")\n\n    content_type = response.headers.get('Content-Type', 'image/jpeg')  # Default to JPEG\n    return f\"data:{content_type};base64,{base64.b64encode(response.content).decode()}\"\n\n# Path to your image\nimage_base_url = 'https://raw.githubusercontent.com/dusty-nv/jetson-containers/refs/heads/dev/data/images/hoover.jpg'\n\n# Getting the Base64 string\nbase64_image = encode_image(image_base_url)\n\ncompletion = client.chat.completions.create(\n    model=\"google/gemma-3-4b-it\",\n    messages=[{\n        \"role\": \"user\",\n        \"content\": [\n            { \"type\": \"text\", \"text\": \"what's in this image?\" },\n            {\n                \"type\": \"image_url\",\n                \"image_url\": {\n                    \"url\": base64_image,\n                },\n            },\n        ],\n    }],\n)\n\nprint(completion.choices[0].message.content)\n</code></pre>"},{"location":"tutorials/microservices_vlm.html#vlm-usage-in-frameworks","title":"VLM usage in frameworks","text":""},{"location":"tutorials/microservices_vlm.html#open-webui","title":"Open WebUI","text":"<p>Let's take a look at how this VLM capability can be used in Open WebUI</p> <ol> <li> <p>As shown in the earlier tutorial, launch the Open WebUI server.</p> <code>docker run</code> command to start Open WebUI server <pre><code>docker run -it --rm \\\n    --name open-webui \\\n    --network=host \\\n    -e PORT=8080 \\\n    -e ENABLE_OPENAI_API=True \\\n    -e ENABLE_OLLAMA_API=False \\\n    -e OPENAI_API_BASE_URL=http://0.0.0.0:9000/v1 \\\n    -e OPENAI_API_KEY=foo \\\n    -e AUDIO_STT_ENGINE=openai \\\n    -e AUDIO_TTS_ENGINE=openai \\\n    -e AUDIO_STT_OPENAI_API_BASE_URL=http://0.0.0.0:8990/v1 \\\n    -e AUDIO_TTS_OPENAI_API_BASE_URL=http://0.0.0.0:8995/v1 \\\n    -v /mnt/nvme/cache/open-webui:/app/backend/data \\\n    -e DOCKER_PULL=always --pull always \\\n    -e HF_HUB_CACHE=/root/.cache/huggingface \\\n    -v /mnt/nvme/cache:/root/.cache \\\n    ghcr.io/open-webui/open-webui:main\n</code></pre> </li> <li> <p>On a web browser on a PC (that is on the same network as Jetson), access  <code>http://&lt;JETSON_IP&gt;:8080/</code></p> </li> <li> <p>Go through the sign-in process</p> </li> <li> <p>In the chat box, click on (plus) icon and select \" Upload Files\", and attach a local image.</p> </li> <li> <p>Supply a text query (e.g. \"Describe this image.\") and hit \"(Send message)\" button.</p> <p></p> </li> <li> <p>Check the response.</p> <p></p> </li> <li> <p>Check the log on terminal running Ollama</p> <p>You may find information like following.</p> <pre><code>INFO 03-15 20:57:25 [logger.py:39] Received request chatcmpl-ffb41435ad4a4bb990b48ecc341850a9: prompt: '&lt;bos&gt;&lt;start_of_turn&gt;user\\n### Task:\\nGenerate 1-3 broad tags categorizing the main themes of the chat history, along with 1-3 more specific subtopic tags.\\n\\n### Guidelines:\\n- Start with high-level domains (e.g. Science, Technology, Philosophy, Arts, Politics, Business, Health, Sports, Entertainment, Education)\\n- Consider including relevant subfields/subdomains if they are strongly represented throughout the conversation\\n- If content is too short (less than 3 messages) or too diverse, use only [\"General\"]\\n- Use the chat\\'s primary language; default to English if multilingual\\n- Prioritize accuracy over specificity\\n\\n### Output:\\nJSON format: { \"tags\": [\"tag1\", \"tag2\", \"tag3\"] }\\n\\n### Chat History:\\n&lt;chat_history&gt;\\nUSER: Describe the photo.\\nASSISTANT: Here\\'s a description of the photo:\\n\\nThe photo showcases a large, full moon dominating the frame. It is a vibrant, creamy white color, with prominent and detailed dark grey and brown craters, maria (smooth, dark plains formed by ancient lava flows), and highlands visible across its surface. \\n\\nThe background is a solid, dark black, which makes the moon stand out dramatically and emphasizes its round shape.  It appears to be a close-up shot, giving a clear and detailed view of the lunar surface.\\n&lt;/chat_history&gt;&lt;end_of_turn&gt;\\n&lt;start_of_turn&gt;model\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=1.0, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=7890, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, lora_request: None, prompt_adapter_request: None.\nINFO 03-15 20:57:25 [engine.py:289] Added request chatcmpl-ffb41435ad4a4bb990b48ecc341850a9.\nINFO:     172.17.0.1:42452 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\nINFO 03-15 20:57:36 [metrics.py:481] Avg prompt throughput: 48.7 tokens/s, Avg generation throughput: 4.1 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\nINFO 03-15 20:57:46 [metrics.py:481] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\n</code></pre> </li> </ol>"},{"location":"tutorials/microservices_vlm.html#n8n","title":"n8n","text":"<p>n8n is a powerful open-source workflow automation tool that enables seamless integration between various applications, APIs, and services -- making it an excellent choice for orchestrating LLMs and VLMs, edge AI deployments, and local AI-powered workflows.</p> <ol> <li> <p>Start n8n server container</p>  Step-by-step Instruction Walk-through video <ol> <li>Go to Models section of Jetson AI Lab</li> <li>Go to Web UI section, and click \"n8n\" card</li> <li>Check the parameter, change as needed, and click on the  (\"Copy to clipboard\") icon in the code snippet under the \"Docker Run\" section<ul> <li>Note the \"Server IP / Port\" section. The default is <code>0.0.0.0:5678</code>.</li> </ul> </li> <li> <p>Paste the <code>docker run</code> command in Jetson terminal and execute</p> <pre><code>docker run -it --rm --name=n8n \\\n    --network=host \\\n    -e N8N_LISTEN_ADDRESS=0.0.0.0 \\\n    -e N8N_PORT=5678 \\\n    -e N8N_SECURE_COOKIE=false \\\n    -v /mnt/nvme/cache/n8n:/root/node/.n8n \\\n    -e DOCKER_PULL=always --pull always \\\n    -e HF_HUB_CACHE=/root/.cache/huggingface \\\n    -v /mnt/nvme/cache:/root/.cache \\\n    n8nio/n8n:stable\n</code></pre> </li> <li> <p>On a web browser on a PC (that is on the same network as Jetson), access <code>http://&lt;JETSON_IP&gt;:5678/</code></p> </li> </ol> <p>  Your browser does not support the video tag. </p> </li> <li> <p>First create a \"Credential\" to configure the connection to the local Ollama server.</p>  Step-by-step Instruction Walk-through video <ol> <li>On the n8n top screen, click on \"\" button on the left, and select \"Credential\".</li> <li>In the \"Add new credential\" prompt box, in the \"Search for app...\" field, type \"openai\" and select \"OpenAi\". Hit \"Continue\" button.</li> <li> <p>In the \"OpenAi account\" prompt box, put the followings;</p> Field Value API Key <code>foo</code> (or whatever random) Organization ID <code></code> (leave it blank) Base URL <code>http://0.0.0.0:9000/v1</code> </li> <li> <p>Once done, hit \"Save\" button. Check you get \" Connection tested successfully\" message.</p> <p></p> </li> <li> <p>Put your mouse cursor over the text \"OpenAi account\" at the top of the prompt box</p> </li> <li>\" (Edit)\" icon shows up, so click on it to edit, and name is something like \"Local Ollama\".</li> <li> <p>Click  icon on the top left corner of the prompt box to close</p> <p></p> </li> </ol> <p>  Your browser does not support the video tag. </p> </li> <li> <p>Then create a \"Workflow\" to use the \"OpenAI Analyze Image\" node to use gemma-3-4b model hosted on Ollama</p>  Step-by-step Instruction Walk-through video <ol> <li>On the n8n top screen, click on \"\" button on the left, and select \"Workflow\".</li> <li>Click on \"Add first step...\" button in the center</li> <li>In the right side pane that shows up, in the field \"Search for app...\" field, type \"openai\" and select \"OpenAI\", the first item.</li> <li>Under the \"Actions (15)\" section, under \"IMAGE ACTION\" category, find and select \"Analyze Image\"</li> <li> <p>In the \"OpenAi account\" prompt box, under the \"Parameters\" tab, put the followings;</p> Field Value Note Credential to connect with \"Local Ollama\" (or whatever credential you created and named) Resource Image (default) Operation Analyze Image (default) Model Specify Method By ID Model <code>google/gemma-3-4b-it</code> (Ollama cares this field, so you need to supply the correct name of the model) Text Input \"What's in this image\" (default) Input Type Image URL(s) (default) URL https://raw.githubusercontent.com/dusty-nv/jetson-containers/refs/heads/dev/data/images/hoover.jpg </li> <li> <p>Click \"Test step\" button the top right of the prompt box, and check the output shown on the right.</p> <p></p> </li> </ol> <p>  Your browser does not support the video tag. </p> </li> </ol>"},{"location":"tutorials/microservices_vlm.html#2-augmented-vlm-microservice","title":"2. Augmented VLM Microservice","text":""},{"location":"tutorials/microservices_vlm.html#challenges-with-local-video-source","title":"Challenges with Local Video Source","text":"<p>A VLM is powerful, but processing a continuous local video stream with it can be complex. Traditional methods require the client to extract frames from the local video source/device, encode them, and send the data over API requests -- adding unnecessary overhead and latency.</p>"},{"location":"tutorials/microservices_vlm.html#solution","title":"Solution","text":"<p>An \"Augmented VLM Microservice\" eliminates this complexity by internalizing video source management.  Instead of handling frame extraction and encoding, the client can simply point to the video source, letting the microservice handle everything internally and efficiently. Additionally, it can provide optional output streams and useful metrics, making integration with applications seamless and efficient.</p> <p>This approach reduces client-side complexity, enhances real-time processing, and improves overall performance for edge AI applications. \ud83d\ude80</p> CloudLocal MicroserviceAugmented MicroserviceAugmented Microservice (Detailed) <p></p> <p>Your client application is responsible for managing the video source, extracting the frame. The API request includes the Base64-encoded image, adding overhead and latency.</p> <p> When VLM moves to local...  </p> <p></p> <p>Even when you have a local VLM Microservice, your client application is still responsible for managing the video source, extracting the frame.</p> <p>  Back The solution is...  </p> <p></p> <p>The Augmented VLM Microservice internalizes the video source management and frame processing, allowing your client application to only point to the video source and keeping itself simple.</p> <p>  Back Details of the Augmented Microservice  </p> <p></p> <p>The Augmented VLM Microservice can optionally produces output/preview video stream and other useful metrics.</p> <p>  Back </p>"},{"location":"tutorials/microservices_vlm.html#example-vila-microservice","title":"Example - VILA Microservice","text":"<p>Let's take a look at an example augmented VLM microservice, VILA Microservice, which is based on VILA 1.5 - 3B.</p>  Step-by-step Instruction Walk-through video <p>Note</p> <p>This section assumes you have installed <code>jetson-containers</code>.</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash ./jetson-containers/install.sh\n</code></pre> <ol> <li> <p>Prepare a video source</p> USB webcamRTSP video stream <p>Before starting the VILA Microservice container using <code>jetson-containers</code>, first hook up a USB webcam to your Jetson. Make sure it shows up on <code>ls /dev/video*</code>.</p> <p>Follow this instruction to install <code>nvstream</code>, start its container, and upload an MP4 video file to start streaming over RTSP.</p> </li> <li> <p>Start VILA Microservice container</p> <pre><code>jetson-containers run dustynv/vila-microservice:r36.4.0\n</code></pre> </li> <li> <p>Start n8n server</p> <p>Follow the instruction above.</p> </li> <li> <p>Make sure you have the credential for <code>http://0.0.0.0:9000/v1</code></p> <p>You can keep using the credential you made of Ollama as we kept the base URL (and the API scheme) same. You can also create a dedicated credential for VILA Microservice (in case you customize the port number, etc).</p> </li> <li> <p>Create a \"Workflow\" like before, but be sure to specify your video source in URL field.</p> USB webcamRTSP video stream Field Value Note Credential to connect with \"Local Ollama\" Or the one you made for VILA Microservice Resource Image (default) Operation Analyze Image (default) Model Specify Method <code>By ID</code> Model <code>Efficient-Large-Model/VILA1.5-3b</code> VILA Microservice does not check this Text Input \"What's in this image\" (default) Input Type Image URL(s) (default) URL <code>v4l2:///dev/video0</code> Field Value Note Credential to connect with \"Local Ollama\" Or the one you made for VILA Microservice Resource Image (default) Operation Analyze Image (default) Model Specify Method <code>By ID</code> Model <code>Efficient-Large-Model/VILA1.5-3b</code> VILA Microservice does not check this Text Input \"What's in this image\" (default) Input Type Image URL(s) (default) URL <code>rtsp://&lt;JETSON_IP&gt;/&lt;PATH_TO_VIDEO_FILE&gt;</code> (e.g. <code>rtsp://10.110.51.72:31554/nvstream/opt/store/nvstreamer_videos/sample_1080p_h264.mp4</code>) </li> <li> <p>Click \"Test step\" button the top right of the prompt box, and check the output shown on the right.</p> <p></p> </li> <li> <p>Open \"VLC\". From \"Media\" menu, select \"Open Network Stream...\", input the address in the following format, and hit \"Play\" button</p> <pre><code>rtsp://&lt;JETSON_IP&gt;:5011/out\n</code></pre> <p></p> </li> <li> <p>Try sending another API request by hitting \"Test step\" button on n8n.</p> <p>Notice the overlay on the video showing the prompt and response changes (and fade away).</p> </li> <li> <p>If you have multiple video source ready, change the URL field, and hit \"Test step\" button again.</p> <p>Notice the stream changes to the new input you specify.</p> </li> </ol> <p>  Your browser does not support the video tag. </p>"},{"location":"vit/index.html","title":"Vision Transformers &amp; Computer Vision","text":"<p>You can quickly run these realtime Vision Transformers (ViT) and computer vision models onboard your Jetson:</p>"},{"location":"vit/index.html#efficient-vit","title":"Efficient ViT","text":""},{"location":"vit/index.html#nanosam","title":"NanoSAM","text":""},{"location":"vit/index.html#nanoowl","title":"NanoOWL","text":""},{"location":"vit/index.html#sam","title":"SAM","text":""},{"location":"vit/index.html#tam","title":"TAM","text":""},{"location":"vit/index.html#ultralytics-yolov8","title":"Ultralytics YOLOv8","text":""},{"location":"vit/tutorial_efficientvit.html","title":"Tutorial - EfficientViT","text":"<p>Let's run MIT Han Lab's EfficientViT on Jetson!</p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>10.9 GB</code> for <code>efficientvit</code> container image</li> <li>Space for checkpoints</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"vit/tutorial_efficientvit.html#how-to-start","title":"How to start","text":"<p>Use the <code>jetson-containers run</code> and <code>autotag</code> commands to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run $(autotag efficientvit)\n</code></pre>"},{"location":"vit/tutorial_efficientvit.html#usage-of-efficientvit","title":"Usage of EfficientViT","text":"<p>The official EfficientViT repo shows the complete usage information: <code>https://github.com/mit-han-lab/efficientvit#usage</code></p>"},{"location":"vit/tutorial_efficientvit.html#run-examplebenchmark","title":"Run example/benchmark","text":"<p>Inside the container, a small benchmark script <code>benchmark.py</code> is added under <code>/opt/efficientvit</code> directory by the jetson-container build process.</p> <p>It is to test EfficientViT-L2-SAM in bounding box mode, so we can use this as an example and verify the output.</p>"},{"location":"vit/tutorial_efficientvit.html#download-l2pt-model","title":"Download <code>l2.pt</code> model","text":"<pre><code>mkdir -p /data/models/efficientvit/sam/\ncd /data/models/efficientvit/sam/\nwget https://huggingface.co/han-cai/efficientvit-sam/resolve/main/l2.pt\n</code></pre> <p>The downloaded checkpoint file is stored on the <code>/data/</code> directory that is mounted from the Docker host.</p>"},{"location":"vit/tutorial_efficientvit.html#run-benchmark-script","title":"Run benchmark script","text":"<pre><code>cd /opt/efficientvit\npython3 ./benchmark.py\n</code></pre> <p>At the end you should see a summary like the following.</p> <pre><code>AVERAGE of 2 runs:\n  encoder --- 0.062 sec\n  latency --- 0.083 sec\nMemory consumption :  3419.68 MB\n</code></pre>"},{"location":"vit/tutorial_efficientvit.html#check-the-outputresult","title":"Check the output/result","text":"<p>The output image file (of the last inference result) is stored as <code>/data/benchmarks/efficientvit_sam_demo.png</code>.</p> <p>It is stored under <code>/data/</code> directory that is mounted from the Docker host. So you can go back to your host machine, and check <code>jetson-containers/data/benchmark/</code> directory.</p> <p>You should find the output like this.</p> <p></p>"},{"location":"vit/tutorial_nanoowl.html","title":"Tutorial - NanoOWL","text":"<p>Let's run NanoOWL, OWL-ViT optimized to run real-time on Jetson with NVIDIA TensorRT.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>7.2 GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"vit/tutorial_nanoowl.html#how-to-start","title":"How to start","text":"<p>Use the <code>jetson-containers run</code> and <code>autotag</code> commands to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run --workdir /opt/nanoowl $(autotag nanoowl)\n</code></pre>"},{"location":"vit/tutorial_nanoowl.html#how-to-run-the-tree-prediction-live-camera-example","title":"How to run the tree prediction (live camera) example","text":"<ol> <li> <p>Ensure you have a camera device connected</p> <pre><code>ls /dev/video*\n</code></pre> <p>If no video device is found, exit from the container and check if you can see a video device on the host side.</p> </li> <li> <p>Install missing module.</p> <pre><code>pip install aiohttp\n</code></pre> </li> <li> <p>Launch the demo     <pre><code>cd examples/tree_demo\npython3 tree_demo.py --camera 0 --resolution 640x480 \\\n    ../../data/owl_image_encoder_patch32.engine\n</code></pre></p> Option Description Example <code>--camera</code> To specify camera index (corresponds to <code>/dev/video*</code>) when multiple cameras are connected <code>1</code> <code>--resolution</code> To specify the camera open resolution in the format <code>{width}x{height}</code> <code>640x480</code> <p>Info</p> <p>If it fails to find or load the TensorRT engine file, build the TensorRT engine for the OWL-ViT vision encoder on your Jetson device.</p> <pre><code>python3 -m nanoowl.build_image_encoder_engine \\\n    data/owl_image_encoder_patch32.engine\n</code></pre> </li> <li> <p>Second, open your browser to <code>http://&lt;ip address&gt;:7860</code></p> </li> <li> <p>Type whatever prompt you like to see what works!</p> <p>Here are some examples</p> <ul> <li>Example: <code>[a face [a nose, an eye, a mouth]]</code></li> <li>Example: <code>[a face (interested, yawning / bored)]</code></li> <li>Example: <code>(indoors, outdoors)</code></li> </ul> </li> </ol>"},{"location":"vit/tutorial_nanoowl.html#result","title":"Result","text":""},{"location":"vit/tutorial_nanosam.html","title":"Tutorial - NanoSAM","text":"<p>Let's run NVIDIA's NanoSAM to check out the performance gain by distillation.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.3GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"vit/tutorial_nanosam.html#how-to-start","title":"How to start","text":"<p>Use the <code>jetson-containers run</code> and <code>autotag</code> commands to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run $(autotag nanosam)\n</code></pre>"},{"location":"vit/tutorial_nanosam.html#run-examples","title":"Run examples","text":"<p>Inside the container, you can move to <code>/opt/nanosam</code> directory, to go through all the examples demonstrated on the repo.</p> <pre><code>cd /opt/nanosam\n</code></pre> <p>To run the \"Example 1 - Segment with bounding box\":</p> <pre><code>python3 examples/basic_usage.py \\\n    --image_encoder=\"data/resnet18_image_encoder.engine\" \\\n    --mask_decoder=\"data/mobile_sam_mask_decoder.engine\"\n</code></pre> <p>The result is saved under <code>/opt/nanosam/data/basic_usage_out.jpg</code>.</p> <p>To check on your host machine, you can copy that into <code>/data</code> directory of the container where that is mounted from the host.</p> <pre><code>cp data/basic_usage_out.jpg /data/\n</code></pre> <p>Then you can go to your host system, and find the file under <code>jetson-containers/data/basic_usage_out.jpg</code></p>"},{"location":"vit/tutorial_nanosam.html#results","title":"Results","text":""},{"location":"vit/tutorial_sam.html","title":"Tutorial - SAM (Segment Anything)","text":"<p>Let's run Meta's <code>SAM</code> on NVIDIA Jetson.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson devices:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB) Jetson Orin NX (16GB) Jetson Orin Nano (8GB)\u26a0\ufe0f<sup>1</sup></p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x) JetPack 6 (L4T r36.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.8GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"vit/tutorial_sam.html#how-to-start","title":"How to start","text":"<p>Use the <code>jetson-containers run</code> and <code>autotag</code> commands to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run $(autotag sam)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start the Jupyter Lab server.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:8888</code>.</p> <p>The default password for Jupyter Lab is <code>nvidia</code>.</p>"},{"location":"vit/tutorial_sam.html#run-jupyter-notebook","title":"Run Jupyter notebook","text":"<p>In Jupyter Lab, navigate to <code>notebooks</code> and open <code>automatic_mask_generator_example.py</code> notebook.</p> <p>Create a new cell at the top, insert the model download command below and run the cell.</p> <pre><code>!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n</code></pre> <p>Then go through executing all the cells below Set-up.</p>"},{"location":"vit/tutorial_sam.html#results","title":"Results","text":"<ol> <li> <p>The biggest <code>vit_h</code> (2.4GB) model may not ran due to OOM, but <code>vit_l</code> (1.1GB) runs on Jetson Orin Nano.\u00a0\u21a9</p> </li> </ol>"},{"location":"vit/tutorial_tam.html","title":"Tutorial - SAM (Segment Anything)","text":"<p>Let's run <code>TAM</code> to perform Segment Anything on videos on NVIDIA Jetson.</p> <p></p> <p>What you need</p> <ol> <li> <p>One of the following Jetson:</p> <p>Jetson AGX Orin (64GB) Jetson AGX Orin (32GB)</p> </li> <li> <p>Running one of the following versions of JetPack:</p> <p>JetPack 5 (L4T r35.x)</p> </li> <li> <p>NVMe SSD highly recommended for storage speed and space</p> <ul> <li><code>6.8GB</code> for container image</li> <li>Spaces for models</li> </ul> </li> <li> <p>Clone and setup <code>jetson-containers</code>:</p> <pre><code>git clone https://github.com/dusty-nv/jetson-containers\nbash jetson-containers/install.sh\n</code></pre> </li> </ol>"},{"location":"vit/tutorial_tam.html#how-to-start","title":"How to start","text":"<p>Use the <code>jetson-containers run</code> and <code>autotag</code> commands to automatically pull or build a compatible container image.</p> <pre><code>jetson-containers run $(autotag tam)\n</code></pre> <p>The container has a default run command (<code>CMD</code>) that will automatically start TAM's web server.</p> <p>Open your browser and access <code>http://&lt;IP_ADDRESS&gt;:12212</code>.</p>"},{"location":"vit/tutorial_tam.html#tam-web-ui","title":"TAM web UI","text":"<p>Check out the official tutorial to learn how to operate the web UI.</p>"},{"location":"vit/tutorial_tam.html#results","title":"Results","text":""},{"location":"vit/tutorial_tam.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"vit/tutorial_tam.html#filenotfounderror-errno-2-no-such-file-or-directory-checkpointse2fgvi-hq-cvpr22pth","title":"<code>FileNotFoundError: [Errno 2] No such file or directory: './checkpoints/E2FGVI-HQ-CVPR22.pth'</code>","text":"<p>You may find the TAM app fails to download a checkpoint file <code>E2FGVI-HQ-CVPR22.pth</code>.</p> <pre><code>Downloading checkpoints from Google Drive... tips: If you cannot see the progress bar, please try to download it manuall               and put it in the checkpointes directory. E2FGVI-HQ-CVPR22.pth: https://github.com/MCG-NKU/E2FGVI(E2FGVI-HQ model)\nAccess denied with the following error:\n\n        Cannot retrieve the public link of the file. You may need to change\n        the permission to 'Anyone with the link', or have had many accesses. \n\nYou may still be able to access the file from the browser:\n\n         https://drive.google.com/uc?id=10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3 \n</code></pre> <p>You can manually download the checkpoint file on your Docker host machine.</p> <pre><code>cd jetson-containers/\npip install gdown\nsource ~/.profile\ngdown https://drive.google.com/uc?id=10wGdKSUOie0XmCr8SQ2A2FeDe-mfn5w3 \nmv E2FGVI-HQ-CVPR22.pth ./data/models/tam/\n</code></pre> <p>And you can try running the TAM container.</p> <pre><code>jetson-containers run $(autotag tam)\n</code></pre>"}]}